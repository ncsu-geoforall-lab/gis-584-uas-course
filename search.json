[
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Mapping and Analytics Using UAS",
    "section": "Instructor",
    "text": "Instructor\nDr. Corey T. White\nOffice hours: by appointment Email: ctwhite@ncsu.edu"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Mapping and Analytics Using UAS",
    "section": "Prerequisites",
    "text": "Prerequisites\nNone, but GIS/MEA582 and/or GIS540 or equivalent courses are highly recommended."
  },
  {
    "objectID": "index.html#educational-approach",
    "href": "index.html#educational-approach",
    "title": "Mapping and Analytics Using UAS",
    "section": "Educational Approach",
    "text": "Educational Approach\nThis course will consist of: lectures, readings, hands-on exercises, homework assignments, and a major project. All the work will be collected within an electronic portfolio that will systematically include the work that you will do for this and other GIS courses. Extra credits will be given for innovative solutions, creativity in problem solving and extensions to given tasks."
  },
  {
    "objectID": "index.html#class-materials-and-schedule",
    "href": "index.html#class-materials-and-schedule",
    "title": "Mapping and Analytics Using UAS",
    "section": "Class materials and schedule",
    "text": "Class materials and schedule\nSee instructions in Course logistics and schedule"
  },
  {
    "objectID": "index.html#moodle",
    "href": "index.html#moodle",
    "title": "Mapping and Analytics Using UAS",
    "section": "Moodle",
    "text": "Moodle\nCourse Forum, assignment submissions, project material, and grades are handled in Moodle.\n\nTextbooks\nNo required textbook, on-line material is used.\nRecommended books:\n\nAmy Frazier, Kunwar Singh, 2021, Fundamentals of Capturing and Processing Drone Imagery and Data, CRC Press.\nJonathan L. Carrivick, Mark W. Smith, and Duncan J. Quincey, 2016, Structure from Motion in the Geosciences, John Wiley & Sons, Ltd.\nPaul Gerin Fahlstrom, Thomas James Gleason, 2012, Introduction to UAV Systems: Fourth Edition, John Wiley & Sons, Ltd.\nDouglas M. Marshall, R. Kurt Barnhart, Eric Shappee, Michael Most, 2016, Introduction to unmanned aircraft systems: Second Edition, CRC Press\n\n\n\nSoftware\nSee Course logistics web page."
  },
  {
    "objectID": "index.html#grading-policy",
    "href": "index.html#grading-policy",
    "title": "Mapping and Analytics Using UAS",
    "section": "Grading policy",
    "text": "Grading policy\n60% homeworks, 40% project 100% is the maximum number of points (total+extra credits) achieved in class. Points are taken off for late submissions.\n\n\n\nGrade\nCourse and each HW\n\n\n\n\nMax\n100\n\n\nA+\n97\n\n\nA\n93\n\n\nA-\n90\n\n\nB+\n87\n\n\nB\n83\n\n\nB-\n80\n\n\nC+\n77\n\n\nC\n73"
  },
  {
    "objectID": "index.html#topics",
    "href": "index.html#topics",
    "title": "Mapping and Analytics Using UAS",
    "section": "Topics",
    "text": "Topics\n\nIntroduction to Unmanned Aerial Systems\nRules and regulations for UAS operations\nFrom images to 3D models: Photogrammetry and Structure from Motion concepts\nUAS flight planning\nImagery processing and structure from motion (SfM)\nAccuracy of UAS-derived DSM and orthophoto\nUAS and lidar data: comparison and fusion\nAnalysis of ultra high resolution elevation models\nAnalysis of multitemporal UAS data and its applications"
  },
  {
    "objectID": "index.html#academic-integrity",
    "href": "index.html#academic-integrity",
    "title": "Mapping and Analytics Using UAS",
    "section": "Academic integrity",
    "text": "Academic integrity\nOverview, Policies, Code of Student Conduct"
  },
  {
    "objectID": "index.html#attendance-policy",
    "href": "index.html#attendance-policy",
    "title": "Mapping and Analytics Using UAS",
    "section": "Attendance policy",
    "text": "Attendance policy\nin regular section, attendance is checked at each class, see also attendance regulations and university definitions of excused absences"
  },
  {
    "objectID": "index.html#accommodation-of-students-with-disabilities",
    "href": "index.html#accommodation-of-students-with-disabilities",
    "title": "Mapping and Analytics Using UAS",
    "section": "Accommodation of students with disabilities",
    "text": "Accommodation of students with disabilities\nDisability Services Office"
  },
  {
    "objectID": "index.html#large-language-model-llm-policy",
    "href": "index.html#large-language-model-llm-policy",
    "title": "Mapping and Analytics Using UAS",
    "section": "Large Language Model (LLM) policy",
    "text": "Large Language Model (LLM) policy\nThe use of LLMs in this course is permitted for the purposes of completing assignments and projects. However, the use of LLMs for the purposes of completing exams is strictly prohibited. Students are also required to cite the use of LLMs in their assignments and projects. Failure to do so will result in a violation of the courses academic integrity policy. Students are also to be aware that the use of LLMs in this course is not a substitute for learning the material. Assignments and projects must be properly cited and contain no plagerized materials. (The policy was created with the aid GitHub CoPilot)"
  },
  {
    "objectID": "course/schedule.html",
    "href": "course/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Schedule will be adjusted as needed during the semester.\n\n\nAug. 19 | Introduction to the Course\n\n\n\nCourse logistics: link (Course instructions, management, and resources)\n\n\nCourse Moodle site: link\n\n\n\nAug. 19-23 | Topic 1: UAS Basics\n\n\n1A. Introduction to Unmanned Aerial Systems\n\n\n\nMon: Lecture\n\n\nWed: Work on assingment 1A in class\n\n\nAssignment 1A: prepare presentation on UAS applications\n\n\nAssignment Due Date: Aug. 28\n\nAug. 26-30 | Topic 1: UAS Basics\n\n\n1B. Rules and regulations for UAS operations\n\n\n\nMon: Lecture\n\n\nWed: Student presentations of UAS applications\n\n\nAssignment: 1B UAS Rule Part 107 test subset\n\n\nAssignment Due Date: Sep. 4\n\nSep. 2-6 | Topic 3: UAS Flight Planning (Revised)\n\n\n3. UAS flight planning\n\n\n\nMon: Labor Day - No Class\n\n\nWed: Lecture\n\n\nAssignment Due Date: Sep. 25\n\nSep. 9 | Pre-Proposal Due\n\n\nProjects Page\n\n\n\nSubmit brief Idea to the project discussions forum\n\n\nAssignment Due Date: Sep. 9\n\nSep. 9-13 | Topic 2: Structure from Motion (Revised)\n\n\n2A. From images to 3D models: Photogrammetry and Structure from Motion concepts\n\n\n\nMon: Lecture\n\n\nWed: Assignment 2A: 3D models from indoor photos\n\n\nProject: submit proposal\n\n\nAssignment Due Date: Sep. 30\n\nSep. 16-20 | UAS FLIGHT\n\n\n\nMon: Assignment 3. UAS flight planning\n\n\nWed: (UAS Flight @ 10:15 am)\n\n\nMeeting point: Mid Pines Road, Raleigh; 35.727104, -78.696173\n\n\nProject: Data acquisition\n\n\nAssignment Due Date: Sep. 25\n\nSep. 23-27 | Topic 2: Structure from Motion\n\n\n2B. UAS Imagery processing\n\n\n\nMon: Lecture\n\n\nWed: In Class Assignment - Processing with AGISOFT Photoscan\n\n\nAssignment Due Date: Sep. 30\n\nSep. 30 - Oct. 4 | Topic 4: Geospatial Analytics\n\n\n4A. Analysis of UAS data processing results\n\n\n\nMon: Lecture\n\n\nWed: In Class Assignments\n\n\nAssignment Due Date: Oct. 23\n\nOct. 7-11 | Topic 4: Geospatial Analytics\n\n\n4B. Point cloud data analysis\n\n\n\nMon: Lecture\n\n\nWed: In Class Assignments\n\n\nProject: Data processing\n\n\nAssignment Due Date: Oct. 23\n\nOct. 14-18 | Fall Break - Guest Speaker\n\n\n\nMon: Fall break - (No Class)\n\n\nWed: Guest speaker: Canceled\n\n\nProject: Data processing\n\n\nAssignment Due Date: Oct. 23\n\nOct. 21-25 | Topic 5: Advanced Analytics\n\n\n5A. UAS and Lidar Data: Comparison, Fusion and Analysis\n\n\n\nMon: Lecture 5A\n\n\nWed: Dr. William Reckling (LinkedIn)\n\n\nProject: Workflow development and testing\n\n\nAssignment Due Date: Nov. 13\n\nOct. 28 - Nov. 1 | Topic 5: Advanced Analytics\n\n\n5B. Analysis of Multitemporal UAS Data and its Applications\n\n\n\nMon: Assignment 5A: Multitemporal UAS data analysis\n\n\nAssignment Due Date: Nov. 13\n\nNov. 4-8 | Topic 5: Advanced Analytics\n\n\n5B. Analysis of Multitemporal UAS Data and its Applications\n\n\n\nMon: Lecture 5B\n\n\nWed: Assignment 5B: Multitemporal UAS data analysis\n\n\nAssignment Due Date: Nov. 13\n\nNov. 11-15 | Topic 6: Imagery processing and SfM with Open Source Solutions\n\n\n6A. OpenDroneMap and WebODM\n\n\n\nMon: Lecture\n\n\nWed: Assignment: Processing with ODM or WebODM, high performance computing\n\n\nProject: Perform analysis and/or modeling\n\n\nAssignment Due Date: Nov. 20\n\nNov. 18-22 | Guest Speaker & Project help sessions\n\n\n\nMon: Guest Speaker: Mitchell DuRant - Cypress Creek Renewables (LinkedIn)\n\n\nWed: Project help session\n\n\nAssignment Due Date: Dec. 4\n\nNov. 25-29 | Thanksgiving break: No Class\n\n\n\nWork on project: visualize results, work on presentation\n\n\nProject presentation and paper requirements, final project titles\n\n\n\nDec. 2-6 | Project presentations (See the requirements and project titles)\n\n\n\nDE section: To record your presentation, use Open Broadcaster Software (tutorial), MS Powerpoint, Libre Office, or any other software with similar recording functionality.\n\n\nUpload the presentation to Moodle or, if the file is too big, upload your file to Google drive and share it with the instructors.\n\n\nPlease keep the presentation to 10 minutes and name your file LastnameFirstname_Presentation.avi, LastnameFirstname_Presentation.pptx (or whatever format you use).\n\n\nRecorded presentations are due Dec. 4.\n\n\nAssignment Due Date: Dec. 4\n\nDec. 9-13 | Finish project paper and upload it to Moodle as a pdf file LastnameFirstname_Paper.pdf\n\n\n\nFor archiving purposes also submit a complete coursework in a ZIP file\n\n\nThe ZIP file should include: project paper, presentation slides, and all homework papers\n\n\nUpload your work as a single ZIP file LastnameFirstname.zip to your Google drive and share it with the instructors.\n\n\nAssignment Due Date: Dec. 9\n\n\nIf you are looking for what is the next step, here are some related courses.\n\n\nNo matching items"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/assignments/assignment_4a.html#task",
    "href": "course/topics/topic_4_GIS_analytics/assignments/assignment_4a.html#task",
    "title": "Assignment 4A",
    "section": "Task",
    "text": "Task\n\nExecute measurements in Agisoft Metashape\nAnalyze differences in DSMs generated using different techniques or acquired at different times. Distinguish between errors, artifacts, and real change in vegetation and/or bare ground surface."
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/assignments/assignment_4a.html#homework",
    "href": "course/topics/topic_4_GIS_analytics/assignments/assignment_4a.html#homework",
    "title": "Assignment 4A",
    "section": "Homework",
    "text": "Homework\nPrepare report on the results of measurements in Agisoft and DSM comparison in GRASS GIS Include figures/graphs and calculations."
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/assignments/point_clouds.html#outline",
    "href": "course/topics/topic_4_GIS_analytics/assignments/point_clouds.html#outline",
    "title": "Assignment 4A - Analysis",
    "section": "Outline",
    "text": "Outline\n\nAnalyze and import lidar point cloud for Mid Pines area\nCompute bare earth and canopy surfaces and derived parameters\nEmploy topographic analysis techniques to highlight subtle terrain features\nExplore voxel model based analysis of multiple return point cloud (optional)"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/assignments/point_clouds.html#data",
    "href": "course/topics/topic_4_GIS_analytics/assignments/point_clouds.html#data",
    "title": "Assignment 4A - Analysis",
    "section": "Data",
    "text": "Data\n\nLake Wheeler data set (formatted as GRASS location): Lake_Wheeler_NCspm (you should already have it from previous assignments)\n2013 lidar point cloud for Midpines\n\nClassified point cloud in LAS format: mid_pines_spm_2013.las\nCloud Optimized Point Cloud\nGround only in TXT format\nFirst return only in TXT format\n\n2015 UAS sample point cloud for Midpines sub-area, derived by SfM\n\nSample point cloud: 2015_06_sample_points_NCsmp.las\n\nASPRS LAS specification file with the list of ASPRS Standard LIDAR Point Classes on p.10"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/assignments/point_clouds.html#software",
    "href": "course/topics/topic_4_GIS_analytics/assignments/point_clouds.html#software",
    "title": "Assignment 4A - Analysis",
    "section": "Software",
    "text": "Software\nIn the assignment we will be using GRASS GIS 8.4 or higher, libLAS (included in GRASS GIS package for MS Windows and Ubuntu), and a web-based point cloud viewer plas.io (might not work with older browsers, most functionality is available in Chrome). Please refer to the Course logistics webpage for links to software in case you don’t have it installed already."
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/assignments/point_clouds.html#quick-data-exploration-in-web-browser",
    "href": "course/topics/topic_4_GIS_analytics/assignments/point_clouds.html#quick-data-exploration-in-web-browser",
    "title": "Assignment 4A - Analysis",
    "section": "Quick data exploration in web browser",
    "text": "Quick data exploration in web browser\nFirst view the downloaded lidar and UAS las files “mid_pines_spm_2013.las” and “2015_06_sample_points_NCsmp.las” in your web browser using plas.io. Try out different settings and tools."
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/assignments/point_clouds.html#gis-based-analysis-of-point-clouds",
    "href": "course/topics/topic_4_GIS_analytics/assignments/point_clouds.html#gis-based-analysis-of-point-clouds",
    "title": "Assignment 4A - Analysis",
    "section": "GIS-based analysis of point clouds",
    "text": "GIS-based analysis of point clouds\nLaunch GRASS GIS with the Lake_Wheeler_ncspm location. Create a new mapset for this assignment.\nChange the current working directory to the directory where you downloaded the LAS files using cd command and path or in case you work in command line in GRASS GUI just type cd and press enter and select the directory using a dialog.\ncd ~/Downloads\nLet’s first look at the lidar point cloud metadata, particularly at the classes, we will later work with the classes 1 and 2.\nWindows:\nlasinfo mid_pines_spm_2013.las\nLinux/Mac\npdal info mid_pines_spm_2013.las\nThe output shows number of points in each class:\n  Point Classifications\n---------------------------------------------------------\n    1340658 Unclassified (1) \n    2580704 Ground (2) \n    66 Low Point (noise) (7) \n    1960603 Reserved for ASPRS Definition (11) \nClass 11 - for the explanation of Categories in this class refer to p. 10 in ASPRS LAS specification file. In this particular case, the classification scheme was not used - the metadata define class 11 as “withheld”.\nGet the geographic extent of the point cloud and then set the computational region to tis extent:\nr.in.lidar -sgo input=mid_pines_spm_2013.las\ng.region n=220218 s=218694 e=637795 w=636271 -p\n\nExplore the density of points\n\nLidar point cloud\nFirst we set the resolution to 1 meter (flag -a ensures region boundaries are adjusted to be even multiples of the resolution value):\ng.region res=1 -pa\nCompute the density of all points using binning:\nr.in.lidar -o input=mid_pines_spm_2013.las output=mid_pines_dens_all method=n\nd.rast mid_pines_dens_all\nd.legend -f -s -d rast=mid_pines_dens_all at=5,50,7,10\nd.out.file mid_pines_dens_all.png\nCompute the density of ground points and set histogram equalized color table for both densities so that we can compare them:\nr.in.lidar -o input=mid_pines_spm_2013.las output=mid_pines_dens_ground class_filter=2 method=n\nr.colors -e map=mid_pines_dens_all,mid_pines_dens_ground color=bcyr\nd.rast mid_pines_dens_ground\nCompare both rasters and save as images for report.\nLater, we will examine the density using v.outlier and imported vector points.\n\n\n\nLidar point cloud: raster binning and classification\nCompute different surfaces by binning. Explore what different returns and classes show.\nCreate a raster map of classes:\n\n1 - ground\n2 - vegetation\n3 - buildings\n\nCreate DSM:\ng.region n=220218 s=218694 e=637795 w=636271 res=2 -pa\nr.in.lidar -o input=mid_pines_spm_2013.las output=mid_pines_all_max method=max resolution=3 class_filter=1,2\nCreate surface based on last return\n\n\n\n\n\n\nCaution\n\n\n\nLast return with r.in.lidar represents canopy because it filters last return where there are multiple returns.\nIn general, last return represents the overall last return from the lidar pulse an is commonly used to identify ground.\n\n\nr.in.lidar -o input=mid_pines_spm_2013.las output=mid_pines_last_mean method=mean return_filter=last class_filter=1,2\nCreate surface representing ground based on already classified points:\nr.in.lidar -o input=mid_pines_spm_2013.las output=mid_pines_ground_mean class_filter=2\nCombine surfaces and create classes:\n\n1 - ground\n2 - vegetation\n3 - buildings\n\nr.mapcalc \"classes = if( not(isnull(mid_pines_last_mean)), 2, if( not(isnull(mid_pines_ground_mean)), 1, if( not(isnull(mid_pines_all_max)), 3, null())))\"\nSet colors with r.colors (right click on classes layer - Set color table), copy and paste the following color rules into “or enter values directly” text field located in Define tab (option rules):\n1 220:220:180\n2 0:180:0\n3 150:0:0\nLook at places where our simplistic classification failed to properly identify buildings. Save image to file:\nd.out.file classes.png\n\n\nHigh resolution DEM/DSM\nImport the point cloud as vector points (without attribute table, not necessary to build points):\nv.in.lidar -b -t -o input=mid_pines_spm_2013.las output=mid_pines_ground class_filter=2\nv.in.lidar -b -t -o input=mid_pines_spm_2013.las output=mid_pines_surface class_filter=1,2 return_filter=first\nRemove the vector layers from your Layer Manager if they were added (you can disable automatic adding of layers in the dialog for the module).\nCheck the point overall point density using v.outlier:\nv.outlier -e input=mid_pines_ground\n\nEstimated point density: 1.111 Estimated mean distance between points: 0.9487\n\nInterpolate with resolution 0.3 meter, also create slope and profile curvature map.\ng.region n=219780 s=219100 e=637250 w=636575 res=0.5 -pa\nv.surf.rst input=mid_pines_ground elevation=mid_pines_ground_elev slope=mid_pines_ground_slope pcurv=mid_pines_ground_profcurv npmin=80 tension=20 smooth=1\nv.surf.rst input=mid_pines_surface elevation=mid_pines_surface_elev slope=mid_pines_surface_slope pcurv=mid_pines_surface_profcurv npmin=80 tension=20 smooth=1\nr.colors map=mid_pines_ground_elev,mid_pines_surface_elev color=elevation\nYou can recompute it with higher npmin=150 to reduce artifacts from segmentation visible on slope and curvature maps (will be much slower!).\nVisualize DEM and DSM in 3D view, use cross-sections.\nLeave just raster map ‘mid_pines_surface_elev’ in Layer Manager, hide legend, zoom to computational region. Go to 3D view, set surface resolution 1 and color to map ‘classes’. Save images.\n\n\nCompute the difference of lidar DEM and GCP heights\nFirst, download the GCP_12_decimal pack file\nand unpack it to GRASS\nv.unpack GCP_12_decimal.pack\nThen, add the columns:\nv.db.addcolumn map=GCP_12_decimal columns=\"dem_height DOUBLE, height_difference DOUBLE\"\nQuery (sample) the raster elevation at the locations of GCPs:\nv.what.rast -i map=GCP_12_decimal raster=mid_pines_ground_elev column=dem_height\nCompute the difference and save it in the attribute column:\nv.db.update map=GCP_12_decimal column=height_difference query_column=\"ASL - dem_height\"\nView the results:\nv.db.select map=GCP_12_decimal columns=ASL,dem_height,height_difference\nCompute extended univariate statistics and show thematic map:\nv.univar -e GCP_12_decimal column=height_difference\nd.vect.thematic map=GCP_12_decimal column=height_difference algorithm=int nclasses=4 colors=blue,30:144:255,173:216:230,255:192:203 icon=basic/circle size=20\nd.legend.vect at=10,40\nAlternatively use Attribute Table Manager to view the results.\n\n\nGround classification (optional)\nDownload extension v.lidar.mcc:\ng.extension v.lidar.mcc\nModule v.lidar.mcc uses a multiscale curvature-based classification algorithm to distinguish ground and non-ground points:\ng.region n=219511 s=219354 w=637071 e=637249 res=1\nv.in.lidar -t -r -o input=mid_pines_spm_2013.las output=mid_pines_sample\nv.lidar.mcc input=mid_pines_sample ground=ground nonground=nonground\nVisualize classification results in 2D or 3D.\n\n\n\nVisualize point density in 3D (optional)\nConvert LAS file into text file:\nlas2txt -i mid_pines_spm_2013.las --parse xyzc -o mid_pines.txt\nSet smaller region for creating the 3D raster:\ng.region n=219502 s=219348 w=637070 e=637276 b=110 t=135 res=2 res3=2 tbres=0.5 -p3\nUse binning to create point density 3D raster. Run r3.in.lidar, directly on the LAS file:\nr3.in.lidar -o input=mid_pines_spm_2013.las n=mid_pines_dens\nWith older GRASS versions where r3.in.lidar was not available yet use an alternative tool:\nr3.in.xyz input=mid_pines.txt output=mid_pines_dens method=n separator=comma\nSet custom color table with r3.colors:\n0% 255:255:100\n5% green\n100% red\nVisualize point density in 3D view using slices and isosurfaces:\n\nLeave only 3D raster mid_pines_dens in Layer Manager.\nSet computational region based on the 3D raster and zoom to it.\nSwitch to 3D.\nOn View page press Reset, set Height to 50 and Z-exag to 2. Nothing is visible yet.\nOn Data page - 3D raster, check Draw wire box, set resolution to 1.\nAdd isosurface and set isosurface value 1 and press Enter.\nCheck toggle normal direction and set different color of isosurface.\nExperiment with different isosurface levels (press enter to confirm the new value).\nRemove isosurface, change draw mode to slices, add slice and set its axis to Z.\nManipulate with the slice.\nSave images for report.\n\n\n\n\nLidar transect visualization (optional)\nDownload extension v.profile.points:\ng.extension v.profile.points\nWe will import small sample of lidar points (limited by computational region extent) with attribute table:\ng.region n=219452 s=219450 w=637132 e=637134\nv.in.lidar -r -o input=mid_pines_spm_2013.las output=mid_pines_all\nCompute profile, set the color based on intensity values in attribute table, and visualize. You need to zoom to the vector map to see it:\nv.profile.points point_input=mid_pines_all output=profile width=5 coordinates=637106,219373,637132,219451\nv.colors map=profile use=attr column=intensity color=grass\nd.vect map=profile color=none width=1 icon=basic/circle\n\n\nUAS point cloud density (optional)\nNote: importing this UAV point cloud may not work for you on Windows.\nImport binned UAV for subarea. First find the extent, then set the region:\nr.in.lidar -go input=2015_06_sample_points_NCspm.las out=midpines_uav_06_02m_raw\ng.region n=219660 s=219288 e=637287 w=636876 res=1 -pa\nCompute the point density at 1 meter resolution and compare with previously computed lidar point density:\nr.in.lidar -o input=2015_06_sample_points_NCspm.las out=midpines_uav_06_1m_dens method=n\nr.colors -e map=mid_pines_dens_all,mid_pines_dens_ground,midpines_uav_06_1m_dens color=bcyr\nAdd legend of the UAV density:\nd.legend -f -s -d rast=midpines_uav_06_1m_dens at=5,50,7,10\nZoom to the UAS subarea and compare the lidar and UAS densities, save images."
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/assignments/volumes.html#measurements-in-agisoft-metashape",
    "href": "course/topics/topic_4_GIS_analytics/assignments/volumes.html#measurements-in-agisoft-metashape",
    "title": "Assignment 4A - Volumes",
    "section": "Measurements in Agisoft Metashape",
    "text": "Measurements in Agisoft Metashape\n\nTask\nPerform measurements in Agisoft Metashape:\n\ncalculate distance, area and volume measuremens\ngenerate profiles and countour lines\n\n\n\nData\n\nImagery from 02/06/2018 Lake Wheeler flight\nAgisoft project files with processed data\nImagery for the volume and area measurements acquired 07/02/2017 at Wake Waste Transfer check localizations\n\n\n\nPerforming measurements on a model\n\nData Preparation\nIn the first part of the assignment you will be working with data from 02/06/2018 Lake Wheeler flight. Because the processing is very time consuming, the processed data (stored in the Agisoft project files is provided).\nAfter opening the 2010_02_06_assignment.psx file, you should see generated orthomosaic and DSM in the Workspace pane.\nAll you have to do is to indicate the new location of the pictures.\nIn order to do that right clik on any of the pictures in the Pictures pane and choose Change Path... and in the dialog window mark the All cameras option. This will automatically apply the updated location to all the pictures in the project.\n\n\n\n\n\n\n\nDistance measurement\nUsing the Ruler instrument for the toolbar menu calculate the length of the longer side of the bigger building.\n\n\nDistance between GCPs\nTo measure distance between two markers in Agisoft:\n\nSelect both markers to be used for distance measurements on the Reference pane using CtrlCtrl button\nSelect Create Scale Bar command form the 3D view context menu (Right click on the marker). The scale bar will be created and an instant added to the Scale Bar list on the Reference pane.\n\n\n\n\n\n\n\nNote\n\n\n\nIn order to see the estimated values, you need to be in the estimates display - on the Reference pane, following button needs to be active\n\n\n\n\n\n\n\nCompare distance between UAV 9 and UAV 12 estimated by Agisoft Metashape with the distance calculated from coordinates:\n\n\n\nGCP\nEasting [m]\nNorthing [m]\nASL [m]\n\n\n\n\nUAV 9\n636744.300\n219381.308\n109.971\n\n\nUAV 12\n637057.310\n219733.909\n116.12\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember to take into account elevation difference in your calculations. Show your work\nHow much (in meters/percentage) do these values differ?\nEuclidean Distance formula: \\[\nd = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2}\n\\]\nCalculate distance in meters: \\[\n\\text{Difference in meters} = \\left| \\text{Metashape distance} - \\text{Calculated distance} \\right|\n\\]\nCalculate percent difference: \\[\n\\text{Percentage difference} = \\left( \\frac{\\text{Difference}}{\\text{Calculated distance}} \\right) \\times 100\n\\]\n\n\n\n\nDistance between cameras\nTo measure distance between two cameras in Agisoft:\n\nSelect the two cameras on the Workspace or Reference pane using CtrlCtrl button. Alternatively, the cameras can be selected in the Model view window using selecting tools from the Toolbar.\nSelect Create Scale Bar command form the 3D view context menu. The scale bar will be created and an instant added to the Scale Bar list on the Reference pane.\n\nMeasure distance between cameras DSC03341 and DSC03752.\n\n\n\nPerforming measurements on DSMs\nSwitch to Ortho view (double click on the Orthomosaic in the Workspace pane)\n\nPoint and distance measurement:\nIn the western part of the area, close to the forest edge, there are some sheaves.\n\n\n\n\n\n\nTip\n\n\n\nsheaf (sheaves) - a bundle of grain stalks laid lengthwise and tied together after reaping.\n\n\n\nUsing Ruler instrument or Draw Polyline from the Toolbar of the Ortho view estimate the diameter and height of the very north sheaf.\n\n\n\nContours:\n\nSelect Generate Contours... command from Tools menu.\nSet round values for Minimal altitude, Maximal altitude parameters as well as the Interval for the contours.\nSet the transparency for the contours for 50% and do not display the labels. (You can do it in Preferences in the Contours context menu in the Workspace pane (contours are in the Shapes folder))\n\n\n\nCross Section:\nChoose some interesting place for cross section (you can see from the contours where are some variations in terrain)\n\nIndicate a line to make a cut of the model using Draw Polyline tool from the Ortho view toolbar (double click ends the line).\nRight button click on the polyline/polygon and select Measure... command from the context menu.\nInclude in the report your profile with the and its location.\n\n\n\n\nVolume and area measurements\nIn this part of the assignment you will be working with imagery acquired 07/02/2017 at Wake Waste Transfer.\nCalculate the volume and the area of the “waste pile” (for excact location see Google Maps)\n\nTips\n\nFollow the Agisoft’s tutorial on volume measurements\nthe delimitation of the pile is up to you - you can examine whatever part of the pile you want to process\nuse a high (or custom) face count setting in mesh generation to have detailed mesh\nremember to close mesh holes (level 100%)\nyou can additionally generate contours"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/part_b_point_cloud_analysis.html#outline",
    "href": "course/topics/topic_4_GIS_analytics/part_b_point_cloud_analysis.html#outline",
    "title": "B. Point cloud data analysis",
    "section": "Outline",
    "text": "Outline\n\nCharacteristics of UAS SfM and lidar-based point cloud data\nPoint cloud data representation and formats\nPoint cloud data processing, visualization, and analysis\nBare earth point extraction\nWorking with classified point clouds\nComputing DEM and topographic analysis\nVoxel-based analysis and vertical profiles"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/part_b_point_cloud_analysis.html#lecture",
    "href": "course/topics/topic_4_GIS_analytics/part_b_point_cloud_analysis.html#lecture",
    "title": "B. Point cloud data analysis",
    "section": "Lecture",
    "text": "Lecture\n\nLecture slides\nLecture Recording: Fall 2024 (NCSU Only)\nIn-Class Recording: Fall 2024 (NCSU Only)"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/part_b_point_cloud_analysis.html#supplemental-materials",
    "href": "course/topics/topic_4_GIS_analytics/part_b_point_cloud_analysis.html#supplemental-materials",
    "title": "B. Point cloud data analysis",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nProcessing point clouds in GRASS GIS, workshop presented at FOSS4G conference in Boston in 2017 (start with the section “Binning the point cloud”).\nUSGS 3DEP Data View\nLAS specification v1.4-R13, July 2013\nLidar and INS\nPast Lecture recordings:\n\nProperties\nProcessing\nDEMs and voxel models"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/part_b_point_cloud_analysis.html#assignment-homework",
    "href": "course/topics/topic_4_GIS_analytics/part_b_point_cloud_analysis.html#assignment-homework",
    "title": "B. Point cloud data analysis",
    "section": "Assignment & Homework",
    "text": "Assignment & Homework\nAssignment 4B"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#outline",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#outline",
    "title": "Point cloud data analysis",
    "section": "Outline",
    "text": "Outline\n\nCharacteristics of UAS and lidar-based point cloud data\nPoint cloud data processing, visualization, and analysis\nComputing DEM / DSM, and topographic parameters\nVoxel-based analysis and vertical profiles"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#what-are-point-clouds",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#what-are-point-clouds",
    "title": "Point cloud data analysis",
    "section": "What are point clouds?",
    "text": "What are point clouds?\n\nDense set of points (x,y,z) defined in 3D space:\n\nDirectly measured using lidar\nDerived from overlapping images using SfM (see previous lectures)\n\n\n\nUAS SfM derived point cloud from Midpines viewed at Hunt library"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#lidar-point-cloud-acquisition",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#lidar-point-cloud-acquisition",
    "title": "Point cloud data analysis",
    "section": "Lidar point cloud acquisition",
    "text": "Lidar point cloud acquisition\n\nMeasured time of pulse return is converted to distance\nGeoreferencing is based on the position (measured by GPS) and exterior orientation (measured by inertial navigation system INS) of the platform"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#multiple-return-lidar-point-cloud",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#multiple-return-lidar-point-cloud",
    "title": "Point cloud data analysis",
    "section": "Multiple return lidar point cloud",
    "text": "Multiple return lidar point cloud\nLidar pulse can penetrate the tree canopy leading to multiple pulse returns\n\nyellow: first return, dark brown: second return"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#multiple-return-point-cloud-profiles",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#multiple-return-point-cloud-profiles",
    "title": "Point cloud data analysis",
    "section": "Multiple return point cloud profiles",
    "text": "Multiple return point cloud profiles\nMultiple return point cloud profile view of returns"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#lidar-point-cloud-data",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#lidar-point-cloud-data",
    "title": "Point cloud data analysis",
    "section": "Lidar point cloud data",
    "text": "Lidar point cloud data\nSet of [x, y, z, (r, i, c, …)] measured points reflected from Earth surface or objects on or above it, where:\n\n[x, y, z] are georeferenced coordinates\nr is the return number\ni is intensity\nc is class\n\nAdditional data: R:G:B, scan direction"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#lidar-point-cloud-preview",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#lidar-point-cloud-preview",
    "title": "Point cloud data analysis",
    "section": "Lidar point cloud preview",
    "text": "Lidar point cloud preview\n\nVisible swath overlap"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#lidar-point-cloud-preview-1",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#lidar-point-cloud-preview-1",
    "title": "Point cloud data analysis",
    "section": "Lidar point cloud preview",
    "text": "Lidar point cloud preview\n\nPoints distributed throughout canopy\nNo points on the wall of the building"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#sfm-derived-point-cloud-data",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#sfm-derived-point-cloud-data",
    "title": "Point cloud data analysis",
    "section": "SfM-derived point cloud data",
    "text": "SfM-derived point cloud data\nSet of [x, y, z, (R, G, B)] points derived from overlapping imagery using Structure from Motion technique:\n\n[x, y, z] are georeferenced coordinates\nR, G, B are Red, Green, Blue channels derived from imagery\n\nAdditional data depend on sensor"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#uas-sfm-point-cloud-preview",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#uas-sfm-point-cloud-preview",
    "title": "Point cloud data analysis",
    "section": "UAS SfM point cloud preview",
    "text": "UAS SfM point cloud preview\n\nOnly top of tree canopy captured\nBuilding densely sampled including the wall\n\n\nUAS SfM point cloud preview\n\nMuch higher density of points with R:G:B included"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#point-cloud-data-formats",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#point-cloud-data-formats",
    "title": "Point cloud data analysis",
    "section": "Point cloud data formats",
    "text": "Point cloud data formats\n\nASCII x,y,z, … format - older data\nBinary LAS format (header, record information, x,y,z,i, … ), industry lidar data exchange format\nCompressed LAZ format\nProprietary formats, especially for waveform data\n\nLearn more at ASPRS LAS1.4 Specification\nand USGS Lidar Base Specification"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#point-cloud-data-processing",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#point-cloud-data-processing",
    "title": "Point cloud data analysis",
    "section": "Point cloud data processing",
    "text": "Point cloud data processing\n\nPreview and analysis of point distribution\nFiltering outliers\nBare earth point extraction\nClassification: canopy, buildings …\nDecimation (point cloud thinning)\nTransformation to surfaces or 3D objects"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#analysis-of-point-distribution",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#analysis-of-point-distribution",
    "title": "Point cloud data analysis",
    "section": "Analysis of point distribution",
    "text": "Analysis of point distribution\nBinning: point statistics for each grid cell at selected resolution\n\nNumber of points per grid cell - map of point densities\nRange, stddv of z-values - map of within cell vertical variability\nIdentify data gaps, potential for artifacts\nUse to select appropriate supported resolution for DEM"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#analysis-of-point-distribution-lidar",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#analysis-of-point-distribution-lidar",
    "title": "Point cloud data analysis",
    "section": "Analysis of point distribution: lidar",
    "text": "Analysis of point distribution: lidar\nIncreased densities along swath overlaps or close to terrestrial station position\n  \nCounty-wide 2013 lidar: all returns and bare earth, terrestrial lidar"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#analysis-of-point-distribution-lidar-1",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#analysis-of-point-distribution-lidar-1",
    "title": "Point cloud data analysis",
    "section": "Analysis of point distribution: lidar",
    "text": "Analysis of point distribution: lidar\nChange in pattern along swath overlaps\n \nMidpines: number of points per 1m grid cell: for all returns and ground"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#analysis-of-point-distribution-sfm",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#analysis-of-point-distribution-sfm",
    "title": "Point cloud data analysis",
    "section": "Analysis of point distribution: SfM",
    "text": "Analysis of point distribution: SfM\nHigh point densities around trees and building edges\n\nMidpines: number of SfM-derived points per 1m grid cell"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#analysis-of-within-cell-z-range",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#analysis-of-within-cell-z-range",
    "title": "Point cloud data analysis",
    "section": "Analysis of within cell z-range",
    "text": "Analysis of within cell z-range\nMaps of z-values range within 3m grid cell\n \nMidpines z-range lidar and UAS, lidar provides better data about the trees than the denser UAS point cloud"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#outliers",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#outliers",
    "title": "Point cloud data analysis",
    "section": "Outliers",
    "text": "Outliers\n\nLidar: birds, particles, material properties\nSfM: errors in point matching\nFiltered by using local z-min, z-max or range thresholds\n\n\nCentennial Parkway - outlier present even in processed data"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#bare-ground-and-feature-extraction",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#bare-ground-and-feature-extraction",
    "title": "Point cloud data analysis",
    "section": "Bare ground and feature extraction",
    "text": "Bare ground and feature extraction\n\nMultiple returns help but not necessary\nFeature or surface needs to be sampled by sufficient number of points\nMultiscale curvature-based algorithm by Evans and Hudak\nProgressive morphological filter by Zhang\n\n\nMidpines: above ground point cloud from lidar by MCC in GRASS"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#decimation",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#decimation",
    "title": "Point cloud data analysis",
    "section": "Decimation",
    "text": "Decimation\n\nThinning of point cloud - subsampling\nReduces the point cloud size - easier to manage data\nThinning threshold should be based on features that need to be preserved\nCount-based decimation: preserves variations in density\nGrid-based decimation: removes variations in density\nDistance and geometry based decimation: more computationally intensive"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#decimation-count-based",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#decimation-count-based",
    "title": "Point cloud data analysis",
    "section": "Decimation: count-based",
    "text": "Decimation: count-based\n \nPreserves relative point densities"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#decimation-grid-based",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#decimation-grid-based",
    "title": "Point cloud data analysis",
    "section": "Decimation: grid-based",
    "text": "Decimation: grid-based\n \nRemoves variations in point densities"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#computing-dem-binning",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#computing-dem-binning",
    "title": "Point cloud data analysis",
    "section": "Computing DEM: binning",
    "text": "Computing DEM: binning\n\nPer cell statistics: mean, min, max, or nearest point z-value\nSufficient for many applications\nNo need to import the points, on-fly raster generation\nMay be noisy, with no-data areas"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#computing-dem-tin",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#computing-dem-tin",
    "title": "Point cloud data analysis",
    "section": "Computing DEM: TIN",
    "text": "Computing DEM: TIN\nMeshes are standard in 3D engineering and design systems:\n\nVariable resolution based on terrain complexity\nVariable level of detail visualization\n2D triangulation leads to TIN geometry not optimal for 3D, e.g. triangles on roads, artificial dams in valleys\nHarder to combine with other geospatial data\nLimited analytics available\nHarder to share - limited exchange formats"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#computing-dem-interpolation-to-raster",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#computing-dem-interpolation-to-raster",
    "title": "Point cloud data analysis",
    "section": "Computing DEM: interpolation to raster",
    "text": "Computing DEM: interpolation to raster\n\nSupports resolution higher than point density\nResults depend on the method used, but most methods work because of high point densities\nHigh resolution raster DEMs can be massive - works for most analytics, converts to TIN for 3D visualization\nEasy to share"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#jockeys-ridge-lidar-1999",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#jockeys-ridge-lidar-1999",
    "title": "Point cloud data analysis",
    "section": "Jockey’s Ridge lidar 1999",
    "text": "Jockey’s Ridge lidar 1999\nBinning at 1m resolution: many NULL cells"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#jockeys-ridge-lidar-1999-1",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#jockeys-ridge-lidar-1999-1",
    "title": "Point cloud data analysis",
    "section": "Jockey’s Ridge lidar 1999",
    "text": "Jockey’s Ridge lidar 1999\nBinning at 3m resolution"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#jockeys-ridge-lidar-1999-2",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#jockeys-ridge-lidar-1999-2",
    "title": "Point cloud data analysis",
    "section": "Jockey’s Ridge lidar 1999",
    "text": "Jockey’s Ridge lidar 1999\nInterpolation at 1m resolution\n\nYou can try TIN for comparison - provide data"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#midpines-uas-sfm",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#midpines-uas-sfm",
    "title": "Point cloud data analysis",
    "section": "Midpines UAS SfM",
    "text": "Midpines UAS SfM\nLow density TIN"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#midpiness-uas-sfm",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#midpiness-uas-sfm",
    "title": "Point cloud data analysis",
    "section": "Midpines’s UAS SfM",
    "text": "Midpines’s UAS SfM\nHigh density TIN"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#midpiness-uas-sfm-1",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#midpiness-uas-sfm-1",
    "title": "Point cloud data analysis",
    "section": "Midpines’s UAS SfM",
    "text": "Midpines’s UAS SfM\nSmoothed high density TIN"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#midpines-uas-sfm-1",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#midpines-uas-sfm-1",
    "title": "Point cloud data analysis",
    "section": "Midpines UAS SfM",
    "text": "Midpines UAS SfM\nHigh density point cloud imported to GRASS GIS and interpolated by spline method"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#midpines-interpolated-dsm",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#midpines-interpolated-dsm",
    "title": "Point cloud data analysis",
    "section": "Midpines interpolated DSM",
    "text": "Midpines interpolated DSM\nLidar and UAS SfM based DSM"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#topographic-analysis",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#topographic-analysis",
    "title": "Point cloud data analysis",
    "section": "Topographic analysis",
    "text": "Topographic analysis\nDeriving topographic parameters from point cloud based DEMs has challenges:\n\nDEMs are often noisy and parameters can reflect noise or scan pattern rather than actual topography\nHigh resolution leads to representation of landforms by 10s or 100s of points or grid cells\nStandard topographic analysis using 3x3 neighborhood leads to noisy patterns of topographic parameters or bias towards point distribution pattern"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#topographic-analysis-using-splines",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#topographic-analysis-using-splines",
    "title": "Point cloud data analysis",
    "section": "Topographic analysis using splines",
    "text": "Topographic analysis using splines\nSimultaneous computation of parameters with interpolation\n\nParameters derived from original points rather than raster\nExplicit equations for partial derivatives: RST\nTens or hundreds of points can be used\nTuning the level of detail by tension and smoothing parameters\n\n\n\n\n\n\n\nNote\n\n\nReference\n\nH.Mitasova, H., Mitas, L. and Harmon, R.S., 2005, Simultaneous spline interpolation and topographic analysis for lidar elevation data: methods for Open source GIS, IEEE GRSL 2(4), pp. 375- 379."
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#topographic-analysis-using-splines-1",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#topographic-analysis-using-splines-1",
    "title": "Point cloud data analysis",
    "section": "Topographic analysis using splines",
    "text": "Topographic analysis using splines\nTuning the level of detail with tension parameter"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#topographic-analysis-using-splines-2",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#topographic-analysis-using-splines-2",
    "title": "Point cloud data analysis",
    "section": "Topographic analysis using splines",
    "text": "Topographic analysis using splines\nTuning the level of detail with tension parameter\n\nProfile curvature and slope maps draped over 1m res. DEM"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#vertical-point-cloud-analysis",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#vertical-point-cloud-analysis",
    "title": "Point cloud data analysis",
    "section": "Vertical point cloud analysis",
    "text": "Vertical point cloud analysis\nVoxel-based point analysis and 3D fragmentation index"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#vertical-point-cloud-analysis-1",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#vertical-point-cloud-analysis-1",
    "title": "Point cloud data analysis",
    "section": "Vertical point cloud analysis",
    "text": "Vertical point cloud analysis\n3D visualization of vertical fragmentation index cross-sections"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#mamoth-cave-park-data",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#mamoth-cave-park-data",
    "title": "Point cloud data analysis",
    "section": "Mamoth Cave Park: data",
    "text": "Mamoth Cave Park: data\n\nClassified point cloud in las format\nRaw full waveform in lwv format\nImagery"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#mamoth-cave-park-canopy",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#mamoth-cave-park-canopy",
    "title": "Point cloud data analysis",
    "section": "Mamoth Cave Park: canopy",
    "text": "Mamoth Cave Park: canopy"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#mamoth-cave-park-bare-earth",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#mamoth-cave-park-bare-earth",
    "title": "Point cloud data analysis",
    "section": "Mamoth Cave Park: bare earth",
    "text": "Mamoth Cave Park: bare earth"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#voxel-models",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#voxel-models",
    "title": "Point cloud data analysis",
    "section": "Voxel models",
    "text": "Voxel models\n \n\n\n\n\n\n\nNote\n\n\n\nPetras, V; Petrasova, A; Jeziorska, J; Mitasova, H, 2016, Processing UAV and lidar point clouds in GRASS GIS, ISPRS Archives."
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#advances-in-lidar-data-acquisition",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#advances-in-lidar-data-acquisition",
    "title": "Point cloud data analysis",
    "section": "Advances in lidar data acquisition",
    "text": "Advances in lidar data acquisition\n\nWaveform, single photon and multispectral lidar\nVelodyne (lidar array - small and light)\nLidar is available for large UAS and helicopters, new small systems are still being tested for accuracy"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#lidar-data-sources",
    "href": "course/topics/topic_4_GIS_analytics/lectures/lecture_4b.html#lidar-data-sources",
    "title": "Point cloud data analysis",
    "section": "Lidar data sources",
    "text": "Lidar data sources\nPublic data sources (see the links here):\n\nNational map elevation data - used to be CLICK: raw point clouds usually in LAS format\nNOAA Digital Coast: coastal point clouds with on-fly binning\nNC Floodplain Mapping: bare Earth: points, 20ft DEM and 50ft DEM with carved channels\nNC data portal QL2 lidar and derived products\nOpenTopography: NCALM data\n\nMore about lidar in GRASS at https://grasswiki.osgeo.org/wiki/LIDAR"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/assignments/assignment_1a_grass.html#task",
    "href": "course/topics/topic_3_flight_planning/assignments/assignment_1a_grass.html#task",
    "title": "Assignment 3",
    "section": "Task",
    "text": "Task\nIn order to execute a safe flight, you need to make sure that the flight path will not cross any obstacles. Also remember that the ground altitude is measured in the take off localization and does not take into account local relief. It can be crucial in the mountainous areas. In our case trees, buildings and other man-made structures are more of a concern. Because both – terrain data (from lidar) and planned flight path are available, the elevation difference between the flight ceiling and the surface can be computed using GIS tools.\nIf you are unfamiliar with GRASS GIS - visit the OSgeoLab youtube channel and watch at least the Getting started with GRASS GIS GUI video.\n\nGeneral workflow:\n\nCompute a raster for elevation of the flight path\nGenerate DSM\nCalculate the elevation difference between the flight path and surface\n\n\n\nData\n\nGRASS Location (Lake_Wheeler_NCspm)\nMid Pines 2013 lidar data from las\nMid Pines 2013 lidar data from copc.laz\nFlight Path (KML)\n\n\n\nSoftware\n\nGRASS GIS\n\n\n\nCompute a raster for elevation of the flight path\nFor this analysis you can run GRASS GIS with the Lake_Wheeler_NCspm location. Create a new mapset to manage your flight analysis. *Copy the unzipped location folder into your grassdata folder (in Windows in Documents)\nREMEMBER\n\n\n\n\n\n\nChange working directory\n\n\n\nSettings &gt; GRASS working environment &gt; Change working directory &gt; select/create any directory\nNow you can use the commands from the assignment requiring the text file without the need to specify the full path to the file\n\n\n\nDownload the flight path kml file (there is a file from another flight here, you can use it as a comparison), copy the file(s) to your working directory folder.\nLaunch GRASS\nImport downloaded kml into GRASS with v.import, which will reproject the input on the fly:\n\n\n\nv.import input=flight_path1.kml output=flight_track_kml\n\n\nConvert vector line to elevation points. Set the maximum distance between the points as 0.3 m\nv.to.points -i input=flight_track_kml output=flight_track_points dmax=0.3\nConvert vector to raster after setting the region to study area and 1m resolution\ng.region vect=flight_track_kml res=1\nv.to.rast input=flight_track_points output=flight_path_altitude_rast use=z\nMake the flight path line thick (wide) enough taking to account the wingspan of the UAV and weather impacts on the flight trajectory. Desired width of the path = 5m (radius 2.5m)\nr.grow -m input=flight_path_altitude_rast output=flight_path_wide radius=2.5\n\n\nGenerate DSM\n\nDownload lidar data for Mid Pines to your computer\nSet region to your study area using g.region or using the zoom function for setting the region\nImport lidar data to GRASS\n\nWindows\nr.in.lidar mid_pines_spm_2013.las output=mid_pines_surface_max method=max class_filter=1,2 return_filter=first resolution=3 -oe \nMac/Linux\nr.in.pdal mid_pines_spm_2013.las output=mid_pines_surface_max method=max class_filter=1,2 return_filter=first resolution=3 -oe \n\nNOTE - If you have any issues with the command r.in.lidar - you can download the interpolated DSMs for Mid Pines and use r.unpack to unpack the rasters.\n\n\n\nCalculate the elevation difference between the flight path and surface\nUse map algebra to compute difference between two rasters - elevation of your flight path and Surface of the terrain\nr.mapcalc expression=\"elev_diff = flight_path_wide - mid_pines_surface_max\"\nUse the acquired diffrence map to evaluate your flight plan, pay close attention to the obstacle free zones.\n\n\nVisualize results\nYou can use screenshots from your flight planning software of choice and GRASS. If you discovered any obstacles and needed to readjust the flight path.\n\ninclude the picture and description of the issue. You can display the orthophoto, COA boundaries for better understanding of the terrain. Sample orthophotos (for the Mid Pines area) can be downloaded from the folder - (Use r.unpack to unpack the rasters)\n\n\n\nExplain your choice of the flight area, parameters and takeoff and landing locations based on the performed analysis\nThe aim is to prove that the flight plan can be used to execute a safe and sucessful flight mission. Describe your thought process - what did you take into account first, what problems did you encouter? Did you need to change your initial plans based on software limitations, legal issues or the relief of the targeted area?"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#objectives",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#objectives",
    "title": "Flight Planning",
    "section": "Objectives",
    "text": "Objectives\n\n\nPhases of flight planning\nSafety procedures and checklists\nGround Control Points and its distirbution and accuracy\nFlight planning software"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#uas-photogrammetric-process",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#uas-photogrammetric-process",
    "title": "Flight Planning",
    "section": "UAS Photogrammetric process",
    "text": "UAS Photogrammetric process\n\nThroughout the whole process, it is important to remember:\n\nWhat is the aim or the project?\nWhat will be the data used for?"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#project-definition",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#project-definition",
    "title": "Flight Planning",
    "section": "Project Definition",
    "text": "Project Definition\n\n\nDefining the scope of the project\nChoosing UAS and sensor\nAssessing the cost, labor, and time consumption\nCollecting information about terrain"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#project-definition-1",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#project-definition-1",
    "title": "Flight Planning",
    "section": "Project Definition",
    "text": "Project Definition\n\n\nDefine the area and resolution based on:\n\nUAS and sensor capabilities\nSpatial extent of the mapping area\nTerrain constraints\nProject requirements"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#project-definition-2",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#project-definition-2",
    "title": "Flight Planning",
    "section": "Project Definition",
    "text": "Project Definition\n\n\nEvaluate the legal constraints, obtain permission\nDefining the coordinate system:\n\nDependent on the desired coordinate system of the final data\nConsistent with the coordinate system of GCP survey"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#flight-planning",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#flight-planning",
    "title": "Flight Planning",
    "section": "Flight planning",
    "text": "Flight planning\n\n\nMission area assessment\nPlanning geometric parameters\nChoosing flight planning and flight logging platform\nPreliminary weather assessment (climate, season, forecasts)\nCreating a flight plan (software specific)"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#placing-ground-control-points-gcps",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#placing-ground-control-points-gcps",
    "title": "Flight Planning",
    "section": "Placing Ground Control Points (GCPs)",
    "text": "Placing Ground Control Points (GCPs)\n\n\n\nA minimum number of 5 GCPs is recommended.\n5 to 10 GCPs are usually enough, even for large projects.\nIn cases that the topography of the area is complex, use more GCPs\nThe GCPs should be distributed evenly in the area\nDo not place the GCPs exactly at the edges of the area"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#placing-ground-control-points-gcps-1",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#placing-ground-control-points-gcps-1",
    "title": "Flight Planning",
    "section": "Placing Ground Control Points (GCPs)",
    "text": "Placing Ground Control Points (GCPs)\n\n\nBefore measuring the GCPs coordinates, the following items must be defined:\n\nGCP coordinate system\nGCP accuracy\nTopographic equipment (total station or hand held GPS?)"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#gcps-accuracy",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#gcps-accuracy",
    "title": "Flight Planning",
    "section": "GCPs Accuracy",
    "text": "GCPs Accuracy\n\n\nFactors for defining GCP accuracy:\n\nAccuracy needed for the final results\nGround Sampling Distance* of the images:\n\nGCP target size: 5-10 x\nGCP accuracy: at least 0.1 GSD\n\n\n\n\n\nDistance between two consecutive pixel centers measured on the ground"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#ground-sampling-distance",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#ground-sampling-distance",
    "title": "Flight Planning",
    "section": "Ground Sampling Distance",
    "text": "Ground Sampling Distance\n\n\nDistance between two consecutive pixel centers measured on the ground:\nThe formula to calculate GSD is:\n\\[\n\\text{GSD} = \\frac{\\text{A} \\times \\text{S}}{\\text{F} \\times \\text{D}}\n\\]\nWhere:\n\nA: Flight Altitude (m)\nS: Sensor(hight, width) (mm)\nF: Focal Length (mm)\nD: Image Dimensions (height, width) (pixels)\n\nBigger GSD = lower spatial resolution\nSmaller GSD = higher spatial resolution"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#site-evaluation-and-checklists",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#site-evaluation-and-checklists",
    "title": "Flight Planning",
    "section": "Site evaluation and checklists",
    "text": "Site evaluation and checklists\n\n\nTerrain check – high obstacles in the take-off, mission, landing, and alternative landing locations\nAsk the locals about possible air traffic or ground activities\nWeather check\n\nTemperature affects battery life\nMost of the UAS can’t operate in rain\n\nUse checklists, don’t rely on your memory!\n\nSample checklists: paper (for Phantom) and RMUS app"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#site-evaluation-and-checklists-1",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#site-evaluation-and-checklists-1",
    "title": "Flight Planning",
    "section": "Site evaluation and checklists",
    "text": "Site evaluation and checklists"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#site-evaluation-and-checklists-2",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#site-evaluation-and-checklists-2",
    "title": "Flight Planning",
    "section": "Site evaluation and checklists",
    "text": "Site evaluation and checklists\nPreflight inspection is required under Part 107.49;\n\nRemote Pilot in Command (RPIC) is required to develop a preflight inspection checklist if the manufacturer has not developed one.\n\n\nNCDOT Unmanned Aircraft Systems Program\n\nThe checklist is usually integrated into the UAS flight software or can be obtained from the UAS vendor\nIn case that is not available, a standard Flight Checklist should be made and followed by the flight crew\n\n\n\n\n\nNote: As of Dec. 1, 2024, North Carolina no longer require commercial and government drone operators to obtain an N.C. permit"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#flight-control",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#flight-control",
    "title": "Flight Planning",
    "section": "Flight control",
    "text": "Flight control\n\n\nThe UAS RPIC should launch, operate, and recover from preset locations so that the aircraft will fly according to the mission plan.\nVisual Line of Sight (VLOS) - the flight crew should have a clear view of the aircraft at all times, called .\nObservation locations should be selected for the maximum line of sight throughout the planned flight operations area (Part 107.31)."
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#flight-control-1",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#flight-control-1",
    "title": "Flight Planning",
    "section": "Flight control",
    "text": "Flight control\n\n\nRP (Remote Pilot), PMC (Person Manipulating the Flight Controls), and VO (Visual Observer) (if used)\n\nmust be able to maintain effective communication with each other at all times (Part 107.33).\n\nUpon any failure during the flight or any loss of visual contact with the UAS, the RPIC should command the aircraft back to the recovery location or utilize the built-in fail-safe features to recover the aircraft."
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#lake-wheeler---imagery",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#lake-wheeler---imagery",
    "title": "Flight Planning",
    "section": "Lake Wheeler - Imagery",
    "text": "Lake Wheeler - Imagery"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#lake-wheeler---aeronautical-chart",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#lake-wheeler---aeronautical-chart",
    "title": "Flight Planning",
    "section": "Lake Wheeler - Aeronautical chart",
    "text": "Lake Wheeler - Aeronautical chart"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#lake-wheeler-test-site",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#lake-wheeler-test-site",
    "title": "Flight Planning",
    "section": "Lake Wheeler test site",
    "text": "Lake Wheeler test site"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#flight-planning-software",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#flight-planning-software",
    "title": "Flight Planning",
    "section": "Flight planning software",
    "text": "Flight planning software\n\nMultiple available platforms\nSome are dedicated for specific UAS and sold with the system by the manufacturer\n\n\n\nCommerical\n\nDroneDeploy\nPix4Dcapture Pro\nDJI GS PRO\nUgCS\nDroneLink\n\n\nOpen Source\n\nQGroundControl\nMission Planner"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#how-to-plan-a-mapping-flight",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#how-to-plan-a-mapping-flight",
    "title": "Flight Planning",
    "section": "How to plan a mapping flight?",
    "text": "How to plan a mapping flight?\nUAS Photogrammetric process"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#location",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#location",
    "title": "Flight Planning",
    "section": "Location",
    "text": "Location\n\nFly larger extent than you need.\nThink about the area you need for analysis"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#camera-specs",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#camera-specs",
    "title": "Flight Planning",
    "section": "Camera Specs",
    "text": "Camera Specs\nGeneral camera setting are usally fine."
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#optional-settings",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#optional-settings",
    "title": "Flight Planning",
    "section": "Optional Settings",
    "text": "Optional Settings\n\nMechanical Shutter: On\nFocus: inf\nShutter Priority: 1/800\nAspect Ratio: 3/2"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#camera-angle",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#camera-angle",
    "title": "Flight Planning",
    "section": "Camera Angle",
    "text": "Camera Angle\n\n\nNadir (Straight down)\n\nMost mapping\n\n\nOblique (Pitched)\n\n70-80 degrees\nImproved 3D\nBuildings\nRough Terrain"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#altitude",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#altitude",
    "title": "Flight Planning",
    "section": "Altitude",
    "text": "Altitude\n\n\n\nImpacts the flight path\nAGL = Above Ground Level\nAltitude (AGL)\n\n70m - 120m\n\nTerrain Aware Fligh Path\n\n\nAltitude 70m  Altitude 120m"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#flight-patterns",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#flight-patterns",
    "title": "Flight Planning",
    "section": "Flight Patterns",
    "text": "Flight Patterns\n\n\nNormal\n\n\n\nLawnmower\n\n\n\nCrosshatch Pattern \n\nMore detail\nLonger flight times"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#overlap",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#overlap",
    "title": "Flight Planning",
    "section": "Overlap",
    "text": "Overlap\n\nFront and Side\n\nMin Overlap 60% forward 40% Laterial (Side)\nHomogeneous Terrian &gt; Overlap\n\n\n\n\n\n\nCaution\n\n\nHow will overlap impact your flight path?"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#drone-speed",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#drone-speed",
    "title": "Flight Planning",
    "section": "Drone Speed",
    "text": "Drone Speed\n~30 km/hr\n\nLighting\nCamera Shutter\nAltitude\nMotion Blur"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#lighting",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#lighting",
    "title": "Flight Planning",
    "section": "Lighting",
    "text": "Lighting\n\nOvercast\nNoon\nAvoid Shadows\nPartly Cloudly"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#summary",
    "href": "course/topics/topic_3_flight_planning/lectures/lecture_3a.html#summary",
    "title": "Flight Planning",
    "section": "Summary",
    "text": "Summary\n\nPhases of UAS flight planning\nSafety procedures and checklists for safe flight operations\nHow does our study site look like\nHow to plan a flight"
  },
  {
    "objectID": "course/topics/topic_2_sfm/assignments/assignment_2a.html",
    "href": "course/topics/topic_2_sfm/assignments/assignment_2a.html",
    "title": "Assignment 2A",
    "section": "",
    "text": "Perform an experiment to investigate the impact of the number and orientation of photos, light, as well as the shape and features of a photographed object on its 3D reconstruction from multiple overlapping imagery.\nSelect an object in your office, at home, or outside, take multiple overlapping images of this object with your camera and create its 3D digital model using appropriate software. Make sure that you do not change the zoom during the capture (unless you want to test what impact it would have). You can also compare the models derived from a high-end digital camera and a smartphone.\nTake the photos from various locations and place features on the physical model to explore the best approach for obtaining a model with minimal distortions. Use any suitable software, for example:\n\nReality Scan - Unreal Engine Reality Capture\nColmap (see below) - free and open-source software.\nMeshroom (AliceVision) (Window and Linux) - free and open-source software.\nReCap (Windows only) 30-days free trial is available and students can get the software for free for 3 years.\nRegard3D (Windows and Mac) - free and open-source software.\nAgisoft Metashape 2.1.2 (which we will be using later for aerial imagery processing).\nPolycam (Mobile app) - with try for free online option. \n\n\n  Example 3D model obtained from 20 photos using catch123 from Autodesk (this software has been discontinued and Autodesk replaced it with ReCap) DEM interpolated in GIS from the point cloud exported from 123D Catch\n\n\nPhoto capturing guidelines\nSource: Agisoft Photoscan Professional User Manual, modified\n\nUse a digital camera with reasonably high resolution (5 MPix or more);\nAvoid ultra-wide angle and fisheye lenses;\nFixed lenses are preferred. If zoom lenses are used - focal length should be set either to maximal or to minimal value during the entire shooting session for more stable results;\nISO should be set to the lowest value, otherwise high ISO values will induce additional noise to images;\nIt is important to capture sharp, not blurred photos;\nAvoid not textured, shiny, highly reflective or transparent objects;\nAvoid unwanted foregrounds;\nAvoid moving objects within the scene to be reconstructed;\nAvoid absolutely flat objects or scenes;\nNumber of photos: more than required is better than not enough;\nGood lighting is required to achieve better quality of the results, yet blinks should be avoided. It is recommended to remove sources of light from camera fields of view. Avoid using flash;\nEach photo should effectively use the frame size: object of interest should take up the maximum area. In some cases, portrait camera orientation should be used;\nDo not try to place the full object in the image frame, if some parts are missing it is not a problem providing that these parts appear on other images.\n\n\n\nAdvice on appropriate capturing scenarios:\n\n \n\n\n\nCOLMAP\nCOLMAP is a general-purpose Structure-from-Motion (SfM) and Multi-View Stereo (MVS) pipeline with a graphical and command-line interface. COLMAP is open source (GNU GPL) and can be installed on all major platforms.\nBinaries can be downloaded from COLMAP website. Note: Dense reconstruction step requires NVIDIA graphics card with CUDA installed. Depending on your computer configuration, install binaries with or without CUDA. Once downloaded, unzip and run COLMAP.bat (on Windows). This should start COLMAP’s graphical user interface.\nIn this video tutorial (no audio), we upload around 20 images of a scaled physical model to a folder and run all steps of the reconstruction pipeline. Notice we can inspect keypoints, matches, and links between cameras and reconstructed points.\n\nNote 1: If you don’t have CUDA, you need to stop before the dense reconstruction step.\n\n\nNote 2: If you have CUDA, stereo reconstruction can crash COLMAP, which is due to so-called GPU timeouts; More information can be found here."
  },
  {
    "objectID": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#outline",
    "href": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#outline",
    "title": "B. Imagery processing and structure from motion (SfM)",
    "section": "Outline",
    "text": "Outline\n\naerial imagery distortions\northorectification process\ninterior and exterior orientation\nBundle Block Adjustment\nGround control points (GCP)\nImage mosaic and point cloud results\nStructure from motion (SfM)"
  },
  {
    "objectID": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#lecture",
    "href": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#lecture",
    "title": "B. Imagery processing and structure from motion (SfM)",
    "section": "Lecture",
    "text": "Lecture\n\nLecture Slides: Photogrammetry and SfM\nLecture Recording: Fall 2024 (NCSU Only)\nAssignment Recordign: Fall 2024"
  },
  {
    "objectID": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#supplemental-materials",
    "href": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#supplemental-materials",
    "title": "B. Imagery processing and structure from motion (SfM)",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nLectures and demos from the 2021 Geological Society of America short course:\n\n“Introduction to Structure from Motion Photogrammetry”\nOpenTopography YouTube Course\n\nY. Furukawa and C. Hernández . Multi-View Stereo: A Tutorial. Foundations and TrendsR in Computer Graphics and Vision, vol. 9, no. 1-2, pp. 1–148, 2013."
  },
  {
    "objectID": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#assignment",
    "href": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#assignment",
    "title": "B. Imagery processing and structure from motion (SfM)",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nGeoprocessing UAS imagery in Agisoft Metashape"
  },
  {
    "objectID": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#homework",
    "href": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#homework",
    "title": "B. Imagery processing and structure from motion (SfM)",
    "section": "Homework",
    "text": "Homework\nPrepare report on generating orthomosaic and Digital Surface Model using images taken by the UAS Trimble UX5 Rover and performing the processing in Agisoft Metashape. Explain the report generated by Agisoft including the data accuracy."
  },
  {
    "objectID": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#structure-from-motion-example",
    "href": "course/topics/topic_2_sfm/part_b_img_processing_sfm.html#structure-from-motion-example",
    "title": "B. Imagery processing and structure from motion (SfM)",
    "section": "Structure from Motion Example",
    "text": "Structure from Motion Example\n\nheight = 600;\nwidth = 800;\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.130.0/build/three.min.js\");\n  await require(\"three@0.130.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  await require(\"three@0.130.0/examples/js/loaders/OBJLoader.js\").catch(() =&gt; {});\n  await require(\"three@0.130.0/examples/js/loaders/GLTFLoader.js\").catch(() =&gt; {});\n  return THREE;\n}\n\nSimplexNoise = {\n    const SimplexNoise = window.SimplexNoise = await require(\"simplex-noise@2.4.0/simplex-noise.js\");\n    return SimplexNoise;\n}\n\n// Create the renderer\nrenderer = {\n  const renderer = new THREE.WebGLRenderer({ antialias: true });\n  renderer.setSize(width, height);\n  return renderer;\n}\n\n// Set up scene, camera, and light\nscene = new THREE.Scene();\ncamera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);\ncamera.position.set(0, 15, 15);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscene.add(new THREE.AmbientLight(0xffffff, 0.5));\n\n\n\n\n\n\n\ndirectionalLight = new THREE.DirectionalLight(0xffffff, 1);\ndirectionalLight.position.set(5, 5, 5).normalize();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscene.add(directionalLight);\n\n\n\n\n\n\n\ncontrols = {\n  const controls = new THREE.OrbitControls(camera, renderer.domElement);\n  controls.enableDamping = true;\n  controls.dampingFactor = 0.25;\n  controls.enableZoom = true;\n  return controls;\n}\n\n// Create the terrain using Simplex Noise\nterrainGeometry = {\n  const widthSegments = 100;\n  const heightSegments = 100;\n  const geometry = new THREE.PlaneGeometry(10, 10, widthSegments, heightSegments);\n  \n  const simplex = new SimplexNoise();\n  const vertices = geometry.attributes.position.array;\n\n  // Modify the z-coordinates of the vertices using Simplex Noise\n  for (let i = 0; i &lt; vertices.length; i += 3) {\n    const x = vertices[i];\n    const y = vertices[i + 1];\n    vertices[i + 2] = simplex.noise2D(x * 0.3, y * 0.3) / 2.0; // Reduce amplitude\n  }\n  \n  geometry.computeVertexNormals(); // Recompute normals after modifying vertices\n  return geometry;\n}\n\n// Create the terrain mesh\nterrain = {\n  const material = new THREE.MeshStandardMaterial({ color: 0x228b22, wireframe: true });\n  const mesh = new THREE.Mesh(terrainGeometry, material);\n  mesh.rotation.x = -Math.PI / 2; // Rotate to make horizontal\n  scene.add(mesh);\n  return mesh;\n}\n\n// Anchor points on the terrain surface, adjusted for terrain rotation\nanchorPoints = {\n  const positions = terrainGeometry.attributes.position.array;\n  const points = [];\n  \n  for (let i = 0; i &lt; 50; i++) { // Create 50 random anchor points\n    const index = Math.floor(Math.random() * (positions.length / 3)) * 3;\n    const point = new THREE.Vector3(positions[index], positions[index + 1], positions[index + 2]);\n    // Correct for terrain rotation\n    point.applyMatrix3(terrain.matrixWorld);\n    points.push(point);\n  }\n\n  const anchorMaterial = new THREE.PointsMaterial({ color: 0xff0000, size: 0.25 });\n  const anchorGeometry = new THREE.BufferGeometry().setFromPoints(points);\n  const anchorMesh = new THREE.Points(anchorGeometry, anchorMaterial);\n  scene.add(anchorMesh);\n  \n  return points;\n}\n\n// Create the line material and add the lines to the scene\nuavlineMaterial = {\n    const uavlineMaterial = new THREE.LineDashedMaterial({\n    color: 0xff0000, // Red color\n    dashSize: 0.1, // Length of dashes\n    gapSize: 0.1, // Length of gaps between dashes\n  });\n  return uavlineMaterial;\n}\n\n// UAV path and camera representation, with paths as lines\nuavCameras = {\n  const uavPath1 = [];\n  \n  \n  const stepSize = 0.25;\n  const rowDistance = 0.25;\n  const startX = -5, endX = 5, startZ = -5, endZ = 5;\n  const startY = 3;\n  const endY = 3;\n  let direction = 1;\n\n  for (let z = startZ; z &lt;= endZ; z += rowDistance) {\n    if (direction === 1) {\n      for (let x = startX; x &lt;= endX; x += stepSize) {\n        uavPath1.push(new THREE.Vector3(x, startY, z));\n      }\n    } else {\n      for (let x = endX; x &gt;= startX; x -= stepSize) {\n        uavPath1.push(new THREE.Vector3(x, endY, z));\n      }\n    }\n    direction *= -1;\n  }\n\n  const uavLineGeometry = new THREE.BufferGeometry();\n  uavLineGeometry.setFromPoints(uavPath1);\n  const uavLine1 = new THREE.Line(uavLineGeometry, uavlineMaterial);\n  uavLine1.computeLineDistances(); // Required for dashed lines\n  scene.add(uavLine1);\n\n  // Create UAV camera mesh representations\n  const cameraMaterial1 = new THREE.MeshBasicMaterial({ color: 0xff0000 });\n\n  const cameraGeometry = new THREE.SphereGeometry(0.1, 32, 32);\n  const uavCamera1 = new THREE.Mesh(cameraGeometry, cameraMaterial1);\n\n\n  scene.add(uavCamera1);\n\n  return { uavCamera1, uavPath1, uavLine1, uavLineGeometry };\n}\n\n\n\nuavFrustum1 = {\n    const uavFrustum1 = new THREE.CameraHelper(camera);\n    // scene.add(uavFrustum1);\n    return uavFrustum1;\n}\n\n\n\nfunction createLine(pointA, pointB, color = 0xffffff) {\n    const geometry = new THREE.BufferGeometry().setFromPoints([pointA, pointB]);\n    const material = new THREE.LineBasicMaterial({ color });\n    const line = new THREE.Line(geometry, material);\n    scene.add(line);\n    return line;\n}\n\nlinesPass1 = [];\n\nlinePass1 = {\n    anchorPoints.forEach(point =&gt; {\n        linesPass1.push(createLine(uavCameras.uavPath1[0], point, 0x00ffff));\n    });\n    return linesPass1;\n}\n        // Create lines from the UAV cameras to anchor points\n\n\nuavFOVPlane = {\n     // Create a plane to represent the UAV's field of view\n    const planeGeometry = new THREE.PlaneGeometry(2, 2); // Adjust size as needed\n    const planeMaterial = new THREE.MeshBasicMaterial({ color: 0x00ff00, side: THREE.DoubleSide }); // Solid fill color\n    const uavFOVPlane = new THREE.Mesh(planeGeometry, planeMaterial);\n    scene.add(uavFOVPlane);\n    return uavFOVPlane;\n}\n\n// Animation loop, moving the UAVs along their paths\n{\n  let uavIndex = 0;\n  let lastUpdateTime = 0;\n  const updateInterval = 50; // Milliseconds between updates\n  \n  while (true) {\n    requestAnimationFrame(() =&gt; {\n      const { uavCamera1, uavPath1, uavLine1, uavLineGeometry } = uavCameras;\n      // Move UAV cameras along the path\n\n            // Move the UAV cameras along their paths\n            uavIndex = (uavIndex + 1) % uavPath1.length;\n            uavCamera1.position.copy(uavPath1[uavIndex]);\n            // uavCamera2.position.copy(uavPath2[uavIndex]);\n\n            if (uavIndex &lt; uavPath1.length - 1) {\n                uavIndex++;\n                const newPoints = uavPath1.slice(0, uavIndex + 1);\n                uavLineGeometry.setFromPoints(newPoints);\n                uavLine1.computeLineDistances(); // Required for dashed lines\n            }\n\n            // Update the positions of the lines to keep them connected to the moving UAV cameras\n            linesPass1.forEach((line, index) =&gt; {\n                const linePositions = line.geometry.attributes.position.array;\n                const anchorPoint = anchorPoints[index];\n\n                // Update the line's start point (UAV camera position)\n                linePositions[0] = uavCamera1.position.x;\n                linePositions[1] = uavCamera1.position.y;\n                linePositions[2] = uavCamera1.position.z;\n\n                // Update the line's endpoint (anchor point on the terrain)\n                linePositions[3] = anchorPoint.x;\n                linePositions[4] = anchorPoint.y;\n                linePositions[5] = anchorPoint.z;\n\n                line.geometry.attributes.position.needsUpdate = true;\n            });\n\n            // // Update the UAV's field of view plane position and orientation\n            uavFOVPlane.position.set(uavCamera1.position.x, 0, uavCamera1.position.z); // Set y to 0 to place it on the terrain surface\n            uavFOVPlane.rotation.x = -Math.PI / 2; // Rotate to align with the terrain surface\n        \n\n      controls.update(); // Update orbit controls\n      renderer.render(scene, camera); // Render scene\n    });\n    await Promises.delay(updateInterval);\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{\n  window.addEventListener('resize', () =&gt; {\n    renderer.setSize(width, height);\n    camera.aspect = width / height;\n    camera.updateProjectionMatrix();\n  });\n}\n\n\n\n\n\n\n\nrenderer.domElement"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#objectives",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#objectives",
    "title": "Imagery Processing",
    "section": "Objectives",
    "text": "Objectives\n\nUnderstand the photogrammetric data processing as a multistep process;\nIndicate data needed for orthophoto/DTM generation from aerial imagery;\nUnderstand the difference between interior and exterior orientation of the photo;\nDescribe the workflow of geoprocessing of aerial imagery in designated software (Agisoft Metashape Professional);"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#photogrammetric-process",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#photogrammetric-process",
    "title": "Imagery Processing",
    "section": "Photogrammetric process",
    "text": "Photogrammetric process"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#photogrammetric-process-1",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#photogrammetric-process-1",
    "title": "Imagery Processing",
    "section": "Photogrammetric process",
    "text": "Photogrammetric process"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#data-processing",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#data-processing",
    "title": "Imagery Processing",
    "section": "Data processing",
    "text": "Data processing"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#uas-data",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#uas-data",
    "title": "Imagery Processing",
    "section": "UAS data",
    "text": "UAS data\nWhat do we get after the flight mission?"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#digital-imagery",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#digital-imagery",
    "title": "Imagery Processing",
    "section": "Digital imagery",
    "text": "Digital imagery\n\n\nusually on the camera SD card\ncan be geotagged (depends on camera)\n\nCamera lens location is “written into” each photo’s EXIF file\nthis is not necessarily the case…"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#flight-log",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#flight-log",
    "title": "Imagery Processing",
    "section": "Flight log",
    "text": "Flight log\n\nOnboard Inertial Measurement Unit (IMU) accurately measures the orientation of airborne sensors,\nInformation is logged into a text file (flight log),\nContains elements of exterior orientation (EO, more later in the lecture)"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#gcp-coordinates",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#gcp-coordinates",
    "title": "Imagery Processing",
    "section": "GCP coordinates",
    "text": "GCP coordinates\n\n\n\nMeasured by GPS coordinates of the panels set before the flight\nPhoto ID points (distinguishable ground features) can be surveyed later on\nIt is important to know the GCPs coordinate system (spatial reference system)"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#spatial-reference-system",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#spatial-reference-system",
    "title": "Imagery Processing",
    "section": "Spatial reference system",
    "text": "Spatial reference system\n\n\n\nDefines how the two-dimensional, projected map in your GIS is related to real places on the earth\nIt is crucial to know what is your data reference system!\n\n\n\n\n\nThere are global map projections, but most map projections are created and optimized to project smaller areas of the earth’s surface\nThere are two different types of coordinate reference systems: Geographic Coordinate Systems and Projected Coordinate Systems\nSpatial reference list (EPSG codes for coordinate reference systems)"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#uas-data-processing-outputs",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#uas-data-processing-outputs",
    "title": "Imagery Processing",
    "section": "UAS data processing outputs",
    "text": "UAS data processing outputs\nWhat do we get after processing the data?"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#orthophoto",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#orthophoto",
    "title": "Imagery Processing",
    "section": "Orthophoto",
    "text": "Orthophoto\n\n\n\nAerial imagery geometrically corrected (“orthorectified”) such that the scale is uniform\nRaster: consists of red, green, and blue bands"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#digital-surface-model",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#digital-surface-model",
    "title": "Imagery Processing",
    "section": "Digital Surface Model",
    "text": "Digital Surface Model\n\nDEM/DTM - Digital Elevation Model / Digital Terrain Model\n\nRepresentation of a terrain’s elevation\nBare-earth raster grid\n\nDSM - Digital Surface Model\n\nRepresentation of a visible surface\nCaptures the natural and built features on the Earth’s surface"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#pointcloud",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#pointcloud",
    "title": "Imagery Processing",
    "section": "Pointcloud",
    "text": "Pointcloud\n\nRepresentation of the external surface of an object\nSet of vertices in a three-dimensional coordinate system\nVector or raster?\nDale Lutz once said, “point cloud is a badly behaved raster”"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#multiple-view-geometry",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#multiple-view-geometry",
    "title": "Imagery Processing",
    "section": "Multiple-view geometry",
    "text": "Multiple-view geometry\n\nScene geometry (structure):\nGiven 2D point matches in two or more images, where are the corresponding points in 3D?\nCorrespondence (stereo matching):\nGiven a point in just one image, how does it constrain the position of the corresponding point in another image?\nCamera geometry (motion):\nGiven a set of corresponding points in two or more images, what are the camera matrices for these views?"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#what-do-we-need",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#what-do-we-need",
    "title": "Imagery Processing",
    "section": "What do we need?",
    "text": "What do we need?\n\nDigital imagery;\n(Digital elevation model or topographic dataset);\nExterior orientation parameters from aerial triangulation or IMU;\n(Camera calibration report);\n(Ground Control Points parameters);\nPhotogrammetric processing software that utilizes collinearity equations.\n\n\nItems in brackets are optional"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#digital-imagery-1",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#digital-imagery-1",
    "title": "Imagery Processing",
    "section": "Digital imagery",
    "text": "Digital imagery"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#structure-from-motion-sfm",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#structure-from-motion-sfm",
    "title": "Imagery Processing",
    "section": "Structure from Motion (SfM)",
    "text": "Structure from Motion (SfM)\n\n\n\nRange imaging technique\nProcess of estimating 3D structures from 2D image sequences\nMay be coupled with local motion signals"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#exterior-orientation-eo",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#exterior-orientation-eo",
    "title": "Imagery Processing",
    "section": "Exterior orientation (EO)",
    "text": "Exterior orientation (EO)\nEO = Position and orientation in the object space\n6 elements necessary for any photogrammetric processing:\n\n\n\nX, Y, and Z of the exposure station position (latitude, longitude, and altitude of the camera)\nAngular orientation: ω, φ, and κ (yaw, pitch, and roll)"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#flight-log-1",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#flight-log-1",
    "title": "Imagery Processing",
    "section": "Flight log",
    "text": "Flight log\n\nLog file contains elements of exterior orientation that are measured by onboard Inertial Measurement Unit (IMU) and written into a text file\n\n\n\nSometimes (most DJI products) exterior orientation parameters are saved in photos’ EXIF file\nLog contains information about the location of the camera, not the location of the depicted object - more info in this section of lecture 3"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#interior-orientation",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#interior-orientation",
    "title": "Imagery Processing",
    "section": "Interior orientation",
    "text": "Interior orientation\n\n\n\nIn the past: camera calibration report\nNow: Self-calibration (auto-calibration) is the process of determining intrinsic camera parameters directly from uncalibrated images\n\n\n\n\n\nCan be automatically derived using Structure from Motion (SfM) methods"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#ground-control-points",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#ground-control-points",
    "title": "Imagery Processing",
    "section": "Ground Control Points",
    "text": "Ground Control Points\n\n\n\nGCP - Target in the project area with known 3 coordinates (X, Y, Z or lat, long, alt)\nFor more information about placing targets and importance of GCPs see this section of lecture 3\nFor more information about processing the data with GCPs see intro to the assignment"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#processing-options",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#processing-options",
    "title": "Imagery Processing",
    "section": "Processing options",
    "text": "Processing options"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#processing-options-1",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#processing-options-1",
    "title": "Imagery Processing",
    "section": "Processing options",
    "text": "Processing options\nEverything boils down to… money (and time)\n\nWhat is my starting budget and equipment?\nHow frequently will I fly?\nDo I have the experience/training necessary for processing (or am I able to hire people who do)?\nDo I have time to process the data by myself?"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#processing-options---software",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#processing-options---software",
    "title": "Imagery Processing",
    "section": "Processing options - software",
    "text": "Processing options - software\n\nAgisoft Metashape\nPix4D\nTrimble Business Center - Aerial Photogrammetry Module\nDrone2Map (ESRI)\nDroneMapper\nOpenDroneMap\nmany many more…"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2b.html#what-did-we-learn",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2b.html#what-did-we-learn",
    "title": "Imagery Processing",
    "section": "What did we learn?",
    "text": "What did we learn?\n\nWhat is a general workflow for UAS imagery processing\nHow do we transform UAS data into orthophoto, DSM, 3D model, and point cloud"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/part_b_Analysis_multitemporal_UAS_data_applications.html#outline",
    "href": "course/topics/topic_5_Advanced Analytics/part_b_Analysis_multitemporal_UAS_data_applications.html#outline",
    "title": "B. Analysis of multitemporal UAS data and its applications",
    "section": "Outline",
    "text": "Outline\n\nmonitoring landscape changes using UAS\nintroduction to temporal data framework\nbasic analysis of 3D time series\napplications: monitoring plant growth, coastal erosion, soil erosion, flooding"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/part_b_Analysis_multitemporal_UAS_data_applications.html#lecture",
    "href": "course/topics/topic_5_Advanced Analytics/part_b_Analysis_multitemporal_UAS_data_applications.html#lecture",
    "title": "B. Analysis of multitemporal UAS data and its applications",
    "section": "Lecture",
    "text": "Lecture\n\nLecture slides"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/part_b_Analysis_multitemporal_UAS_data_applications.html#supplemental-materials",
    "href": "course/topics/topic_5_Advanced Analytics/part_b_Analysis_multitemporal_UAS_data_applications.html#supplemental-materials",
    "title": "B. Analysis of multitemporal UAS data and its applications",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nSpatio-temporal data handling and visualization in GRASS GIS (FOSS4G 2014 workshop)\nGIS-based Analysis of Coastal Lidar Time-Series, Springer brief available for free through NCSU libraries\nPast Lecture recordings:\n\nIntroduction\nAnalysis"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/part_b_Analysis_multitemporal_UAS_data_applications.html#assignment-homework",
    "href": "course/topics/topic_5_Advanced Analytics/part_b_Analysis_multitemporal_UAS_data_applications.html#assignment-homework",
    "title": "B. Analysis of multitemporal UAS data and its applications",
    "section": "Assignment & Homework",
    "text": "Assignment & Homework\nPrepare report on mapping plant growth, viewsheds and water flow patterns using ultrahigh resolution DEMs.\nAssignment"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#outline",
    "href": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#outline",
    "title": "Assignment 5B",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to GRASS temporal framework\nAnalyze the time series of UAS DEMs\nEstimate crop biomas and its temporal change\nEvaluate impact of vegetation change on viewshed extent\nEstimate volume of eroded and deposited soil"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#data",
    "href": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#data",
    "title": "Assignment 5B",
    "section": "Data",
    "text": "Data\nYou should have everything already for Mid Pines area, get anything missing from the Course logistics web page:\n\nGRASS Project: LakeWheeler_NCspm\n2013 lidar interpolated DEM and DSM: download, use r.unpack.\nFields polygon as packed raster use r.unpack in your mapset to import it\nText file with timeseries"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#tools",
    "href": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#tools",
    "title": "Assignment 5B",
    "section": "Tools",
    "text": "Tools\n\nGRASS GIS 8.4\nGRASS Addon: r.patch.smooth\n\nInstall r.patch.smooth with the following command:\ng.extension r.patch.smooth"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#analyze-time-series-of-uas-dsms",
    "href": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#analyze-time-series-of-uas-dsms",
    "title": "Assignment 5B",
    "section": "Analyze time series of UAS DSMs",
    "text": "Analyze time series of UAS DSMs\n\nCreate a new mapset and copy the UAS DSMs\nStart GRASS GIS in Lake_Wheeler_NCspm Location and create a new mapset “assignment5b”. Open GRASS GIS in this mapset\ng.copy raster=2015_03_18_DSM_agi_6GCP@PERMANENT,2015_03_18_DSM_agi_6GCP\ng.copy raster=2015_06_20_DSM_agi_11GCP@PERMANENT,2015_06_20_DSM_agi_11GCP\ng.copy raster=2015_07_27_DSM_agi_2GCP_4ID@PERMANENT,2015_07_27_DSM_agi_2GCP_4ID \ng.copy raster=2015_08_14_DSM_agi_3GCP_4ID@PERMANENT,2015_08_14_DSM_agi_3GCP_4ID \ng.copy raster=2015_09_04_DSM_agi_6GCP@PERMANENT,2015_09_04_DSM_agi_6GCP\ng.copy raster=2015_09_21_DSM_agi_6GCP@PERMANENT,2015_09_21_DSM_agi_6GCP\ng.copy raster=2015_10_06_DSM_agi_8GCPs@PERMANENT,2015_10_06_DSM_agi_8GCPs\n\n\nRegister the time series in temporal framework\nFirst, create a space-time raster dataset (STRDS) with absolute temporal type:\nt.create output=uas_dsm type=strds temporaltype=absolute semantictype=mean title=\"UAS_DSM\" description=\"UAS-based 2015 DSMs processed by Agisoft\"\nNow, create a text file named “series.txt” with the following content in your working directory, or download the prepared file into your working directory.\nThese are the DSMs with their time stamps:\n2015_03_18_DSM_agi_6GCP|2015-03-18\n2015_06_20_DSM_agi_11GCP|2015-06-20\n2015_07_27_DSM_agi_2GCP_4ID|2015-07-27\n2015_08_14_DSM_agi_3GCP_4ID|2015-08-14\n2015_09_04_DSM_agi_6GCP|2015-09-04\n2015_09_21_DSM_agi_6GCP|2015-09-21\n2015_10_06_DSM_agi_8GCPs|2015-10-06\nRegister the DSMs into the created STRDS uas_dsm:\nt.register input=uas_dsm file=series.txt\nNow check it was registered correctly:\nt.info input=uas_dsm\nSee different ways we can list raster maps registered in the dataset:\nt.rast.list input=uas_dsm columns=name,start_time where=\"start_time &gt;= '2015-08-01'\"\nt.rast.list input=uas_dsm columns=name,start_time,min,max\nVerify the listed raster maps are actually in the database:\ng.list type=raster pattern=\"2015*\"\nYou can also use the Data tab in Layer Manager.\nWe can set common color table to all maps in the dataset:\nt.rast.colors input=uas_dsm color=elevation\nd.rast 2015_06_20_DSM_agi_11GCP@assignment5b\nd.rast 2015_10_06_DSM_agi_8GCPs@assignment5b\nTo get better idea about the dates and spatial extents, we can use Timeline tool accessible from Temporal - GUI tools - Timeline tool.\nSelect dataset uas_dsm, you can click on the drawn data points, and then also check 3D plot of spatio-temporal extents. Stretch the window as needed.\nWe can plot the elevation timeseries using Temporal Plot Tool.\n\nOpen the tool from Temporal - GUI tools - Temporal plot tool.\nSelect dataset uas_dsm and click on the arrow next to the X and Y coordinates field.\nThen click somewhere on the Map Display and press Draw.\n\nYou should see a plot of elevation values. Try different places (road, field, building).\n\n\nFind the area mapped by all surveys\nWe will find area mapped by all surveys. First we will set the extent to the maximum bounding box, use t.info to get the values:\nt.info input=uas_dsm@assignment5b -g\ng.region n=219942 s=219037 e=637439 w=636456 -pa res=0.3\nNext, we use t.rast.series to derive raster representing the number of overlapping DSMs. By using -n flag we create masking raster representing the intersection of all DSMs, and then we change computational region to the extent of its non-null bounding box:\nt.rast.series input=uas_dsm@assignment5b method=count output=count\nt.rast.series input=uas_dsm@assignment5b method=count output=intersection -n\nd.rast count\nd.rast intersection\ng.region zoom=intersection -p\nr.mask raster=intersection\n\n\n\n\n\n\nNote\n\n\n\nHow large is the area? (use r.report)\n\n\n\n\nTemporal aggregation\nOur space-time raster dataset contains two DSMs from September:\nt.rast.list input=uas_dsm@assignment5b columns=name,start_time\nWe will temporally aggregate the dataset by month by averaging all DSMs within each month (in this case averaging the September DSMs):\nt.rast.aggregate input=uas_dsm@assignment5b output=uas_dsm_aggr basename=uas_dsm_aggr suffix=time granularity=\"1 months\" method=average\nt.rast.list input=uas_dsm_aggr columns=name,start_time\n\n\nMinimum elevation (core)\nYou should already have lidar DSM and DEMs if you haven’t done already, unzip it in your working directory and unpack:\nr.unpack -o input=mid_pines_lidar2013_dsm.pack\nr.unpack -o input=mid_pines_lidar2013_dem.pack\n\n\n\n\n\n\nNote\n\n\n\nFind the minimum elevation (core) - how close it is to lidar bare ground?\n\n\nt.rast.series input=uas_dsm method=minimum output=minimum\nr.mapcalc \"diff_lidar_uas = mid_pines_lidar2013_dem - minimum\"\nr.colors map=diff_lidar_uas color=differences\nd.rast diff_lidar_uas\nParts of the minimum raster are significantly below lidar ground, find out which DSM is causing it:\nt.rast.series input=uas_dsm@assignment5b method=min_raster output=min_raster\nd.rast min_raster\nDisplay resulting raster min_raster and query it (select the layer in Layer Manager and use Query tool in Map Display) or use r.what:\nr.what map=min_raster coordinates=636879,219432\nr.what map=min_raster coordinates=637232,219651\nValue 2 represents third (numbering starts with 0) raster in our time series. Since there is some problem with the third and fourth raster (2015_07_27_DSM_agi_2GCP_4ID, 2015_08_14_DSM_agi_3GCP_4ID@assignment5b), we will exclude them from the analysis:\nt.rast.series input=uas_dsm@assignment5b method=minimum output=minimum where=\"start_time &lt; '2015-07-01' or start_time &gt; '2015-09-01'\" --o\nr.mapcalc \"diff_lidar_uas = mid_pines_lidar2013_dem - minimum\" --o\nr.colors map=diff_lidar_uas color=differences\nApply now the color table dif_lidar_uav.txt we used for highlighting differences in the last assignment:\nr.colors map=diff_lidar_uas rules=dif_lidar_uav.txt\nExplain what you see.\nLet’s look now which temporal datasets we have:\nt.list columns=id,number_of_maps,start_time"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#estimate-crop-biomass",
    "href": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#estimate-crop-biomass",
    "title": "Assignment 5B",
    "section": "Estimate crop biomass",
    "text": "Estimate crop biomass\nWe will use raster algebra on the time series to compute crop biomass. First we need to make sure that we take into account only fields (not forests).\nYou can use get the packed raster representation of fields (link is at the top).\nWe assume crop is anything higher than 0.3 and lower than 2 m (to exclude the building for example):\nr.unpack -o fields.pack\nt.rast.mapcalc inputs=uas_dsm@assignment5b expression=\"if (fields, if(uas_dsm - mid_pines_lidar2013_dem &gt; 0.3 && uas_dsm - mid_pines_lidar2013_dem &lt; 2, uas_dsm - mid_pines_lidar2013_dem, null()))\" output=veg_uas_lidar basename=veg_uas_lidar\nt.rast.univar input=veg_uas_lidar where=\"start_time &lt; '2015-07-01' or start_time &gt; '2015-09-01'\"\nd.rast veg_uas_lidar_2\nr.report -n map=veg_uas_lidar_2 units=me nsteps=1\nr.univar map=veg_uas_lidar_2\nUse Python shell (tab in Layer Manager) to compute the biomass volume for June (paste each line into Python shell and press Enter):\nsum = grass.parse_command('r.univar', map='veg_uas_lidar_2', flags='g')['sum']\nfloat(sum) * grass.region()['nsres'] * grass.region()['ewres']\nRemove the mask before we proceed in further analysis:\nr.mask -r"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#evaluate-impact-of-vegetation-change-on-viewshed-extent",
    "href": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5b.html#evaluate-impact-of-vegetation-change-on-viewshed-extent",
    "title": "Assignment 5B",
    "section": "Evaluate impact of vegetation change on viewshed extent",
    "text": "Evaluate impact of vegetation change on viewshed extent\nWe would like to set up a webcam to monitor this area. First we will compute a cumulative viewshed of this area to find places from which we will have likely good visibility. We pick several places distributed all over the area from which we compute viewsheds on a lidar DSM with observer height of 3 meters (webcam height).\ng.region raster=mid_pines_lidar2013_dsm res=1 -pa\nr.viewshed input=mid_pines_lidar2013_dsm output=vshed_1 coordinates=637087,219376 observer_elevation=3\nr.viewshed input=mid_pines_lidar2013_dsm output=vshed_2 coordinates=637004,219653 observer_elevation=3\nr.viewshed input=mid_pines_lidar2013_dsm output=vshed_3 coordinates=636782,219374 observer_elevation=3\nr.viewshed input=mid_pines_lidar2013_dsm output=vshed_4 coordinates=636839,219150 observer_elevation=3\nr.viewshed input=mid_pines_lidar2013_dsm output=vshed_5 coordinates=636665,219527 observer_elevation=3\nr.viewshed input=mid_pines_lidar2013_dsm output=vshed_6 coordinates=636863,219503 observer_elevation=3\nNow we compute the cumulative viewshed as the number of places (from the coordinates above) from which a cell is visible. This gives us better idea which places are visually more prominent and therefore likely to have good view of the fields.\nr.series input=vshed_1,vshed_2,vshed_3,vshed_4,vshed_5,vshed_6 output=cum_viewshed method=count\nd.rast cum_viewshed\nDisplay the result with legend to see which places are potentially good for placing a webcam.\nSince we need the webcam to capture the field during entire year we can run t.rast.series with method=maximum on the time series of UAS DSMs to derive the DSM series envelope surface and use it to analyze the viewshed. Of course, this assumes that the crops will be the same the following year. We exclude the problematic DSMs as we did while deriving the core surface:\ng.region raster=mid_pines_lidar2013_dsm\nt.rast.series input=uas_dsm@assignment5b method=maximum output=maximum where=\"start_time &lt; '2015-07-01' or start_time &gt; '2015-09-01'\"\nWe need to update the lidar-based DSM with the UAS envelope data and analyze the viewshed while taking into account the corn growing in the field. First we need to make sure that we take into account entire field to create a lidar-based DSM updated in the fields using UAS data.\nr.mapcalc \"maximum_clip = if (fields, maximum)\"\nr.patch.smooth input_a=maximum_clip input_b=mid_pines_lidar2013_dsm output=lidar_maximum_dsm smooth_dist=10\nr.relief lidar_maximum_dsm out=lidar_maximum_dsm_relief zscale=5\nd.rast lidar_maximum_dsm_relief\nLook at the relief map around the building - where do people park their cars?\nNow we selected a location based on the cumulative viewshed and look at the difference between the viewshed based on lidar and the envelope surface:\nr.viewshed input=mid_pines_lidar2013_dsm output=viewshed_lidar_dsm observer_elevation=3 coordinates=636917,219223\nr.viewshed input=lidar_maximum_dsm output=viewshed_maximum observer_elevation=3 coordinates=636917,219223\n\nr.colors viewshed_lidar_dsm co=reds\nr.colors viewshed_maximum co=greens\nTo compare the viewsheds display the resulting maps with transparency.\n\n\n\n\n\n\nNote\n\n\n\nDiscuss the result. Can you find a better place for the webcam? Would placing the webcam higher help?\n\n\n\nEstimate volume of a building\nWe need to first extract the mask of the building:\ng.region n=219388.5 s=219366.9 w=637100.1 e=637138.5 res=0.3 -pa\nr.mapcalc \"building = 2015_06_20_DSM_agi_11GCP - mid_pines_lidar2013_dem\"\nr.mapcalc \"building_mask = if(building &gt; 2, 1, null())\"\nNow we compute the volume\nr.volume input=building clump=building_mask\n\n\n\n\n\n\nNote\n\n\n\nWhat is the volume of the building?"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#outline",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#outline",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Outline",
    "text": "Outline\n\nMotivation for combining lidar and UAS SfM data\nAnalysis of lidar and UAS SfM DSMs differences\nPatching and smooth fusion of UAS and lidar DSM\nOverland flow simulation on fused DEM"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#point-clouds-from-lidar",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#point-clouds-from-lidar",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Point Clouds from Lidar",
    "text": "Point Clouds from Lidar\n\nMeasured variable is the time of return for each pulse\nGeoreferencing is based on the position (measured by GPS) and exterior orientation (measured by inertial navigation system INS) of the platform\n(x, y, z) is derived from time, GPS positioning, and INS parameters\n\n\nPoint cloud visualization from Lidar"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#point-clouds-from-sfm",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#point-clouds-from-sfm",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Point Clouds from SfM",
    "text": "Point Clouds from SfM\n\nMeasured variable is reflected energy captured as imagery\nGeoreferencing can be done entirely from GCPs\n(x, y, z) is derived from overlapping images and GCPs\nAlternatively: GPS and INS for the position and orientation of images and camera parameters\n\n\nPoint cloud visualization from SfM"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#comparison-of-point-cloud-properties",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#comparison-of-point-cloud-properties",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Comparison of Point Cloud Properties",
    "text": "Comparison of Point Cloud Properties\n\n\nAirborne Lidar:\n\nPasses through vegetation, multiple returns\nShifts between swaths, corduroy effect\nRegional coverage\nHigh cost\n\n\nUAS SfM:\n\nVery high point density\nLimited capability to map under vegetation\nInfluenced by cast shadows\nLow cost but limited area\n\n\nDifferent distribution of errors and distortions:\nLidar and SfM provide an independent set of measurements."
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#motivation-for-lidar-and-uas-sfm-fusion",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#motivation-for-lidar-and-uas-sfm-fusion",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Motivation for Lidar and UAS SfM Fusion",
    "text": "Motivation for Lidar and UAS SfM Fusion\n\nLow-cost DEM updates in rapidly changing landscapes (e.g., construction sites)\nReplacing vegetated areas in UAS SfM with bare ground\nWatershed analysis when only part of the watershed was mapped by UAS\nImprove accuracy and level of detail over stable features (buildings, bridges)\nAdd your suggestion"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#fusion-techniques",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#fusion-techniques",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Fusion Techniques",
    "text": "Fusion Techniques\n\nMerge the point clouds, decimate or bin at high resolution, interpolate: computationally demanding\nInterpolate DEMs, randomly sample, patch, and reinterpolate: loss of detail, time-consuming\nInterpolate DEMs and patch: sharp edges\nSmooth fusion - requires DEMs at the same resolution"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#workflow",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#workflow",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Workflow",
    "text": "Workflow\n\nEvaluate the point clouds, select resolution\nInterpolate lidar and UAS DEMs at the same resolution\nEvaluate the differences between the lidar and UAS DEMs\nIf applicable, identify errors and apply corrections\nApply smooth fusion"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#identify-distortions-due-to-processing",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#identify-distortions-due-to-processing",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Identify Distortions Due to Processing",
    "text": "Identify Distortions Due to Processing\n\nIdentify the spatial pattern of distortions\nDifference maps between Lidar DSM and UAS SfM DSMs\n\n\n\nAgisoft\n\n\n\nDifference Maps\n\n\n\nPix4D\n\n\n\nDifference Maps"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#identify-changes-in-topography",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#identify-changes-in-topography",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Identify Changes in Topography",
    "text": "Identify Changes in Topography\n\nVegetation growth or removal\nChange in elevation surface due to erosion processes\nUAS SfM - lidar difference maps with custom color\n\n\n\n\n\n\nElevation Difference Maps\n\n\n\n\n\n\nField Image"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#comparing-profiles-fields",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#comparing-profiles-fields",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Comparing Profiles: Fields",
    "text": "Comparing Profiles: Fields\n\nIdentify systematic errors (vertical shifts)\nDifferences due to vegetation growth, erosion/deposition\n\n\nProfiles Comparison Fields"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#comparing-profiles-building",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#comparing-profiles-building",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Comparing Profiles: Building",
    "text": "Comparing Profiles: Building\n\nHigh accuracy, lower level of detail in lidar data\nNegligible systematic error\n\n\nProfiles Comparison Building"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#comparing-profiles-road",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#comparing-profiles-road",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Comparing Profiles: Road",
    "text": "Comparing Profiles: Road\n\nRelatively easy to identify stable features\nHigh accuracy - differences between lidar and UAS SfM within 4cm\n\n\nProfiles Comparison Road"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#correct-for-systematic-error-and-distortions",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#correct-for-systematic-error-and-distortions",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Correct for Systematic Error and Distortions",
    "text": "Correct for Systematic Error and Distortions\n\nSystematic error: if due to GPS shift, use median difference\nSystematic error: if tilt, use regression function\nComplex UAS SfM distortions can be reduced using interpolated GCP differences or differencing with lidar\nImportance of well-distributed, accurate GCPs"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#updating-lidar-dem-by-patching",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#updating-lidar-dem-by-patching",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Updating Lidar DEM by Patching",
    "text": "Updating Lidar DEM by Patching\n\nReplace the grid cells in the updated area by UAS DEM\nAverage over an overlap\nUsually creates an edge - requires post-processing\n\n\n\n\n\n\nPatching Results\n\n\n\n\n\n\nPatching Results\n\n\n\nImages by Brendan Harmon"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#updating-lidar-dem-by-patching-1",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#updating-lidar-dem-by-patching-1",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Updating Lidar DEM by Patching",
    "text": "Updating Lidar DEM by Patching\nEdge between lidar and UAS SfM DEMs\n\nEdge Visualization"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#smooth-fusion",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#smooth-fusion",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Smooth Fusion",
    "text": "Smooth Fusion\n\nDerive overlap raster\nWeighted smoothing based on the distance from the edge\n\n\n\n\n\n\nFusion Results\n\n\n\n\n\n\nFusion Results"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#fusion-method",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#fusion-method",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Fusion Method",
    "text": "Fusion Method\nLinear combination of elevation surfaces \\(z_{A}\\) and \\(z_{B}\\) with weights given by overlap width \\(s\\) and distance \\(d\\) to the edge of \\(z_{A}\\):\n\n\n\\[\nz_{AB} = z_{A} w + z_{B}(1 - w),\n\\\\[10pt]\nw = f(s, d) =\n\\begin{cases}\n  \\frac{d}{s} & 0 \\leq d &lt; s \\\\\n  1 & d \\geq s\n\\end{cases}\n\\]\n\n\n\n\nFusion Method Schema"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#influence-of-overlap-width-s",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#influence-of-overlap-width-s",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Influence of Overlap Width \\(s\\)",
    "text": "Influence of Overlap Width \\(s\\)\n\nOverlap Influence"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#fusion-with-spatially-variable-s",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#fusion-with-spatially-variable-s",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Fusion with Spatially Variable \\(s\\)",
    "text": "Fusion with Spatially Variable \\(s\\)\nBy taking into account spatially variable differences \\(\\Delta z\\) between DEMs \\(A\\) and \\(B\\) along the overlap:\n\n\n\nWe get a more gradual transition where differences are high\nWe preserve subtle features of both DEMs where differences are small\n\n\n\n\n\nSpatially Variable Fusion"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#fusion-with-spatially-variable-s-1",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#fusion-with-spatially-variable-s-1",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Fusion with Spatially Variable \\(s\\)",
    "text": "Fusion with Spatially Variable \\(s\\)\nAbsolute elevation difference between lidar and UAS DEM controls the width of the transition overlap.\n\nFusion Visualization"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#lidar-and-uas-sfm-patching-and-fusion",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#lidar-and-uas-sfm-patching-and-fusion",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Lidar and UAS SfM Patching and Fusion",
    "text": "Lidar and UAS SfM Patching and Fusion\n\n\n\n\n\nPatched vs. Fused DSMs\n\n\n\n\n\n\nPatched vs. Fused DSMs\n\n\n\nSimply patched vs. fused UAS and lidar DSMs with overlap width \\(s = 20\\ m\\)"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#fusing-vegetated-patch-with-bare-ground",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#fusing-vegetated-patch-with-bare-ground",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Fusing Vegetated Patch with Bare Ground",
    "text": "Fusing Vegetated Patch with Bare Ground\n\n\n\n\n\nBare Ground Fused\n\n\n\n\n\n\nBare Ground Fused\n\n\n\nVegetated areas in UAS DSM are replaced with lidar DEM. Yellow shows bare ground in UAS DSM."
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#spatially-variable-overlap",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#spatially-variable-overlap",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Spatially Variable Overlap",
    "text": "Spatially Variable Overlap\n\nSpatial OverlapBare ground UAS DSM is used to update lidar bare ground DEM using spatially variable overlap"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#modeling-overland-flow",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#modeling-overland-flow",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Modeling Overland Flow",
    "text": "Modeling Overland Flow\nMicrotopography captured at ultra-high resolution by UAS SfM poses special challenges to flow routing:\n\nReal depressions are important features\nComplex pattern of ponding, overflow of barriers is not supported by standard tools\nNoisy surface requires a robust algorithm"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#path-sampling-method",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#path-sampling-method",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Path Sampling Method",
    "text": "Path Sampling Method\nStochastic method for solving flow continuity equations\n\n\n\n\n\nFlow Animation\n\n\n\n\n\n\nFlow Animation\n\n\n\n\nMitasova et al., 2005"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#flow-over-uas-mapped-field",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#flow-over-uas-mapped-field",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Flow Over UAS Mapped Field",
    "text": "Flow Over UAS Mapped Field\nCrop surface creates barrier leading to artificial ponding\n\nFlow Over Mapped Field"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#flow-over-patched-dems",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#flow-over-patched-dems",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Flow Over Patched DEMs",
    "text": "Flow Over Patched DEMs\nReplacing crop surface by lidar bare ground: patching creates artificial flow pattern, smooth fusion improves accuracy of flow distribution\n\n\n\n\n\nFlow Pattern\n\n\n\n\n\n\nFlow Pattern"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#example-from-assignment",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#example-from-assignment",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Example from Assignment",
    "text": "Example from Assignment\nWater flow on DSM created by simple patching vs. smooth patching\n\n\n\n\n\nFlow Comparison\n\n\n\n\n\n\nFlow Comparison"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#example-from-assignment-1",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#example-from-assignment-1",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "Example from Assignment",
    "text": "Example from Assignment\nWater flow on DSM vs. bare ground fused from UAS DSM and lidar DEM\n\n\n\n\n\nFlow on DSM vs. Bare Ground\n\n\n\n\n\n\nFlow on DSM vs. Bare Ground"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#what-did-we-learn",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5a.html#what-did-we-learn",
    "title": "UAS and Lidar Data: Comparison, Fusion, and Analysis",
    "section": "What Did We Learn?",
    "text": "What Did We Learn?\n\nEvaluation and interpretation of differences between lidar and UAS SfM DEMs\nTechniques for smooth fusion of high-resolution DEMs\nImpact of fusion technique on overland flow modeling"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/assignments/assignment_1b.html#task",
    "href": "course/topics/topic_1_uas_basics/assignments/assignment_1b.html#task",
    "title": "Assignment 1B",
    "section": "Task",
    "text": "Task\nUAS Rule Part 107 test: select 5 questions and briefly explain the correct answers in the the provided sample questions"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/assignments/assignment_1b.html#homework",
    "href": "course/topics/topic_1_uas_basics/assignments/assignment_1b.html#homework",
    "title": "Assignment 1B",
    "section": "Homework",
    "text": "Homework\nPrepare brief report on the selected section of the Part 107 test - 5 questions and answers with explanation."
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/part_b_rules_regs.html#outline",
    "href": "course/topics/topic_1_uas_basics/part_b_rules_regs.html#outline",
    "title": "B. Rules and regulations for UAS operations",
    "section": "Outline",
    "text": "Outline\n\nlevels of regulatory authority\nSmall UAS Rule (Part 107)\nUAS authorizations to fly\ntypes of airspace and restrictions to flight\ncurrent information resources"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/part_b_rules_regs.html#lecture",
    "href": "course/topics/topic_1_uas_basics/part_b_rules_regs.html#lecture",
    "title": "B. Rules and regulations for UAS operations",
    "section": "Lecture",
    "text": "Lecture\n\nLecture Slides: Rules and Regulations\nLecture Recording: Fall 2024 (NCSU Only)\nPast Videos (2017)\n\nPart 1\nPart 2\nPart 3\nPart 4\nPart 5\n\n\n\nSupplemental materials\n\nFAA Unmanned Aircraft Systems\nBecoming a Pilot sUAS Rule (Part 107)\nWhere can I find study materials for the Part 107 aeronautical knowledge test?\nOperating UAS in North Carolina\nUAS Activities at NC State: Rules and Regulations\nAeronautical charts and related georeferenced data\n\nView digital aeronautical charts\nSkyvector:charts, airspace and airport information for pilots\nDownload georeferenced digital aeronautical charts, download Charlotte for NC chart\nFAA Data Portal\n\nWeather data\n\nAviation Weather Center, interactive METAR, TAFs and other NOAA NWS resources"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/part_b_rules_regs.html#assignment",
    "href": "course/topics/topic_1_uas_basics/part_b_rules_regs.html#assignment",
    "title": "B. Rules and regulations for UAS operations",
    "section": "Assignment",
    "text": "Assignment\nUAS Rule Part 107 test: select 5 questions and briefly explain the correct answers in the the provided sample questions"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/part_b_rules_regs.html#homework",
    "href": "course/topics/topic_1_uas_basics/part_b_rules_regs.html#homework",
    "title": "B. Rules and regulations for UAS operations",
    "section": "Homework",
    "text": "Homework\nPrepare brief report on the selected section of the Part 107 test - 5 questions and answers with explanation."
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#objectives",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#objectives",
    "title": "Introduction to UAS",
    "section": "Objectives",
    "text": "Objectives\n\n\nUse the proper terminology and understand its meaning\nDescribe the elements of UAS\nClassify different UAS according to their make and characteristics\nDescribe different classes of the UAS\nUnderstand the current state of the UAS development\nDescribe the objectives beyond the use of the UAS"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#what-is-an-uav",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#what-is-an-uav",
    "title": "Introduction to UAS",
    "section": "What is an UAV?",
    "text": "What is an UAV?\nFor the brief summary read the overview article"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-or-uav-or-maybe-a-drone",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-or-uav-or-maybe-a-drone",
    "title": "Introduction to UAS",
    "section": "UAS or UAV? Or maybe a drone?",
    "text": "UAS or UAV? Or maybe a drone?\n\n\nUAV = Unmanned Aerial Vehicle\nUAS = Unmanned Aerial Systems\nDrone = Dynamic Remotely Operated Navigation Equipment\nRPA = Remotely Piloted Aircraft"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-unmanned-aircraft-system",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-unmanned-aircraft-system",
    "title": "Introduction to UAS",
    "section": "UAS: Unmanned Aircraft System",
    "text": "UAS: Unmanned Aircraft System\n\n\n\nUnmanned = without a person onboard (operated by automatic or remote control)\nAircraft = able to fly\nSystem = associated elements related to safe operations (may include control stations, control links, support equipment, payloads, flight termination systems, and launch recovery equipment)\n\n\n\nConsists of three elements:\n\nUnmanned Aircraft\nControl Station\nData Link"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-or-uav-or-maybe-a-drone-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-or-uav-or-maybe-a-drone-1",
    "title": "Introduction to UAS",
    "section": "UAS or UAV? Or maybe a drone?",
    "text": "UAS or UAV? Or maybe a drone?\n\n\nUnmanned Aerial Vehicle (UAV)\n\nExcludes: missiles, weapons, or exploding warheads,\nIncludes: all classes of airplanes, helicopters, airships, and powered-lift aircraft,\nDoesn’t include: traditional balloons, rockets, tethered aircraft, and un-powered gliders.\n\n\n\n\n\n\nMore about terminology in the article: Don’t use the ‘D’ Word: They’re ‘UAVs’ or ‘RPAs’ But Definitely Not ‘Drones’"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#what-does-a-uas-look-like",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#what-does-a-uas-look-like",
    "title": "Introduction to UAS",
    "section": "What does a UAS look like?",
    "text": "What does a UAS look like?\nTypes and classification of UAS"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#different-shapes-and-sizes",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#different-shapes-and-sizes",
    "title": "Introduction to UAS",
    "section": "Different shapes and sizes",
    "text": "Different shapes and sizes"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#different-designs",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#different-designs",
    "title": "Introduction to UAS",
    "section": "Different Designs",
    "text": "Different Designs"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#different-classifications",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#different-classifications",
    "title": "Introduction to UAS",
    "section": "Different Classifications",
    "text": "Different Classifications\n\n\n\nPhysical size\nWeight\nEndurance\nAltitude\nWing loading\nEngine type\nRange\nPerformance\nCapabilities\nType\n\n\n\n\n\nMore about classification in the article: Perspectives on Unmanned Aircraft Classification for Civil Airworthiness Standards"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#simplified-classifications",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#simplified-classifications",
    "title": "Introduction to UAS",
    "section": "Simplified classifications",
    "text": "Simplified classifications\n\n\n\n\n\n\nMore (much much more) about classifications in the article\nCondensed basics including classification can be found in Unmanned Aircraft Systems for Civilian Missions"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#how-does-a-uas-work",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#how-does-a-uas-work",
    "title": "Introduction to UAS",
    "section": "How does a UAS work?",
    "text": "How does a UAS work?\nSystem elements"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#generic-unmanned-aircraft-system",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#generic-unmanned-aircraft-system",
    "title": "Introduction to UAS",
    "section": "Generic Unmanned Aircraft System",
    "text": "Generic Unmanned Aircraft System\n\n\n\nAir vehicle\nMission planning element\nCommand and control element\nCommunication link\nLaunch and recovery element (for some of them)\nPayload"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#sensors",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#sensors",
    "title": "Introduction to UAS",
    "section": "Sensors",
    "text": "Sensors"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#sensors-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#sensors-1",
    "title": "Introduction to UAS",
    "section": "Sensors",
    "text": "Sensors"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-sensors-for-mapping",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-sensors-for-mapping",
    "title": "Introduction to UAS",
    "section": "UAS sensors for mapping",
    "text": "UAS sensors for mapping\n\nRGB cameras\nInfrared sensors\nMultispectral and hyperspectral sensors\nLaser scanners\nThermal sensors"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#rgb-natural-color-cameras",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#rgb-natural-color-cameras",
    "title": "Introduction to UAS",
    "section": "RGB (natural color) cameras",
    "text": "RGB (natural color) cameras\n\n\n\nMost common payload for consumer-grade UAS\nPhoto or video mode\nMapping (orthophoto and DSM generation) possible even with non-photogrammetric cameras\nVariety of cameras, lenses, and mounting systems - some suitable for 3D modeling"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#multispectral-and-hyperspectral-cameras",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#multispectral-and-hyperspectral-cameras",
    "title": "Introduction to UAS",
    "section": "Multispectral and hyperspectral cameras",
    "text": "Multispectral and hyperspectral cameras\n\n\n\nMiniaturization is challenging in terms of optics and sensor calibration\nWeight, cost, data quality has improved; spectral bands, resolution need improvements\nCameras with NIR band: agriculture and vegetation mapping (for NDVI)\nThe more bands the more information, but also higher price of the sensor\n\n$$ multispectral - couple thousands\n$$$ hyperspectral - tens of thousands"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#active-sensors---lidar-and-sar",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#active-sensors---lidar-and-sar",
    "title": "Introduction to UAS",
    "section": "Active sensors - LiDAR and SAR",
    "text": "Active sensors - LiDAR and SAR\n\n\n\nActive sensors can reach below-canopy ground surface\nLarge trade-offs between performance and size or cost of LiDAR\nLiDAR now common on UAS thanks to miniaturization\nSAR (Synthetic Aperture Radar) used experimentally, still faces challenges in adaptation to UAS"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#thermal-imaging",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#thermal-imaging",
    "title": "Introduction to UAS",
    "section": "Thermal imaging",
    "text": "Thermal imaging\n\n\n\nUsed in forest fire monitoring, search and rescue missions\nFor mapping purposes coupled with visible band sensors (see example FLIR DUO)"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#sensor-and-platform-integration",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#sensor-and-platform-integration",
    "title": "Introduction to UAS",
    "section": "Sensor and platform integration",
    "text": "Sensor and platform integration\nThe optimal combination of carrier (Unmanned Vehicle) and sensing payload needs to be determined based on:\n\n\n\nVolume, size, and weight specifications\nSpecific application requirements\nMounting: integrated by manufacturer or custom solutions\nThe sensors must be adapted to the carrier and vice versa"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#who-uses-uas",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#who-uses-uas",
    "title": "Introduction to UAS",
    "section": "Who uses UAS?",
    "text": "Who uses UAS?\nTypes of UAV Operations"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#types-of-uas-operations",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#types-of-uas-operations",
    "title": "Introduction to UAS",
    "section": "Types of UAS Operations",
    "text": "Types of UAS Operations\n\nPublic Operations\nGovernmental, limited by federal statute to certain government operations within U.S. Airspace;\nCivil Operations\nNon-Governmental, must be conducted in accordance with all Federal Aviation Administration (FAA) regulations;\nModel Aircraft\nHobby or Recreation only."
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#faa-federal-aviation-administration",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#faa-federal-aviation-administration",
    "title": "Introduction to UAS",
    "section": "FAA – Federal Aviation Administration",
    "text": "FAA – Federal Aviation Administration\n\n\nNAS – National Airspace System\nThe common network of U.S. airspace:\n\nair navigation facilities, equipment, and services;\nairports or landing areas;\naeronautical charts, information and services;\nrules, regulations, and procedures;\ntechnical information; and manpower and material."
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#integration-of-uas-into-the-nas",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#integration-of-uas-into-the-nas",
    "title": "Introduction to UAS",
    "section": "Integration of UAS into the NAS",
    "text": "Integration of UAS into the NAS\n\nreview of current policies, regulations, environmental impact, privacy considerations, standards, and procedures;\nidentification of gaps in current UAS technologies and regulations, standards, policies, or procedures;\ndevelopment of new technologies and new or revised regulations, standards, policies, and procedures;\nand the associated development of guidance material, training, and certification of aircraft systems, propulsion systems, and airmen."
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#can-i-use-it",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#can-i-use-it",
    "title": "Introduction to UAS",
    "section": "Can I use it?",
    "text": "Can I use it?\nRegulations, standards, policies, and procedures"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#know-before-you-fly",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#know-before-you-fly",
    "title": "Introduction to UAS",
    "section": "Know before you fly",
    "text": "Know before you fly\n\n“Know Before You Fly” is an educational campaign that provides prospective unmanned aircraft users with the information and guidance they need to fly safely and responsibly.\nDifferent rules apply to recreational and business users and government entities\nNew regulations (effective on August 29, 2016) known as “Part 107”"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#what-are-uas-used-for",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#what-are-uas-used-for",
    "title": "Introduction to UAS",
    "section": "What are UAS used for?",
    "text": "What are UAS used for?"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#army-and-government",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#army-and-government",
    "title": "Introduction to UAS",
    "section": "Army and Government",
    "text": "Army and Government\n\n\n\n\nDepartment of Agriculture\nDepartment of Commerce\nDepartment of Defence\nDepartment of Energy\nDepartment of Homeland Security\nDepartment of Interior\nDepartment of Justice\nNASA\nNOAA\nState Universities\nState Law Enforcement\n\n\n\n\n\nMilitary Use"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#civilian-application-fields",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#civilian-application-fields",
    "title": "Introduction to UAS",
    "section": "Civilian Application Fields",
    "text": "Civilian Application Fields"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#application-examples-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#application-examples-1",
    "title": "Introduction to UAS",
    "section": "Application Examples (1)",
    "text": "Application Examples (1)\n\n\n\nAgriculture:\n\nUAS equipped with fertilizer and pesticide dispersing equipment can be used to spray over large fields;\n\nTelecommunications:\n\nAs mobile relay platforms, e.g., in disaster zones for emergency telecommunications"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#application-examples-2",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#application-examples-2",
    "title": "Introduction to UAS",
    "section": "Application Examples (2)",
    "text": "Application Examples (2)\n\n\n\nNews Broadcasting:\n\nIn providing aerial video feeds for news events;\n\nAir Traffic Control and Ground Traffic Control:\n\nTo monitor traffic and accidents over highways and streets, capture violation of traffic rules;"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#application-examples-3",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#application-examples-3",
    "title": "Introduction to UAS",
    "section": "Application Examples (3)",
    "text": "Application Examples (3)\n\n\n\nMineral Exploration:\n\nIn aerial survey, to find minerals in hard-to-reach regions;\nIn existing mines, to map extracted material volumes;\n\nCoastal Monitoring:\n\nMapping dynamic shorelines and post-storm assessment"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-for-remote-sensing-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-for-remote-sensing-1",
    "title": "Introduction to UAS",
    "section": "UAS for Remote Sensing (1)",
    "text": "UAS for Remote Sensing (1)\n\n\n\nAgriculture and Environment:\n\nCrop monitoring, vegetation mapping, forest fire monitoring, animals detection;\n\nIntelligence, Surveillance, and Reconnaissance:\n\nDetection of lost persons in difficult-to-access situations, support fire brigades in real-time crisis management, rapid disaster management;"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-for-remote-sensing-2",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-for-remote-sensing-2",
    "title": "Introduction to UAS",
    "section": "UAS for Remote Sensing (2)",
    "text": "UAS for Remote Sensing (2)\n\n\n\nAerial Monitoring in Engineering:\n\nInfrastructure inspections, distributed wind measurement, landslide monitoring;\n\nCultural Heritage:\n\nOrthophotos of archaeological sites, 3D models of man-made structures;"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-for-remote-sensing-3",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#uas-for-remote-sensing-3",
    "title": "Introduction to UAS",
    "section": "UAS for Remote Sensing (3)",
    "text": "UAS for Remote Sensing (3)\n\n\n\nGeneral surveying, mapping, and photogrammetry, cadastral applications:\n\nCadastral surveying, alternative to traditional surveying, high-precision parcel boundary determination;"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#use-your-imagination",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#use-your-imagination",
    "title": "Introduction to UAS",
    "section": "Use your imagination!",
    "text": "Use your imagination!"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#why-should-you-know-how-to-use-uas",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#why-should-you-know-how-to-use-uas",
    "title": "Introduction to UAS",
    "section": "Why should you know how to use UAS?",
    "text": "Why should you know how to use UAS?\nHow to obtain spatial data?"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#barriers",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1a.html#barriers",
    "title": "Introduction to UAS",
    "section": "Barriers",
    "text": "Barriers\n\nConstantly changing legislation and regulations\nPolitical and societal acceptance\n\n… what do you consider a barrier in UAS development? What obstacles do you see for yourself as a potential UAS user?"
  },
  {
    "objectID": "course/logistics.html",
    "href": "course/logistics.html",
    "title": "Logistics",
    "section": "",
    "text": "Course website and Moodle\nThe course material can be accessed on the Schedule web page.\nCourse Forum, assignment submissions, project material, and grades are handled in Moodle.\n\n\nLectures\n\nFocus of lectures is on principles, theory, concepts, methods and lectures are mostly software independent.\nLecture slides and video are available at each topic’s web page - you can access them through Schedule or Moodle.\nTopics webpages also include links to addition reading and other resources.\nNCSU Panopto: Fall 2024 Letcure Recordings\n\n\n\nAssignments\n\nMost topics include hands-on assignment - the links to assignments are provided at each topic’s web page - you can access them through the course schedule.\nFollowing the Schedule, you will submit a brief (2 pages minimum, including images and tables) paper for each topic with the following paragraphs:\n\nIntroduction: task, problem\nMethods: general description of methods and concepts\nResults: present the results, including maps, graphs, tables with comments\nDiscussion: was there anything unexpected or problematic in the results? Were there any issues related to the methodology or software? Include suggestions for improvements here.\nConclusion: 1-2 sentences on what you have learned\n\nSubmit your report through Moodle in pdf format, use naming convention LastnameFirstname_HW#.pdf.\n\n\n\nSoftware\n\nGRASS GIS 8.4 (you can use these instructions to install it)\nAgisoft Metashape (provided at NCSU) - you can work on the demo version.\nOpenDroneMap\n\n\n\nComputational Resources\n\nNCSU students may use VCL (Virtual Computing Lab)[https://vcl.ncsu.edu/]\n\nMake a reservation using the Agisoft Metashape & GRASS GIS - Ubuntu environment.\n\n\nOther resources\n\nMicMac documentation can be found here, not used in this course.\nPDAL - Point Data Abstraction Library\nLibLAS and LAStools for managing the .las files in GRASS\nNCSU Virtual Computing Lab (VCL)\n\n\n\nData\n\nGeneral resources\n\nNorth Carolina dataset (formatted as GRASS location)\n\n\n\nUAS data\n\nMany of these links are currently broken and in the process of being migrated to a new hosting serivce.\n\n\nBoundaries of all COAs obtained by NGAT in North Carolina (google maps, shp)\nStudy site (Lake Wheeler): COA boundaries (shp, kml), GCP coordinates (txt, kml), also included in the GRASS location (see processed data)\nSurvey data: down-sampled pictures and log from the flight 06/20/2015, sample pictures and log from the flight 09/22/2016, all pictures and log (full flight) from the flight 09/22/2016\nProcessed data:\n\nMid Pines dataset (formatted as GRASS location): LakeWheeler_NCspm with timeseries of DSMs from 2015 flights\nSmaller sample point cloud: 2015_06_sample_points_NCsmp.las\nMid Pines point clouds (limited extent)\nOptional (very large files: 1GB+)\n\nMid Pines point clouds from 2015 flights in las, obj and txt formats\nPoint clouds for the gully area at Midpines (high density, from 2015 flights)\n\n\n\n\n\nLidar data\n\nClassified lidar point cloud in LAS format for Mid_Pines\nSecref and Mid Pines interpolated DEM/DSM (use r.unpack)\n\n\n\nOrthophotography\n\nOrthophotos as GRASS GIS packed rasters, use r.unpack from within the LakeWheeler_NCspm location to import them, run it with -o to override projection (it is just a different EPSG for HARN versus NAD83 datum, the difference is negligible)"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#objectives",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#objectives",
    "title": "UAS Rules and Regulations",
    "section": "Objectives",
    "text": "Objectives\n\n\nUnderstand the role of the Federal Aviation Administration (FAA) in ensuring safety in National Airspace System (NAS)\nRecognize the 3 main types of UAS operations and distinguish between them\nName the requirements for legal operations of commercial purposes in the U.S. Both in regard to the pilot and to the UAS\nOutline safety guidelines for recreational use of UAS"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#objectives-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#objectives-1",
    "title": "UAS Rules and Regulations",
    "section": "Objectives",
    "text": "Objectives\n\n\nKnow operating rules for commercial use of UAS (Part 107)\nRecognize permanent and temporary “no-drone-zones” and knows where to find information about them\nUnderstand the role of Certificate of Authorization and the process of obtaining it\nRecognize when the educational use of UAS can be classified as recreational and when commercial"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#federal-aviation-administration-faa",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#federal-aviation-administration-faa",
    "title": "UAS Rules and Regulations",
    "section": "Federal Aviation Administration (FAA)",
    "text": "Federal Aviation Administration (FAA)\n\n\n\nCreated in 1958 in response to a series of fatal accidents and midair collisions involving commercial aircraft\nMandated to develop plans and policies for the use of navigable airspace to ensure the safety of aircraft and the efficient use of airspace\nSince the creation of the FAA, American airspace has become one of the most regulated fields in the United States"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#national-airspace-system-nas",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#national-airspace-system-nas",
    "title": "UAS Rules and Regulations",
    "section": "National Airspace System (NAS)",
    "text": "National Airspace System (NAS)\n\n\nAlready congested with piloted aircraft, and adding a swarm of UAVs requires thoughtful planning\nMain mandate: to ensure that UASs do not endanger current users of the NAS (including manned or other unmanned aircraft) nor compromise the safety of the people and property on the ground\nThe lack of detect-sense-and-avoid capability of the current UAS technology is a major concern for the FAA"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#types-of-uas-operations",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#types-of-uas-operations",
    "title": "UAS Rules and Regulations",
    "section": "Types of UAS operations",
    "text": "Types of UAS operations\n\n\n\nFAA distinguishes 3 main types of operations\nDifferent rules apply for legal operations of UAS depending on the type of UAS operations\nAdditional type - educational use can be considered as recreational or commercial"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#recreational-use-of-uas",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#recreational-use-of-uas",
    "title": "UAS Rules and Regulations",
    "section": "Recreational use of UAS",
    "text": "Recreational use of UAS\n\n\n\nThe recreational use of sUAS is the operation of an unmanned aircraft for personal interests and enjoyment\nUsing a sUAS to take photographs for your own personal use would be considered recreational\nUsing the same device to take photographs or videos for compensation or sale to another individual would be considered a commercial operation"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#recreational-use---safety-guidelines",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#recreational-use---safety-guidelines",
    "title": "UAS Rules and Regulations",
    "section": "Recreational use - safety guidelines",
    "text": "Recreational use - safety guidelines\n\nFollow community-based safety guidelines, as developed by organizations such as the Academy of Model Aeronautics (AMA)\n\n\n\n\nFly no higher than 400 feet and remain below any surrounding obstacles when possible\nKeep your sUAS in eyesight at all times, and use an observer to assist if needed"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#recreational-use---safety-guidelines-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#recreational-use---safety-guidelines-1",
    "title": "UAS Rules and Regulations",
    "section": "Recreational use - safety guidelines",
    "text": "Recreational use - safety guidelines\n\nRemain well clear of and do not interfere with manned aircraft operations, and you must see and avoid other aircraft and obstacles at all times\n\n\n\n\nDo not intentionally fly over unprotected persons or moving vehicles, and remain at least 25 feet away from individuals and vulnerable property.\nContact the airport and control tower before flying within 5 miles of an airport"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#recreational-use---safety-guidelines-2",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#recreational-use---safety-guidelines-2",
    "title": "UAS Rules and Regulations",
    "section": "Recreational use - safety guidelines",
    "text": "Recreational use - safety guidelines\n\nRemain well clear of and do not interfere with manned aircraft operations, and you must see and avoid other aircraft and obstacles at all times\n\n\n\n\nEnsure the operating environment is safe and that the operator is competent and proficient in the operation of the sUAS.\nDo not fly near or over sensitive infrastructure or property such as power stations, water treatment facilities, correctional facilities, heavily traveled roadways, government facilities, etc.\nDo not fly in adverse weather conditions such as in high winds or reduced visibility."
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#recreational-use---safety-guidelines-3",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#recreational-use---safety-guidelines-3",
    "title": "UAS Rules and Regulations",
    "section": "Recreational use - safety guidelines",
    "text": "Recreational use - safety guidelines\n\n\n\nCheck and follow all local laws and ordinances before flying over private property.\nDo not conduct surveillance or photograph persons in areas where there is an expectation of privacy without the individual’s permission.\nDo not fly under the influence of alcohol or drugs"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use-of-uas",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use-of-uas",
    "title": "UAS Rules and Regulations",
    "section": "Commercial use of UAS",
    "text": "Commercial use of UAS\nAny commercial use in connection with a business, including:\n\n\n\nSelling photos or videos taken from a UAS\nUsing UAS to provide contract services, such as industrial equipment or factory inspection\nUsing UAS to provide professional services, such as security or telecommunications\nUsing UAS to monitor the progress of work your company is performing"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---remote-pilot-requirements",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---remote-pilot-requirements",
    "title": "UAS Rules and Regulations",
    "section": "Commercial use - Remote Pilot requirements",
    "text": "Commercial use - Remote Pilot requirements\n\nMust be at least 16 years of age\nBe able to read, speak, write, and understand English\nBe in a physical and mental condition to safely fly a drone\nPass the initial aeronautical knowledge exam: “Unmanned Aircraft General – Small (UAG)”\n\n\n\n\nMust pass the applicable Transportation Security Administration (TSA) vetting\nHow to obtain a Remote Pilot Airman Certificate (video)"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#remote-pilot-certificate",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#remote-pilot-certificate",
    "title": "UAS Rules and Regulations",
    "section": "Remote Pilot Certificate",
    "text": "Remote Pilot Certificate\n\n\nRequirements\n\nMust be easily accessible by the remote pilot during all UAS operations\nCertificate holders must complete an online recurrent training every 24 calendar months to maintain aeronautical knowledge recency (Effective - April 2021)\nChecklist for Obtaining a Remote Pilot Certificate\nFAA Safety Team Part 107 Small UAS Training Course (login required)"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---uas-requirements",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---uas-requirements",
    "title": "UAS Rules and Regulations",
    "section": "Commercial use - UAS requirements",
    "text": "Commercial use - UAS requirements\n\n\n\nMust weigh less than 55 lbs.\nMust undergo pre-flight check by remote pilot in command (or the person supervising the operation)\n\n\n\n\n\nThe U.S. Court of Appeals for the District of Columbia Circuit invalidated the registration requirement with the decision from May 19th, 2017\nThe registration requirement is back on since 12/13/2017"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---location-requirements",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---location-requirements",
    "title": "UAS Rules and Regulations",
    "section": "Commercial use - location requirements",
    "text": "Commercial use - location requirements\n\n\nClass B/C/D: May operate with permission from local Air Traffic Control\nClass E/G: May operate while following all other regulations"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---location-requirements-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---location-requirements-1",
    "title": "UAS Rules and Regulations",
    "section": "Commercial use - location requirements",
    "text": "Commercial use - location requirements\n\n\nLine Pattern\n\nSolid line – higher class (More Restrictive)\nDashed line – lower class (Less Restrictive)\n\nLine Color\n\nBlue line – higher class (More Restrictive)\nMagenta line – lower class (Less Restrictive)\n\nFAA online course on airspace classification and map\n\n\nThis translates to:\n\nSolid blue – class B, Solid magenta – class C\nDashed blue – class D, Dashed magenta – class E"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---operating-rules",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---operating-rules",
    "title": "UAS Rules and Regulations",
    "section": "Commercial Use - Operating Rules",
    "text": "Commercial Use - Operating Rules\n\nMust fly under 400 feet above ground level (AGL) or, if flying at an altitude higher than 400 feet AGL, stay within 400 feet of a structure\n\n\n\n\nMust not fly from a moving vehicle unless you are in a sparsely populated area\nMust fly at or below 100 mph"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---remote-id",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---remote-id",
    "title": "UAS Rules and Regulations",
    "section": "Commercial Use - Remote ID",
    "text": "Commercial Use - Remote ID\n\nUAS must broadcast identification and location to authorities and nearby operators. (Effective March 2021)\nReal-time tracking and identification of UAS in flight.\nYou can operate without a Remote ID with FAA recognized Identification Area."
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---operating-rules-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---operating-rules-1",
    "title": "UAS Rules and Regulations",
    "section": "Commercial Use - Operating Rules",
    "text": "Commercial Use - Operating Rules\n\n\n\nNighttime operations and flights over people allowed under certain conditions without a waiver (Effective - March 2021)\n\nRequires additional training\nRequires anti-collision lighting for nighttime operations\n\n\n\n\n\n\nMust yield right of way to manned aircraft"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---flight-over-people",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---flight-over-people",
    "title": "UAS Rules and Regulations",
    "section": "Commercial Use - Flight Over People",
    "text": "Commercial Use - Flight Over People\n4 Classes of UAS\nRequired for all Classes\n\nAnit-collision ights\nTraining"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#class-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#class-1",
    "title": "UAS Rules and Regulations",
    "section": "Class 1",
    "text": "Class 1\nWeight Limit: Less than 0.55 pounds (250 grams).\nRequirements:\n\nNo exposed rotating parts that can cause lacerations.\n\nOperations:\n\nCan fly over people without additional restrictions.\n\nExample UAS - DJI Mavic Mini / Mini 2 / Mini 3 Pro"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#class-2",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#class-2",
    "title": "UAS Rules and Regulations",
    "section": "Class 2",
    "text": "Class 2\nWeight Limit: No specific weight limit.\n\nMust not casue injury greater than 11 ft-lbs of kinetic energy.\n\nRequirements:\n\nNo exposed rotating parts that can cause lacerations.\nFollow Remote ID rule\n\nOperations:\n\nCan fly in open-air assemblies\nAllows limited operations over people.\n\nExample UAS\n\nParrot Anafi"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#class-3",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#class-3",
    "title": "UAS Rules and Regulations",
    "section": "Class 3",
    "text": "Class 3\nWeight Limit: No specific limit, but similar to Category 2.\n\nMust not cause injury greater than the severity of injury from 25 ft-lbs of kinetic energy.\n\nRequirements:\n\nNo exposed rotating parts that can cause lacerations.\nFollow Remote ID rule\n\nOperations:\n\nCan fly over “any-person” under certain conditions.\n\nOperating in a controlled environment where everyone is notified the UAS is operating above.\nPersons are part of the UAS team\nPersons are protect by a covered area\n\n\nExample UAS- Can fly in open-air assemblies\n\nDJI Phantom 4 Pro V2.0"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#class-4",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#class-4",
    "title": "UAS Rules and Regulations",
    "section": "Class 4",
    "text": "Class 4\nWeight Limit: No specific weight limit.\nRequirements:\n\nMost restricted\nUAS must have airworthiness certificate issued (same as a crewed aircraft)\nFollow Remote ID rule\n\nOperations:\n\nCan operate over people, including sustained flight over open-air assemblies, subject to additional safety requirements.\n\nExample UAS:\n\nDJI Matrice 300 RTK"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---operating-rules-2",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#commercial-use---operating-rules-2",
    "title": "UAS Rules and Regulations",
    "section": "Commercial Use - Operating Rules",
    "text": "Commercial Use - Operating Rules\n\n\n\nMust keep the UAS in sight (i.e. visual line of sight), either by the remote pilot in command or a visual observer"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#waivers-from-operating-rules-for-commercial-use-of-uas",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#waivers-from-operating-rules-for-commercial-use-of-uas",
    "title": "UAS Rules and Regulations",
    "section": "Waivers from operating rules for commercial use of UAS",
    "text": "Waivers from operating rules for commercial use of UAS\n\nIf you want to operate UAS for commercial purposes outside of these rules, you may apply for a certificate of waiver\nThe FAA will grant waivers if operation can be performed safely but may otherwise not be allowed under Part 107"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#waivable-sections-of-part-107",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#waivable-sections-of-part-107",
    "title": "UAS Rules and Regulations",
    "section": "Waivable sections of part 107",
    "text": "Waivable sections of part 107\n\n\n\nOperation from a moving vehicle or aircraft (§ 107.25)*\nDaylight operation (§ 107.29)\nVisual line of sight aircraft operation (§ 107.31)*\nVisual observer (§ 107.33)\nOperation in certain airspace (§ 107.41)\n\n\n\nYielding the right of way (§ 107.37(a))\nOperation of multiple small unmanned aircraft systems (§ 107.35)\nOperation over people (§ 107.39)\nOperating limitations for small unmanned aircraft (§ 107.51)\n\n\n\nNo waiver of this provision will be issued to allow the carriage of property of another by aircraft for compensation or hire"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#waivable-sections-of-part-107-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#waivable-sections-of-part-107-1",
    "title": "UAS Rules and Regulations",
    "section": "Waivable sections of part 107",
    "text": "Waivable sections of part 107\n\nas of January 2017"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#accident-reporting-107.9",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#accident-reporting-107.9",
    "title": "UAS Rules and Regulations",
    "section": "Accident Reporting (§ 107.9)",
    "text": "Accident Reporting (§ 107.9)\nNo later than 10 days after an operation, a remote pilot in command must report to the Federal Aviation Administration in a manner acceptable to the Administrator, any operation of the small unmanned aircraft involving at least:\n\n\n\nSerious injury to any person or any loss of consciousness; or\nDamage to any property, other than the small unmanned aircraft, unless the cost of repair (including materials and labor) or fair market value of the property does not exceed $500"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#where-not-to-fly",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#where-not-to-fly",
    "title": "UAS Rules and Regulations",
    "section": "Where NOT to fly",
    "text": "Where NOT to fly"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#airspace-restrictions",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#airspace-restrictions",
    "title": "UAS Rules and Regulations",
    "section": "Airspace restrictions",
    "text": "Airspace restrictions\n\n\n\nSecurity Sensitive Airspace Restrictions\nTemporary Flight Restrictions (TFRs)\nRestricted or Special Use Airspace\nStadiums and sporting events\nWildfires\nAirports"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#security-sensitive-airspace-restrictions",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#security-sensitive-airspace-restrictions",
    "title": "UAS Rules and Regulations",
    "section": "Security Sensitive Airspace Restrictions",
    "text": "Security Sensitive Airspace Restrictions\n\n\n\nProhibited all UAS flights within the airspace\nRestrictions extend from the ground up to 400 feet AGL\nApply to all types and purposes of UAS flight operations\nRemain in effect 24 hours a day, 7 days a week\n\n\n\n14 CFR § 99.7"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#temporary-flight-restrictions",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#temporary-flight-restrictions",
    "title": "UAS Rules and Regulations",
    "section": "Temporary Flight Restrictions",
    "text": "Temporary Flight Restrictions\n\n\n\nTFR: Area of airspace where air travel is limited because of a temporary hazardous condition, such as a wildfire or chemical spill; a security-related event\nTFR contains the details about the restriction (size, altitude, time period that it is in effect, and what types of operations are restricted and permitted)\n\n\n\n\nTFR map\nTFR list"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#restricted-or-special-use-airspace",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#restricted-or-special-use-airspace",
    "title": "UAS Rules and Regulations",
    "section": "Restricted or Special Use Airspace",
    "text": "Restricted or Special Use Airspace\n\n\nTypes of Special Use Airspace include: - Prohibited areas - Restricted areas - Warning areas - Military operation areas (MOAs) - Alert areas - Controlled firing areas (CFAs)\n\n\nThe airspace surrounding Washington DC is the most restricted in the country More info"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#stadiums-and-sporting-events",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#stadiums-and-sporting-events",
    "title": "UAS Rules and Regulations",
    "section": "Stadiums and sporting events",
    "text": "Stadiums and sporting events\n\n\nUAS operations are prohibited within a radius of 3 nautical miles of the stadium or venue of:\n\nMajor League Baseball\nNational Football League\nNCAA Division One Football\nNASCAR Sprint Cup, Indy Car, and Champ Series races"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#wildfires",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#wildfires",
    "title": "UAS Rules and Regulations",
    "section": "Wildfires",
    "text": "Wildfires\n\n\nIt is illegal to fly your UAS in or around a wildfire firefighting operation"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#airports",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#airports",
    "title": "UAS Rules and Regulations",
    "section": "Airports",
    "text": "Airports\n\nRecreational operators are required to give notice for flights within 5 miles of an airport to both the airport operator and air traffic control tower, if the airport has a tower\nRecreational operations are not permitted in Class B airspace around most major airports without specific air traffic permission and coordination"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#public-entities",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#public-entities",
    "title": "UAS Rules and Regulations",
    "section": "Public Entities",
    "text": "Public Entities\n\n\nPublicly funded university, law enforcement agency, fire department, or any other federal or state government agency have\nTWO OPTIONS:\n\nFollow the same requirements and operating rules for business users (known as “Part 107”)\n\nOR\n\nOperate UAS for a government entity outside of these rules, you may apply for a blanket public Certificate of Authorization (COA)"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#certificate-of-authorization",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#certificate-of-authorization",
    "title": "UAS Rules and Regulations",
    "section": "Certificate of Authorization",
    "text": "Certificate of Authorization\n\nCan be obtained: - Only by government entities – such as federal and state government agencies, law enforcement agencies, and public colleges and universities - Only for public (governmental) UAS aircraft operations\nAllows: - Flights at or below 400 feet in Class G airspace nationwide - Self-certification of the UAS pilot - The ability to obtain emergency COAs under special circumstances"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#certificate-of-authorization-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#certificate-of-authorization-1",
    "title": "UAS Rules and Regulations",
    "section": "Certificate of Authorization",
    "text": "Certificate of Authorization\n\n\n\nCOAs are issued for a specific period of time, usually two years\nInclude special provisions unique to each proposal, such as a defined block of airspace and time of day sUAS can be used"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#how-to-apply-for-coa",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#how-to-apply-for-coa",
    "title": "UAS Rules and Regulations",
    "section": "How to apply for COA?",
    "text": "How to apply for COA?\n\n\nSince 2009, the FAA has taken steps to streamline the application process by transitioning online\nVisit the FAA website for more information on how to apply for a COA online\nThe average COA processing time is less than 60 days\nExpedited authorization is available in emergency and life-threatening situations\nSample COA Application"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#educational-use",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#educational-use",
    "title": "UAS Rules and Regulations",
    "section": "Educational use",
    "text": "Educational use\n\n\n\nNew legal interpretation from the FAA (May 5, 2016), the use of UAS by students in accredited education institutions as part of their coursework will be allowed under recreational guidelines for model aircraft\nThis does not apply to research efforts, which would too closely link operation of the UAS to faculty members’ professional duties and compensation"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#uas-best-practices",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#uas-best-practices",
    "title": "UAS Rules and Regulations",
    "section": "UAS best practices",
    "text": "UAS best practices\n\n\n\nIf you can, tell other people you’ll be taking pictures or video of them before you do so\nIf someone has a reasonable expectation of privacy, don’t violate it by taking pictures or video, unless you’ve got a very good reason\nDon’t fly over other people’s private property without permission if you can easily avoid doing so"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#uas-best-practices-1",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#uas-best-practices-1",
    "title": "UAS Rules and Regulations",
    "section": "UAS best practices",
    "text": "UAS best practices\n\n\n\nDon’t gather personal data for no reason, and don’t keep it for longer than you have to\nIf you keep sensitive data about other people, secure it against loss or theft\nIf someone asks you to delete personal data about him or her that you’ve gathered, do so, unless you’ve got a good reason not to\n\n\n\n\nDon’t harass people with your drone"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#resources",
    "href": "course/topics/topic_1_uas_basics/lectures/lecture_1b.html#resources",
    "title": "UAS Rules and Regulations",
    "section": "Resources",
    "text": "Resources\n\n\n\nU.S. Air Space map\nKnow Before you Fly website\nRegister your drone\nOperating UAS in North Carolina\nAviation Weather Center"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/part_a_introduction.html#outline",
    "href": "course/topics/topic_1_uas_basics/part_a_introduction.html#outline",
    "title": "A. Introduction to Unmanned Aerial Systems",
    "section": "Outline",
    "text": "Outline\n\noverview of UAS technology, system’s classifications\nUAS components and operations\nUAS applications\nin class demonstration of selected systems"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/part_a_introduction.html#lecture",
    "href": "course/topics/topic_1_uas_basics/part_a_introduction.html#lecture",
    "title": "A. Introduction to Unmanned Aerial Systems",
    "section": "Lecture",
    "text": "Lecture\n\nLecture Slides: Introduction to UAS\nLecture Recording: Fall 2024 (NCSU Only)\nPast Videos (2017)\n\nPart 1\nPart 2\nPart 3\nPart 4\nPart 5\nPart 6"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/part_a_introduction.html#supplemental-materials",
    "href": "course/topics/topic_1_uas_basics/part_a_introduction.html#supplemental-materials",
    "title": "A. Introduction to Unmanned Aerial Systems",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nSystems\n\nFixed wing\n\nsenseFly\neBee Classic\neBee Plus\neBee SQ\n\n\n\nRotary wing\n\nZX5 by Trimble\nInspire 2 by DJI\nMavic 2 by DJI\nPhantom 4 Pro by DJI"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/part_a_introduction.html#assignment",
    "href": "course/topics/topic_1_uas_basics/part_a_introduction.html#assignment",
    "title": "A. Introduction to Unmanned Aerial Systems",
    "section": "Assignment",
    "text": "Assignment\n\nUAS: systems and applications"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/part_a_introduction.html#homework",
    "href": "course/topics/topic_1_uas_basics/part_a_introduction.html#homework",
    "title": "A. Introduction to Unmanned Aerial Systems",
    "section": "Homework",
    "text": "Homework\nPrepare presentation about UAS relevant to your research or interests, describe its technical specifications, main applications and a case study. (Assignment 1A)"
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/assignments/assignment_1a.html#task",
    "href": "course/topics/topic_1_uas_basics/assignments/assignment_1a.html#task",
    "title": "Assignment 1A",
    "section": "Task",
    "text": "Task\nDuring the first lecture you’ve learned about different types of UAS and their applications. Since it is practically impossible to be familiar with all of the existing and emerging UAS your task is to select one system and prepare a short presentation for the class. If you have previous experience with UAS your presentation can describe your project. This way we will be able to learn about a broad range of UAS and their applications.\nThe presentation should be 7 - 10 minutes long and include the following:\n\nName of the UAS\nPhoto and/or diagram of its components\nType\nPlace (country) and year of manufacture\nTechnical specifications (size, weight, endurance, wingspan, payload….)\nMain applications\nCase study that shows how this UAS was used in industry/research/combat/private needs and what kind of outputs/results were obtained\n\nTry to find something relevant to your research interest or just something unique that will be worth sharing."
  },
  {
    "objectID": "course/topics/topic_1_uas_basics/assignments/assignment_1a.html#homework",
    "href": "course/topics/topic_1_uas_basics/assignments/assignment_1a.html#homework",
    "title": "Assignment 1A",
    "section": "Homework",
    "text": "Homework\nUpload your presentation to Moodle, Topic 1. On-campus students will present according to the schedule."
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#outline",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#outline",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Outline",
    "text": "Outline\n\nMotivation for time series data acquisition\nUAS-based monitoring survey design\nProcessing UAS data time series, temporal data framework\nBasic analysis of 3D data time series, volumes\nDynamic visualization of time series\nApplications in crop monitoring, viewsheds"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#uas-for-monitoring-changes",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#uas-for-monitoring-changes",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "UAS for monitoring changes",
    "text": "UAS for monitoring changes\nLow cost and rapid deployment: excellent for monitoring changes at local scale (fields, small watersheds)\n\ncrop monitoring: growth, disease, stress\nerosion processes: coastal, stream bank, rills and gullies\nnatural disasters: flooding, landslides, fire\nmining, construction sites"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#monitoring-design",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#monitoring-design",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Monitoring design",
    "text": "Monitoring design\nAssess:\n\nmetrics to quantify changes: relative height, volume, feature migration\nspatial resolution needed to capture the changes\ntemporal resolution: regular intervals, events\naccessibility: flying over people, line of sight, suitable GCPs\ngeoreferencing, rectification: GCPs distribution and survey"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#processing-time-series",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#processing-time-series",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Processing time series",
    "text": "Processing time series\n\nanalyze and interpolate point clouds: DSMs with aligned, common resolution grids\nraster DSMs are more suitable for time series than TIN\nuse GCPs and permanent features to evaluate accuracy, correct errors and distortions\nassign time stamps and register raster DSMs within GIS-based temporal data framework"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#temporal-data-framework",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#temporal-data-framework",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Temporal data framework",
    "text": "Temporal data framework\n\nsupports efficient processing, management and analysis of space-time data sets\nspace-time dataset is a set of maps (raster, vector) registered in a temporal database\nspace-time dataset may represent a dynamic process\nindividual maps represent the states of the dynamic system at a given time\n\nGebbert, S. and Pebesma, E. (2014). A temporal GIS for field based environmental modeling. Environmental Modelling and Software, 53, 1–12."
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#time-stamp-type",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#time-stamp-type",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Time stamp type",
    "text": "Time stamp type\n\ntime stamp: assigns time to an individual map in space-time data set\ntime instant - snapshot at given time: 2013-10-15 13:00:00 (absolute time)\ntime interval - defined by start and end time: day, month, year (relative time)\na single UAS survey represents a snapshot (state) which can be agreggated into intervals"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#registered-time-series",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#registered-time-series",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Registered time series",
    "text": "Registered time series\nTimeline tool: time and spatial extent of registered maps"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#temporal-plot",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#temporal-plot",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Temporal plot",
    "text": "Temporal plot\n\nPlot time series of elevation values at a given location"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#temporal-count-and-intersection",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#temporal-count-and-intersection",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Temporal count and intersection",
    "text": "Temporal count and intersection\n\nCount: number of maps (temporal snapshots) where the given cell has non-null value (overlap)\nIntersection: grid cells with non-null values from each map in time series"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#temporal-aggregation",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#temporal-aggregation",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Temporal aggregation",
    "text": "Temporal aggregation\n\nTemporally aggregate maps over a given period of time - for example to derive monthly average elevation"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#basic-time-series-analysis",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#basic-time-series-analysis",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Basic time series analysis",
    "text": "Basic time series analysis\nPer cell statistics computed for each cell over the time series:\n\nMean and standard deviation\nMin, max elevation and range\nTime at minimum, time at maximum\nLinear regression: slope, offset, regression coefficient\n\nReference:\nMitasova, H., Hardin, E., Overton, M., and Harmon, R.S., 2009, New spatial measures of terrain dynamics derived from time series of lidar data, Proc. 17th Int. Conf. Geoinformatics, Fairfax, VA."
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#basic-time-series-analysis-core-envelope",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#basic-time-series-analysis-core-envelope",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Basic time series analysis: core, envelope",
    "text": "Basic time series analysis: core, envelope\n\nCore surface: min. elevation measured for each cell\nEnvelope: max. elevation measured for each cell\n\nExample: cutting plane with lidar, core and envelope"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#detecting-surveys-with-large-distortions",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#detecting-surveys-with-large-distortions",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Detecting surveys with large distortions",
    "text": "Detecting surveys with large distortions\n\nDerive core surface from UAS time series\nCompute difference between UAS core and lidar bare ground surface\nCompute time of minimum raster to identify the distorted DSMs with elevation well below lidar bare ground"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#corrected-uas-core-surface",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#corrected-uas-core-surface",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Corrected UAS core surface",
    "text": "Corrected UAS core surface\n\nRemove distorted DSMs from derivation of core surface\nDifference between UAS core and lidar bare ground surface is now very small\nHist. equalized color ramp for differences highlights a small shift in lidar swath"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#envelope-and-range-applications",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#envelope-and-range-applications",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Envelope and range applications",
    "text": "Envelope and range applications\n\nEnvelope: what is the max height of crop in each pixel over the monitored period?\nTime of maximum - when was the crop highest at each grid cell?\nWhere is the largest range and variability in crop height?"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#envelope-and-range-applications-1",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#envelope-and-range-applications-1",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Envelope and range applications",
    "text": "Envelope and range applications\n\nUse envelope to show all cars ever parked at the site\nCore, snapshot, envelope, surfaces can be used to manage parking area"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#basic-time-series-analysis-regression",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#basic-time-series-analysis-regression",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Basic time series analysis: regression",
    "text": "Basic time series analysis: regression\n\nApplies to well designed, systematic monitoring with longer time series\nSelect subset where the changes are close to linear - e.g. crop growth period\nCompute per cell linear regression analysis: map of regression slope and offset"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#map-algebra-for-time-series",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#map-algebra-for-time-series",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Map algebra for time series",
    "text": "Map algebra for time series\n\nApply map algebra expression for each map in the time series at each grid cell\nOutput is new time series which is registered as a new space-time dataset\nThis is different (and much simpler) from temporal map algebra"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#extract-crop-height-time-series",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#extract-crop-height-time-series",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Extract crop height time series",
    "text": "Extract crop height time series\nMap algebra can be used to extract grid cells with elevation above lidar bare ground within a selected elevation interval (z1,z2)\n  \nMaps of relative elevation (0.3m, 2.0m) above bare ground lidar for 3 time snapshots in our time series. The middle map shows distortions rather than crop."
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#estimate-crop-volume",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#estimate-crop-volume",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Estimate crop volume",
    "text": "Estimate crop volume\nSummary statistics can be used to estimate above ground crop biomass"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#estimate-volume-of-structures",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#estimate-volume-of-structures",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Estimate volume of structures",
    "text": "Estimate volume of structures\n\nCompute volume based on difference between bare ground and above ground feature, such as building"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#analysis-viewsheds",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#analysis-viewsheds",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Analysis: Viewsheds",
    "text": "Analysis: Viewsheds\nEvaluate influence of vegetation on the viewshed area\n\nSpatial extent of viewshed changes over time depending on the height of surrounding vegetation, different colors show the spatial extent of viewshed at different times"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#analysis-viewsheds-1",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#analysis-viewsheds-1",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Analysis: Viewsheds",
    "text": "Analysis: Viewsheds\nProvide analysis to support siting of a monitoring webcam\n\nSpatial extent of viewshed changes over time depending on the height of surrounding vegetation"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#dynamic-visualization",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#dynamic-visualization",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "Dynamic visualization",
    "text": "Dynamic visualization\n\nSee Terrain time series visualization in the GRASS temporal workshop\nWe covered the basics in the flow modeling example"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#what-we-have-learned",
    "href": "course/topics/topic_5_Advanced Analytics/lectures/lecture_5b.html#what-we-have-learned",
    "title": "Analysis of multitemporal UAS data and its applications",
    "section": "What we have learned",
    "text": "What we have learned\n\nUAS 3D monitoring basic considerations\nTemporal framework concept\nComputing core and envelope and its application\nIdentification of distorted DSMs in time series\nEstimation of volumes\nAnalyzing changing viewsheds\nDynamic visualization"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5a.html#outline",
    "href": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5a.html#outline",
    "title": "Assignment 5A",
    "section": "Outline",
    "text": "Outline\n\nanalyze the differences between the lidar (2013, 2015 surveys) and UAS DEM/DSM\nfuse selected lidar and UAS DSM\nevaluate impact of fusion technique on water flow simulation"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5a.html#data",
    "href": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5a.html#data",
    "title": "Assignment 5A",
    "section": "Data",
    "text": "Data\nYou should have everything already for Mid Pines area, get anything missing from the Course logistics web page\n\nGRASS Project: LakeWheeler_NCspm\n2013 lidar interpolated DEM and DSM: download, use r.unpack."
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5a.html#tools",
    "href": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5a.html#tools",
    "title": "Assignment 5A",
    "section": "Tools",
    "text": "Tools\n\nGRASS GIS 8.4\nGRASS Addon: r.patch.smooth\n\nInstall r.patch.smooth with the following command:\ng.extension r.patch.smooth"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5a.html#workflow",
    "href": "course/topics/topic_5_Advanced Analytics/assignments/assignment_5a.html#workflow",
    "title": "Assignment 5A",
    "section": "Workflow",
    "text": "Workflow\nFirst we need to evaluate the DSMs to find out which are suitable for fusion\n\nCompute the difference between a selected lidar DSM and selected UAS DSMs: entire raster, along a stable linear feature (road and roof).\nUse the results to evaluate whether there is a significant systematic error, such as vertical shift in profiles along the road.\nUse the raster differences to identify distortions, look for unexpected patterns, verify with profile, and exclude distorted areas from study area.\nYou have lidar DEM, DSM and a low-distortion subset of UAS DSM:\n\nCreate a new DSM for the entire area using two approaches and compare how the two approaches work for water flow modeling.\nCombine parts of UAS and lidar DEM to create bare ground, model water flow and compare with water flow modeled on DSM.\n\n\n\n1. Analyze the difference between the lidar and UAS DSM\nDownload the suggested color table for differences, used in the workflow as dif_lidar_uav.txt.\nIf you run r.colors from command line, make sure the color table file is in your current working directory.\n -40 red\n -1 orange\n -0.5 yellow\n -0.1 grey\n 0 white\n 0.1 grey\n 0.5 cyan\n 1 aqua\n 35 blue\nDownload lidar DSM and DEMs if you haven’t done already, unzip it in your working directory and unpack:\nr.unpack -o input=mid_pines_lidar2013_dsm.pack\nr.unpack -o input=mid_pines_lidar2013_dem.pack\nCompute the differences between the lidar and UAS DSMs and evaluate their spatial distribution and\nsummary statistical properties:\ng.region rast=mid_pines_lidar2013_dsm -p\nr.mapcalc \"diff_lidardsm_agi_june = mid_pines_lidar2013_dsm - 2015_06_20_DSM_agi_11GCP\"\nr.colors diff_lidardsm_agi_june rules=dif_lidar_uav.txt\nr.univar diff_lidardsm_agi_june\nr.mapcalc \"diff_lidardsm_pix4d_june = mid_pines_lidar2013_dsm - 2015_06_20_pix4d_11GCP_dsm\"\nr.colors diff_lidardsm_pix4d_june rules=dif_lidar_uav.txt\nr.univar diff_lidardsm_pix4d_june\nEvaluate possible systematic shift along stable features using the following commands:\nr.profile -g input=diff_lidardsm_agi_june output=road.txt coordinates=637208,219491,637059,219734\nv.in.ascii input=road.txt output=profile_road separator=space columns=\"x double,y double,profile double,diff double\"\nv.univar map=profile_road column=diff\nFor more visual comparison use GUI Profile Tool.\n\nIn Map Display toolbar, find Analyze map - Profile surface map and select raster maps mid_pines_lidar2013_dsm, 2015_06_20_DSM_agi_11GCP, and 2015_06_20_pix4d_11GCP_dsm.\nIn Profile toolbar, use the second tool Draw transect in map display window and then click in Map Display on the road where you want to start the profile and then where you want to end it.\nUse Plot options to show legend and to set colors of the DSM profiles (look for the yellow wheel icon).\n\n\n\n\n\n\n\nWarning\n\n\n\nWhy are the distortions / errors low along the road but high in the fields?\n\n\nBecause the shift in the lidar and UAS DSM is small we will ignore it in our first set of fusion examples, but we will use the analysis above to select the region with minimal distortion in the UAS DSM and compute a clipped DSM:\ng.region n=219720.9 s=219257.1 w=636762.9 e=637190.4 res=0.3 -p\nr.mapcalc \"2015_06_20_DSM_agi_11GCP_cl = 2015_06_20_DSM_agi_11GCP\"\n\n\n2. Patch and fuse the lidar and UAS DSM\nFor the rest of the assignment we will use rasters 2015_06_20_DSM_agi_11GCP_cl, mid_pines_lidar2013_dsm, and mid_pines_lidar2013_dem. To make the instructions more readable, we will create a copy of these rasters with short names:\ng.copy raster=2015_06_20_DSM_agi_11GCP_cl,uas\ng.copy raster=mid_pines_lidar2013_dsm,lidar_dsm\ng.copy raster=mid_pines_lidar2013_dem,lidar_dem\nPatch UAS DSM with lidar DSM to cover the entire Mid Pines study area and use shaded relief to check for edges:\ng.region rast=lidar_dsm -p\nr.patch uas,lidar_dsm out=lid_uas_patch\nr.relief lid_uas_patch out=lid_uas_patch_relief zscale=5\nWe have visible discontinuity along the patch edges. Therefore we will use weighted average to smooth out the overlap with the following workflow. We will use two different overlap widths — 5 and 20 meters — compare the results and try to explain how the smoothing works in your report:\nr.grow.distance -n input=uas distance=distance\nr.mapcalc \"patched_smooth_5 = if(distance &gt; 5, uas, if(distance == 0, lidar_dsm, (1 - distance/5) * lidar_dsm + (distance/5 * uas)))\"\nr.mapcalc \"patched_smooth_20 = if(distance &gt; 20, uas, if(distance == 0, lidar_dsm, (1 - distance/20) * lidar_dsm + (distance/20 * uas)))\"\nr.relief patched_smooth_5 out=patched_smooth_5_relief zscale=5\nr.relief patched_smooth_20 out=patched_smooth_20_relief zscale=5\n\n\n3. Create bare ground by fusing UAS DSM and lidar DEM\nTo properly model water flow on the UAS DSM, we replace vegetation captured in the DSM with lidar bare ground (lidar_dem). We extract only those parts of UAS DSM where it is higher than lidar ground by 30 cm:\ng.region rast=uas -p\nr.mapcalc \"uas_ground = if(uas - lidar_dem &lt; 0.3, uas, null())\"\nNow we will fuse lidar and the UAS ground using the same method as above. This time, we will use addon r.patch.smooth:\ng.extension r.patch.smooth\nIn the first case we will use overlap width 10m:\ng.region rast=lidar_dsm -p\nr.patch.smooth input_a=uas_ground input_b=lidar_dem output=lidar_uas_ground_smooth smooth_dist=10\nr.relief lidar_uas_ground_smooth out=lidar_uas_ground_smooth_relief zscale=5\nIn the second case we will use an alternative approach (using -s flag) which uses spatially variable overlap width based on the elevation differences along the borders (see r.patch.smooth manual):\nr.patch.smooth -s input_a=uas_ground input_b=lidar_dem output=lidar_uas_ground_smooth2 overlap=overlap transition_angle=1.3 parallel_smoothing=13\nr.relief lidar_uas_ground_smooth2 out=lidar_uas_ground_smooth2_relief zscale=5\nAdd raster overlap to see the spatially variable overlap.\n\n\n4. Compare surface flow pattern on patched and fused DSMs\nThe aim of the processing is to investigate the flow pattern simulated on different DSMs and DEMs. The lidar 2013 and the timeseries of UAS-based DSMs will be used as input for simulations using r.sim.water.\nFirst, we examine the watershed boundaries to see which area to include:\ng.region rast=lidar_dem -p\nr.watershed elevation=lidar_dem threshold=50000 accumulation=flowacc basin=basin\nr.to.vect -t input=basin output=basin type=area\nWe will compute water flow on our fused DEM within our selected region. We need first order spatial derivatives which can be derived either using r.slope.aspect or during interpolation phase with v.surf.rst.\ng.region n=219728.7 s=219435 w=636708.9 e=637122.6 res=0.3 -pa\nr.slope.aspect elevation=lid_uas_patch dx=lid_uas_patch_dx dy=lid_uas_patch_dy\nThe simulation writes water depth and discharge raster maps so we can look at first results soon although the simulation takes a lot of time. Depending on your processing power, you can make the region smaller.\nr.sim.water -t elevation=lid_uas_patch dx=lid_uas_patch_dx dy=lid_uas_patch_dy rain_value=30 man_value=0.15 depth=lid_uas_patch_depth discharge=lid_uas_patch_disch nwalkers=100000 niterations=20 output_step=1 hmax=0.2 halpha=8.0 hbeta=1.0\nNow we will compute the water flow simulation on the DSM patched using smooth fusion:\nr.slope.aspect elevation=patched_smooth_20 dx=patched_smooth_20_dx dy=patched_smooth_20_dy\nr.sim.water -t elevation=patched_smooth_20 dx=patched_smooth_20_dx dy=patched_smooth_20_dy rain_value=30 man_value=0.15 depth=patched_smooth_20_depth discharge=patched_smooth_20_disch nwalkers=100000 niterations=20 output_step=1 hmax=0.2 halpha=8.0 hbeta=1.0\nNow compare the results (lid_uas_patch_depth.20 and patched_smooth_20_depth.20) at the edge of the datasets.\nFinally, we will simulate water flow on the bare ground fused from UAV DSM and lidar DEM:\nr.slope.aspect elevation=lidar_uas_ground_smooth dx=lidar_uas_ground_smooth_dx dy=lidar_uas_ground_smooth_dy\nr.sim.water -t elevation=lidar_uas_ground_smooth dx=lidar_uas_ground_smooth_dx dy=lidar_uas_ground_smooth_dy rain_value=30 man_value=0.15 depth=ground_depth discharge=ground_disch nwalkers=100000 niterations=20 output_step=1 hmax=0.2 halpha=8.0 hbeta=1.0\nCompare ground_depth.20 with the previous results.\n\nAnimation and visualisation the results\nGRASS GIS Animation Tool allows to create animations from series of raster and vector maps and export animated gifs.\nPick one or more of the simulated flows and load all the corresponding *_depth maps to the animation tool. The water flow pattern is better visible in 2d when it’s draped over a shaded relief, so add corresponding relief raster and set its transparency to 50% (last icon in Add new animation/Edit animation dialog).\n\nExport the animation as GIF file.\nLook at this youtube video to see how to control Animation Tool."
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/part_a_UAS_lidar_data_comparison_fusion_analysis.html#outline",
    "href": "course/topics/topic_5_Advanced Analytics/part_a_UAS_lidar_data_comparison_fusion_analysis.html#outline",
    "title": "A. UAS and lidar data: comparison, fusion and analysis",
    "section": "Outline",
    "text": "Outline\n\nAnalysis of lidar and UAS DSMs differences\nPatching and smooth fusion of UAS and lidar DSM\nOverland flow simulation on fused DEM"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/part_a_UAS_lidar_data_comparison_fusion_analysis.html#lecture",
    "href": "course/topics/topic_5_Advanced Analytics/part_a_UAS_lidar_data_comparison_fusion_analysis.html#lecture",
    "title": "A. UAS and lidar data: comparison, fusion and analysis",
    "section": "Lecture",
    "text": "Lecture\n\nLecture Slides\nLecture Recording: Fall 2024 (NCSU Only)\nIn-Class Recording: NA"
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/part_a_UAS_lidar_data_comparison_fusion_analysis.html#supplemental-materials",
    "href": "course/topics/topic_5_Advanced Analytics/part_a_UAS_lidar_data_comparison_fusion_analysis.html#supplemental-materials",
    "title": "A. UAS and lidar data: comparison, fusion and analysis",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nDrone LiDAR vs Photogrammetry: A Technical Guide, brief article from SPAR3D (online news on 3D technology: sensing, processing and visualization)\nFusion of multi-scale DEMs using a regularized super-resolution method. Linwei Yue , Huanfeng Shen , Qiangqiang Yuan , Liangpei Zhang. International Journal of Geographical Information Science. Vol. 29, Iss. 12, 2015\nPetrasova et al. 2017, Fusion of high-resolution DEMs for water flow modeling. Open Geospatial Data, Software and Standards 2:6."
  },
  {
    "objectID": "course/topics/topic_5_Advanced Analytics/part_a_UAS_lidar_data_comparison_fusion_analysis.html#assignment-homework",
    "href": "course/topics/topic_5_Advanced Analytics/part_a_UAS_lidar_data_comparison_fusion_analysis.html#assignment-homework",
    "title": "A. UAS and lidar data: comparison, fusion and analysis",
    "section": "Assignment & Homework",
    "text": "Assignment & Homework\nAssignment 5A: Fusion of UAS and lidar data\n\nOptional, advanced topic: Python Scripting for UAS/lidar analysis in GRASS GIS"
  },
  {
    "objectID": "course/topics/topic_2_sfm/lectures/lecture_2a.html#ground-control-points-1",
    "href": "course/topics/topic_2_sfm/lectures/lecture_2a.html#ground-control-points-1",
    "title": "Photogrammetry and Structure from Motion Concepts",
    "section": "Ground Control Points",
    "text": "Ground Control Points\n\n\nPre-marked (Panels): marking or painting figures or symbols on the ground before the UAS flies"
  },
  {
    "objectID": "course/topics/topic_2_sfm/part_a_images_to_3d.html#outline",
    "href": "course/topics/topic_2_sfm/part_a_images_to_3d.html#outline",
    "title": "A. From images to 3D models: Photogrammetry and Structure from Motion concepts",
    "section": "Outline:",
    "text": "Outline:\n\nprinciples of photogrammetry\nUAS low range flight and consumer camera image acquisition\nlateral and forward overlap, stereoscopic coverage\nprinciples of Structure from Motion\n3D reconstruction from multiple overlapping imagery"
  },
  {
    "objectID": "course/topics/topic_2_sfm/part_a_images_to_3d.html#lecture",
    "href": "course/topics/topic_2_sfm/part_a_images_to_3d.html#lecture",
    "title": "A. From images to 3D models: Photogrammetry and Structure from Motion concepts",
    "section": "Lecture",
    "text": "Lecture\n\nLecture Slides: Photogrammetry and SfM\nLecture Recording: Fall 2024 (NCSU Only)\nIn-Class Recording: Fall 2024 (NCSU Only)"
  },
  {
    "objectID": "course/topics/topic_2_sfm/part_a_images_to_3d.html#supplemental-materials",
    "href": "course/topics/topic_2_sfm/part_a_images_to_3d.html#supplemental-materials",
    "title": "A. From images to 3D models: Photogrammetry and Structure from Motion concepts",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nLecture Slides 2016, learn about history of aerial mapping\nISPRS 2016 paper on Reconstruction of Palmyra from tourist photos\nISPRS 2016 paper on Reconstruction of popular monuments from crowdsourced photos\nOpen Heritage - 3D models of iconic locations"
  },
  {
    "objectID": "course/topics/topic_2_sfm/part_a_images_to_3d.html#assignment",
    "href": "course/topics/topic_2_sfm/part_a_images_to_3d.html#assignment",
    "title": "A. From images to 3D models: Photogrammetry and Structure from Motion concepts",
    "section": "Assignment",
    "text": "Assignment\nPerform an experiment to investigate impact of the number and orientation of photos, light, as well as shape and features of a photographed object on its 3D reconstruction from multiple overlapping imagery.\n\nLab Experiment: 3D reconstruction from multiple overlapping imagery"
  },
  {
    "objectID": "course/topics/topic_2_sfm/part_a_images_to_3d.html#homework",
    "href": "course/topics/topic_2_sfm/part_a_images_to_3d.html#homework",
    "title": "A. From images to 3D models: Photogrammetry and Structure from Motion concepts",
    "section": "Homework",
    "text": "Homework\nPrepare short report on your 3D modeling experiment.\n3D models from previous courses are available on OsgeoLab Skechfab account. If you would like your model to be added to the collectionl please indicate at the end of your report “I agree for my 3D model to be uploaded to the OsgeoLab Skechfab collection and I want(/don’t want) my name to be listed as an author in the model description” and submit on through moodle your 3D model (preferably in the .obj format with accompaning .mtl and .jpg files, or check other formats supported by Sketchfab)"
  },
  {
    "objectID": "course/topics/topic_2_sfm/sfm_demo.html#structure-from-motion-example",
    "href": "course/topics/topic_2_sfm/sfm_demo.html#structure-from-motion-example",
    "title": "Structure from Motion Visualization",
    "section": "Structure from Motion Example",
    "text": "Structure from Motion Example\n\n\n\n\n\nStructure from Motion Visualization - Correctly Positioned Anchor Points"
  },
  {
    "objectID": "course/topics/topic_2_sfm/assignments/assignment_2b.html#geoprocessing-of-the-uas-data",
    "href": "course/topics/topic_2_sfm/assignments/assignment_2b.html#geoprocessing-of-the-uas-data",
    "title": "Assignment 2B",
    "section": "Geoprocessing of the UAS data",
    "text": "Geoprocessing of the UAS data\nCompleting this assignment you will generate orthomosaic and Digital Surface Model using pictures taken from the UAS Trimble UX5 Rover (Flight mission executed on September 22nd 2016). The Study area is located at Lake Wheeler COA. Additionally you will be able to see the processing results in the generated report and optionally you will be able to export also 3D model and Point cloud as well as Camera calibration and orientation data.\nThe process can be very time consuming (depending on computational power of your device and desired quality). In order to minimize the processing time, we will process only fraction of the data collected and we will use imagery downsampled by 50%. It will allow us to generate outputs in the classroom.\n\n\nGeneral workflow (with GCPs)\nPreparation\n\nStage 1: Building simple geometry (in order to place GCPs)\nStage 2: Placing GCPs\nStage 3: Building complex geometry\nStage 4: Exporting results\n\n\n\nPreparation\n\nData\n\nOPTION 1 - Small area\nYou can use a smaller area to accelerate processing. The photos are not downsampled in order to obtain the highest resolution of the outputs.\n\ndownload data - photos, log and coordinates of 3 GCPs\n\n\n\nOPTION 2 - Full flight, photos original resolution\n\n\n\n\n\n\nWarning\n\n\n\nLONG PROCESSING TIME!\n\n\n\ndownload data - photos, log and coordinates of 12 GCPs\n\n\n\nOPTION 3: You can use your own data\n\n\n\nSoftware\n\nAgisoft Metashape Professional (installer)\n\n\n\nPreferences\nWhen launching Metashape for the first time, some settings need to be adjusted to optimize performance. These settings need to be done only once, at the first use of Metashape, and are loaded by default in subsequent sessions.\n\nMenu &gt; Tools &gt; Preferences\nGeneral tab:\n\nLeave default (optional – write log to file: enabled, if you want to save the txt log file)\n\nGPU tab:\n\nCheck the USE CPU when performing GPU accelerated processing option at the bottom\n\nAppearance Tab:\n\nLeave default (or adjust to personal taste)\n\nNavigation Tab:\n\nLeave default (or adjust to personal taste)\n\nAdvanced tab:\n\nProject Files\n\nKeep key points: disabled\nKeep depth maps: disabled\nStore absolute image paths: disabled\n\nExport/Import: enable all\nMiscellaneous:\n\nEnable fine level task division: enabled\n\n\n\n\n\nPreferences\n\n\n\n\n\n\nStage 1: Aligning Photos\nIn order to localize the GCPs first a preliminary simple model needs to be built.\n\nAdding photos\n\nMenu &gt; Workflow &gt; Add Photos\nIndicate the path to the folder (downloaded data) containing photos and select them all. In the Reference pane, you can see that the photos have been loaded, but the coordinate system indicates local coordinates. It can be changed in the Settings.\nClick the settings icon  &gt; change coordinate system to WGS 84 (EPSG:4326).\n\n\n\n\nLoading camera positions\nClick Import button  on the Reference pane toolbar &gt; select file containing camera positions information (2016_09_22_sample.txt) in the Open dialog.\n\n\n\n\n\n\n\nNote\n\n\n\nAgisoft supports the camera orientation files in 5 formats: .csv, .txt, .tel, .xml, and .log. The default format of the Trimble Aerial Imaging log is .jxl. In order to convert the Trimble .jxl log file please run script by Vaclav Petras or use provided already converted log in .txt format.\n\n\nMake sure that the columns are named properly – you can adjust their placement by indicating column number (top right of dialog box).\n\n\nAligning photos\n\nAccuracy: Low\nCheck: Reference preselection\nKey point limit: default\nTie point limit: default\nAdaptive camera model fitting: enabled\n\n\nMenu &gt; Workflow &gt; Align Photos\n\nClick Save  in the toolbar.\n\n\n\n\nStage 2: Placing GCPs\n\nLoading the GCPs coordinates\nClick Import button  on the Reference pane toolbar &gt; select the file containing Ground Control Points (GCP_3.txt) coordinates in the Open dialog.\n\nMake sure that the columns are named properly – you can adjust their placement by indicating column number (top right of dialog box). This time you uncheck the Rotation box since GCPs are stationary and do not need determining yaw pitch and roll angles.\nThe window with the message: Can’t find match for ‘UAV 3’ entry. Create new marker? will pop up.\n\nChoose ‘Yes to All’ – it will create new marker for each of the named GCPs from the file. They will be listed in Reference pane under the list of photos.\n\n\nIndicating GCPs on the pictures\nNow you need to find each of the GCPs and indicate its localization on all photos depicting it. You can also see the process on the instructional video.\nThe Model pane shows approximate positions of GCPs, it is better visible if the ‘Show cameras’ option is disabled (Menu &gt; View &gt; Show/Hide Items &gt; Show cameras).\n \nChoose in the context menu of the selected point on the list (right click) &gt; Filter Photos by Marker.\n\nIn the Photos pane appear only the images in which the currently selected GCP is probably* visible.\n\n\n\n\n\n\nNote\n\n\n\n*This is possible because our pictures are geotagged (log indicates the position of each photo), if you will be working with the pictures that are not geotagged or in the area that you don’t know and are not able to “guess” which photo depicts which GCP, it is useful to build the dense point cloud or even mesh and texture (see the following steps) to see the preliminary model of the area in the Model pane, not just a sparse cloud (this is depicted on the figures above).\n\n\nOpen an image by double clicking the thumbnail. It will open in a tab next to the Model pane. The GCP will appear as a grey icon . This icon needs to be moved to the middle of GCP visible on the photo.\n\nDrag the marker to the correct measurement position. At that point, the marker will appear as a green flag, meaning it is enabled and will be used for further processing.\nDouble click on the next photo and repeat the steps. As soon as the GCP marker position has already been indicated on at least two images, the proposed position will almost exactly match the point of measurement. You can now slightly drag the marker to enable it (turning it into a green flag) or leave it unchanged (gray marker icon) to exclude it from processing.\nFilter photos by each marker again. Agisoft adjusts the GCPs positions on the run, so you can locate the GCP on additional images that will appear in the Photos pane.\nIn this sample processing we will include 3 GCPs. Mathematically, you need to indicate marker positions for at least 3 GCPs. Accurate error estimates can be calculated with at least 4 GCPs, while often at least 5 are needed to cover the center of the project as well, which reduces the chance of error propagation and resulting terrain distortions especially on flat or undulating terrain types.\nClick Save  in the toolbar.\n\n\nOptimizing alignment\nYou can see the errors by clicking on the View Errors icon . Best results are obtained when the alignment is first optimized based on the camera coordinates only, and a second time based on the GCP only.\nClick Optimize icon  in the Ground Control toolbar (check all the boxes).\n\n\n\n\n\n\nWarning\n\n\n\nDepending on the version of Metashap the optimize icon may appear as a star.\n\n\n\n\nOptimizing based on the GCP Markers\nIn the Ground Control pane:\n\ndisable all the camera coordinates (select one &gt; press Ctrl+A &gt; right-click &gt; choose Uncheck).\nenable all the GCP Marker coordinates (select one &gt; press Ctrl+A &gt; right-click &gt; choose Check).\n\nClick Settings icon  in the Ground Control toolbar:\n\nleave the default values (Marker accuracy (m) = 0.005 m)\n\nClick Optimize icon  in the Ground Control toolbar (leave all options at the default).\nClick Save  in the toolbar.\nYou can now see how much the errors were reduced through optimization by clicking on the View Errors icon .\n\n\n\nStage 3: Building complex geometry\nBefore starting the Build Dense Cloud step it is recommended to check the bounding box of the reconstruction (to make sure that it includes the whole region of interest, in all dimensions). The bounding box should also not be too large (increased processing time and memory requirements).\nThe bounding box can be adjusted using the Resize Region  and the Rotate Region  tool from the toolbar. Make sure that the base (red plane) is at the bottom.\n\nThe next steps will be time consuming (depending on the desired quality and number of pictures). You can execute them one step at the time or you can also set a batch processing that does not require user interaction until the end of geoprocessing (especially useful in case of Ultra High quality that requires even several days of processing). The batch processing will be explained at the end of this section.\n\nMenu &gt; Workflow &gt; Build Dense Cloud (Build Point Cloud)\n\nQuality: Medium\nDepth filtering: Aggressive\n\n\n\nMedium (will downsample the images to get a 3D coordinate every 4 pixels)\nHigh (will downsample the images to get a 3D coordinate every 2 pixels; typically takes several hours but will give acceptable results for most cases)\nUltra high (will calculate a 3D coordinate for every pixel in original imagery; may take more than a day or several days)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen working with large datasets take into consideration the RAM size requirements for the different target qualities with respect to the number of images (in appendix in the manual).\n\n\n\nClick Save  in the toolbar\n\n\nMenu &gt; Workflow &gt; Build Mesh (Build Model)\n\nSource data: Dense cloud or Point cloud\nSurface type: Height field\nFace count: Medium\nAdvanced options: leave default\n\n\n\n\n\n\n\nNote\n\n\n\nFace count set at “0” means that Metashape will determine an optimum number of faces (but this may not be enough to describe all the features on terrain)\n\n\n\n\n\nMenu &gt; Workflow &gt; Build Texture\n\nTexture Type: Diffuse Map\nSource Data: Images\nMapping mode: Orthophoto\nBlending mode: Mosaic (default)\nTexture size/count: 4096\nEnable color correction: disabled (unchecked) (Not visible in all versions)\n\nClick Save  in the toolbar\n\n\n\nEditing geometry\nSometimes it is necessary to edit geometry before building texture atlas and exporting the model.\nIf the overlap of the original images was not sufficient, the model can contain holes. In this case, to obtain a holeless model, use the Close Holes command. It is crucial if you want to perform any volume calculations - in this case, 100% holes need to be closed.\n\nMenu &gt; Tools &gt; Mesh (Model) &gt; Close Holes\nIn the Close Holes dialog, select the size of the largest hole to be closed (in percentage of the total model size).\n\n\n\n\n\n\nWarning\n\n\n\nDO not choose 100% unless you are calculating volumes (then you have to close 100% holes)\n\n\n\nThis does not apply to our data, since we have sufficient image overlap. But due to strong dependency on weather conditions, the holes in data are common with UAS.\n\n\n\nBuilding DEM\n\nMenu &gt; Workflow &gt; Build DEM\nIn this step, the parameters of the exported DEM are determined. In order to work in the commonly used in NC reference system, we will not keep the project’s WGS84 system, but change it to NC State Plane (ESPG:3358).\n\n\n\n\nBuilding Orthophoto\n\nMenu &gt; Workflow &gt; Build Orthophoto (Orthomosaic)\nOrthophoto can be built only in the same coordinate system as the DEM.\n\n\n\n\nBatch processing\n\nMenu &gt; Workflow &gt; Batch process &gt; Add\nBatch processing allows setting multiple processes in the preset order and executing them one after another without user intervention. All the parameters should be set the same as explained above.\n\nOptimize Alignment (Align Photos) - if the GCPs were placed but the optimization not performed, otherwise: skip\nBuild Dense Cloud (Point cloud)\nBuild Mesh (Build Model)\nBuild Texture\nBuild DEM\nBuild Orthophoto (Build Orthomosaic)\n\n\n\n\n\n\n\nStage 4: Exporting results\nThis option will be disabled if you are working in the demo version.\n\nOrthomosaic\n\nFile &gt; Export Orthophoto &gt; Export JPEG/TIFF/PNG\n\nProjection Type: geographic (default)\nDatum: WGS 1984\nWrite KML file (footprint) and World file (.tfw) = check if desired (if left unchecked, georeferencing information will still be contained in the GeoTIFF .tif file)\nBlending mode: Mosaic (default)\nPixel size: leave default\n\n\nLeave default values. Fill out the desired name and save as type TIFF/GeoTIFF (*.tif).\n\n\n\nDigital surface model\n\nMenu &gt; File &gt; Export DEM\n\nLeave default values. Fill out the desired name and save as type TIFF/GeoTIFF (*.tif).\n\n\n\nGenerating report\n\nMenu &gt; File &gt; Generate report\nIndicate the name and path for the PDF file.\n\nProcessing outputs from OPTION 1 (downsampled images) are provided here.\n\nThe following export steps are optional.\n\n\n\nModel\n\nMenu &gt; File &gt; Export model &gt; OBJ/FBX/KMZ\nIndicate the name, path, and format of the output model.\nMetashape supports model export in the following formats: Wavefront OBJ, 3DS file format, VRML, COLLADA, Stanford PLY, STL models, Autodesk FBX, Autodesk DXF, Google Earth KMZ, U3D, Adobe PDF. Some file formats (OBJ, 3DS, VRML, COLLADA, PLY, FBX) save texture images in a separate file. The texture file should be kept in the same directory as the main file describing the geometry.\n3D Models are a great visualization tool. Model generated by following the assignment instructions as well as model from full flight processing can be viewed on Sketchfab.\n\n\n\nPoint cloud\n\nMenu &gt; File &gt; Export points\nIndicate the name, path, and format of the output file.\nMetashape supports point cloud export in the following formats: Wavefront OBJ, Stanford PLY, XYZ text file format, ASPRS LAS, ASTM E57, U3D, Potree, Metashape OC3, PDF.\n\n\n\nCamera calibration and orientation data\n\nMenu &gt; Tools &gt; Export &gt; Export Cameras\nIndicate the name, path, and format of the output file.\nMetashape supports camera data export in the following formats: Metashape structure file format (XML based), Bundler OUT file format, CHAN file format, Boujou TXT file format, Omega phi Kappa text file format, PATB Exterior orientation, BINGO Exterior orientation, AeroSys Exterior orientation, Inpho project file."
  },
  {
    "objectID": "course/topics/topic_2_sfm/assignments/assignment_2b.html#render-3d-model",
    "href": "course/topics/topic_2_sfm/assignments/assignment_2b.html#render-3d-model",
    "title": "Assignment 2B",
    "section": "Render 3D Model",
    "text": "Render 3D Model\nHere is an exmaple of the rendered 3D model exported as a gltf file using Three.js\n\nheight = 600;\nwidth = 800;\n\nTHREE = {\n  const THREE = window.THREE = await require(\"three@0.130.0/build/three.min.js\");\n  await require(\"three@0.130.0/examples/js/controls/OrbitControls.js\").catch(() =&gt; {});\n  await require(\"three@0.130.0/examples/js/loaders/OBJLoader.js\").catch(() =&gt; {});\n  await require(\"three@0.130.0/examples/js/loaders/GLTFLoader.js\").catch(() =&gt; {});\n  return THREE;\n}\n\n// Initialize scene\nscene = {\n    const scene = new THREE.Scene();\n    return scene;\n}\n\n// Initialize renderer\nrenderer = {\n    const renderer = new THREE.WebGLRenderer({ antialias: true });\n    renderer.setSize(width, height);\n    return renderer;\n}\n\ncontrols = {\n    const controls = new THREE.OrbitControls(camera, renderer.domElement);\n    controls.addEventListener(\"change\", () =&gt; renderer.render(scene, camera));\n    invalidation.then(() =&gt; (controls.dispose(), renderer.dispose()));\n    return controls;\n}\n\n\nlights = {\n    // Create a directional light with increased intensity\n    const directionalLight = new THREE.DirectionalLight(0xffffff, 2);  // Intensity set to 2 (brighter)\n    directionalLight.position.set(0, 10, 10).normalize();\n    scene.add(directionalLight);\n\n    // Add ambient light for soft overall illumination\n    const ambientLight = new THREE.AmbientLight(0xffffff, 1);  // Intensity set to 0.5 (softer ambient light)\n    scene.add(ambientLight);\n    \n    return { directionalLight, ambientLight };\n}\n\n// Initialize camera\ncamera = {\n    const camera = new THREE.PerspectiveCamera(85, width / height, 0.1, 1000);\n    camera.position.set(0, 20, 20);\n    // Resize the camera aspect when the window is resized\n    window.addEventListener('resize', () =&gt; {\n        camera.aspect = width / height;\n        camera.updateProjectionMatrix();\n        renderer.setSize(width, height);\n    });\n    return camera;\n}\n\n// Function to center the camera on the model\nfunction centerViewOnModel(model, camera) {\n    // Compute the bounding box of the model\n    const box = new THREE.Box3().setFromObject(model);\n\n    // Get the center of the model\n    const center = box.getCenter(new THREE.Vector3());\n\n    // Get the size of the model (distance between min and max corners)\n    // const size = box.getSize(new THREE.Vector3());\n\n    // // Move the camera based on the size of the model\n    // const maxDim = Math.max(size.x, size.y, size.z);\n    // const fov = camera.fov * (Math.PI / 180);  // Convert vertical FOV to radians\n    // let cameraZ = Math.abs(maxDim / (2 * Math.tan(fov / 2)));  // Distance from the object\n    // cameraZ *= 0.5; // Optionally add some padding\n\n    // Set the camera position\n    camera.position.set(center.x, center.y + 200, center.z);\n\n    // Make the camera look at the center of the model\n    camera.lookAt(center);\n}\n\nimportedObj = {\n  const url = await FileAttachment('data/model.glb').url()\n  return loadObject(url)\n}\n\n\nloadObject = (url) =&gt; new Promise ((resolve,reject) =&gt; {\n  const loader = new THREE.GLTFLoader();\n  // instantiate a loader\n  loader.load(\n      // resource URL\n    url,\n    // called when resource is loaded\n    function ( gltf ) {\n      const model = gltf.scene;\n      scene.add(model);\n      centerViewOnModel(model, camera);\n      resolve(model);\n    },\n      // called when loading is in progresses\n      function ( xhr ) {\n          return ( xhr.loaded / xhr.total * 100 ) + '% loaded' ;\n    },\n      // called when loading has errors\n      function ( error ) {\n        reject (\"Error in loading\")\n      }\n  );\n})\n\n// Render loop\n{\n  while (true) {\n    requestAnimationFrame(() =&gt; {\n      // Rotate the scene slightly\n    //   scene.rotation.y += 0.001;\n    //   scene.rotation.z -= 0.001;\n      // Render the scene with the camera\n      renderer.render(scene, camera);\n    });\n    await Promises.delay(16); // Delay of ~16ms for smooth animation (60fps)\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrenderer.domElement"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/part_a_uas_flight_planning.html#outline",
    "href": "course/topics/topic_3_flight_planning/part_a_uas_flight_planning.html#outline",
    "title": "UAS Flight Planning",
    "section": "Outline",
    "text": "Outline\n\nDefining the data collection purpose\nSurvey area, take off and landing locations\nTerrain and weather conditions\nSafe flight\nFlight parameters\nFlight planning and management platforms"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/part_a_uas_flight_planning.html#lecture",
    "href": "course/topics/topic_3_flight_planning/part_a_uas_flight_planning.html#lecture",
    "title": "UAS Flight Planning",
    "section": "Lecture",
    "text": "Lecture\n\nFlight planning introduction\nLecture Recording (NCSU Only)\nAssignment Recording (NCSU Only)"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/part_a_uas_flight_planning.html#supplemental-materials",
    "href": "course/topics/topic_3_flight_planning/part_a_uas_flight_planning.html#supplemental-materials",
    "title": "UAS Flight Planning",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nFlight - watch the following if you were unable to attend the flight session\n\nFixed wing flight (eBee X): set up, launch and landing\nVertical take-off (VTOL) fixed wing flight with Wingtra drone\nRotary wing flight (Phantom)\n\nFlight with Drone Deploy flight planning app\nFlight with Pix4D Capture flight planning app\n\n\n\n\nFlight planning and management platforms\n\nCommerical Options\n\nDroneDeploy\nPix4Dcapture Pro\nDJI GS PRO\nUgCS\nDroneLink\n\n\n\nOpen Source Options\n\nQGroundControl\nMission Planner\nMavProxy"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/part_a_uas_flight_planning.html#assignment",
    "href": "course/topics/topic_3_flight_planning/part_a_uas_flight_planning.html#assignment",
    "title": "UAS Flight Planning",
    "section": "Assignment",
    "text": "Assignment\nPrepare a flight plan:\n\n\nFlight plan analysis in GRASS GIS"
  },
  {
    "objectID": "course/topics/topic_3_flight_planning/part_a_uas_flight_planning.html#homework",
    "href": "course/topics/topic_3_flight_planning/part_a_uas_flight_planning.html#homework",
    "title": "UAS Flight Planning",
    "section": "Homework",
    "text": "Homework\nPrepare a report on the flight plan: purpose, permits, map of the area boundary, with take off, landing, locations, ground control points and flight altitude analysis."
  },
  {
    "objectID": "course/topics/topic_helene/tutorial.html#set-up-environment",
    "href": "course/topics/topic_helene/tutorial.html#set-up-environment",
    "title": "Helene Photogrametery",
    "section": "Set up environment",
    "text": "Set up environment\n\nimport os\nimport sys\nimport subprocess\nfrom pathlib import Path\nfrom IPython.display import display\n\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor, as_completed"
  },
  {
    "objectID": "course/topics/topic_helene/tutorial.html#download-images-for-processing",
    "href": "course/topics/topic_helene/tutorial.html#download-images-for-processing",
    "title": "Helene Photogrametery",
    "section": "Download images for processing",
    "text": "Download images for processing\n\ndef query_feature_service(flight_id):\n    \"\"\"Query FeatureService by flight_id\"\"\"\n    try:\n        rest_url = 'https://services.arcgis.com/XG15cJAlne2vxtgt/ArcGIS/rest/services/Image_Points_view/FeatureServer/3/query'\n\n        query = f\"project='CAP - H-TS Helene 2024' AND mission='24-1-5849' AND flight='{flight_id}'\"\n\n        params = {\n            'where': query,\n            'outFields': '*',\n            'f': 'json',\n            'returnGeometry': 'false'\n        }\n        response = requests.get(rest_url, params=params)\n        json_data = response.json()\n        url_list = list(map(lambda feat: feat.get('url'), json_data['features']))\n        return url_list\n    except requests.RequestException as e:\n        print(f\"Error fetching data from FeatureService: {e}\")\n\n# Function to download an image from a URL\ndef download_image(url, save_path, session):\n    \"\"\"Download and save image from a url\"\"\"\n    try:\n        response = session.get(url)\n        response.raise_for_status()  # Check for HTTP errors\n        with open(save_path, 'wb') as file:\n            file.write(response.content)\n        print(f\"Downloaded {url} to {save_path}\")\n    except requests.RequestException as e:\n        print(f\"Failed to download {url}: {e}\")\n\n# Main function to set up the thread pool and download images\ndef run(image_urls, save_dir, num_threads=4):\n    \"\"\"Creates thread pool and download images from a list of urls\"\"\"\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n        with requests.Session() as session:\n            futures = []\n            for url in image_urls:\n                filename = os.path.basename(url)\n                save_path = os.path.join(save_dir, filename)\n                futures.append(executor.submit(download_image, url, save_path, session))\n\n            for future in as_completed(futures):\n                future.result()  # This will raise \n\n\nRun the download command\nMake sure to set appropriate threads for your system.\n\nurl_list = query_feature_service(flight_id='A0046B')\n\nrun(urls=url_list, save_dir=\"imagery_data/nadir/{flight}\", num_threads=32)\n\n\n\nRun data in your favorite photogrametry software\n\nWebODM\nAgisoft Metashape"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/part_a_analysis_uas_processing_results.html#outline",
    "href": "course/topics/topic_4_GIS_analytics/part_a_analysis_uas_processing_results.html#outline",
    "title": "A. Analysis of UAS data processing results",
    "section": "Outline",
    "text": "Outline\n\nCompare survey data and errors from different flights\nRecognize differences in geoprocessing products generated by different softwares\nUnderstand significance of using GCPs in the processing and their impact on the final products\nRecognize what processing steps need to be modified in order to improve particular errors\nUnderstand what causes the ‘Bowl effect’ and know the solutions to fix it\nExecute distance, area and volume measurements"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/part_a_analysis_uas_processing_results.html#lecture",
    "href": "course/topics/topic_4_GIS_analytics/part_a_analysis_uas_processing_results.html#lecture",
    "title": "A. Analysis of UAS data processing results",
    "section": "Lecture",
    "text": "Lecture\n\nLecture Slides\nLecture Recording: Fall 2024 (NCSU Only)\nIn-Class Recording: Fall 2024 (NCSU Only)"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/part_a_analysis_uas_processing_results.html#supplemental-materials",
    "href": "course/topics/topic_4_GIS_analytics/part_a_analysis_uas_processing_results.html#supplemental-materials",
    "title": "A. Analysis of UAS data processing results",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nBowl effect explained: Mitigating systematic error in topographic models derived from UAV and ground-based image networks\nImage residuals, camera calibration and impact of GCPs: Evaluation of structure from motion for soil microtopography measurement\nAlternative to GCPs - RTK/PPK equipped UAS: Do RTK/PPK drones give you better results than using GCPs?"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/part_a_analysis_uas_processing_results.html#assignment-homework",
    "href": "course/topics/topic_4_GIS_analytics/part_a_analysis_uas_processing_results.html#assignment-homework",
    "title": "A. Analysis of UAS data processing results",
    "section": "Assignment & Homework",
    "text": "Assignment & Homework\nAssignment 4A"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/assignments/analysis.html#gis-based-analysis-of-uas-derived-data-processing-outputs",
    "href": "course/topics/topic_4_GIS_analytics/assignments/analysis.html#gis-based-analysis-of-uas-derived-data-processing-outputs",
    "title": "Assignment 4A - Analysis",
    "section": "GIS based analysis of UAS derived data processing outputs",
    "text": "GIS based analysis of UAS derived data processing outputs\nIn this assignment, we analyze the DSMs derived from the UAS imagery in terms of their accuracy and type of distortions based on the use of GCPs and different processing software. Then we map the terrain change (due to vegetation growth, erosion, and other impacts using two different UAS surveys).\n\nIncluding GCPs in geoprocessing improves the results. Does it have an effect on the phenomenon that you are investigating?\nAgisoft is only one example of the software solutions available for the UAS data processing. In the GRASS location, DSMs are available, generated in different environments: Agisoft, Pix4D, and Trimble Business Center (for one example flight). Compare the differences between these DSMs.\nIn Imagery processing assignment, you’ve used arbitrary given parameter values in each processing step. You can change various modes, quality, values, and options to investigate if it improves the results.\nAgisoft Manual.\n\n\nData\n\nLake Wheeler data set (formatted as GRASS location): Lake_Wheeler_NCspm (you should already have it from previous assignments)\n\n\n\nDSMs comparison - influence of GCPs\nIn order to detect the bowl effect, an additional DSM needs to be computed. This time GCPs should not be used in the processing. You can repeat all the steps in Agisoft from the Imagery processing assignment.\nBatch processing commands should be in this order:\n\nAlign Photos\nOptimize Alignment (this is only based on camera coordinates)\nBuild Dense Point cloud (quality: medium)\nGenerate DEM\n\nNow you should have two DSMs:\n\nOne generated in processing with 3 GCPs in the Imagery processing assignment\nOne generated in processing without GCPs\n\nPacks are provided for those working in DEMO mode; save them in your working directory.\n\nCalculate differences between processed sample data\nWe will use GRASS GIS to compare the DSMs.\nOpen GRASS with the “Lake_Wheeler_NCspm” location and create a new mapset for this assignment (you can call it “analysis”). Then change your working directory (Settings &gt; GRASS working environment &gt; Change working directory) to where your DSM files are.\nr.unpack DSMagi_2016_09_22_sample_noGCPs.pack -o\nr.unpack DSMagi_2016_09_22_sample.pack -o\nCalculate the differences in elevation in the DSM generated with GCPs and the new one without GCPs in GRASS GIS using raster map algebra:\ng.region rast=DSMagi_2016_09_22_sample\nr.mapcalc expression=\"GCP_noGCP_class = DSMagi_2016_09_22_sample - DSMagi_2016_09_22_sample_noGCPs\"\nChange the color map to better see the differences. You can experiment with your own color table or enter the following values directly in the r.colors dialog (second tab: Define):\n0 red\n35 orange\n36 yellow\n37 cyan\n38 aqua\n39 blue\n\n\nCalculate differences between DSMs from larger area\nYour sample data has very limited extent. To fully understand the phenomenon caused by the absence of GCPs, compare the sample DSMs from the Lake_Wheeler_NCspm_course location. There were 4 GCPs used for generating sample_DSM. You can see localization by adding the GCP_12 vector layer to the display.\ng.region rast=sample_DSM\nr.mapcalc expression=\"GCP_noGCP = sample_DSM - sample_DSM_noGCPs\"\nChange the color map to better see the differences. You can experiment with your own color table or enter the following values directly in the r.colors dialog (second tab: Define):\n0 red\n35 orange\n36 yellow\n37 cyan\n38 aqua\n39 blue\n\nWhy are these patterns different? Consider the distribution of the GCPs in both datasets and the shape of the area.\n\n\n\n\nCompare DSMs generated in different software\nFor the flight from 2015-06-20, in the Lake_Wheeler_NCspm_course, there are DSMs with GCPs generated in 3 different software environments (Trimble Business Center, Agisoft, and Pix4D). Additionally, there are DSMs generated without GCPs in Agisoft and Pix4D.\nCheck the bowl effect for the Agisoft generated products:\ng.region rast=2015_06_20_DSM_agi_11GCP\nr.mapcalc expression=\"agi_GCP_agi_noGCP = 2015_06_20_DSM_agi_11GCP - 2015_06_20_DSM_agi_noGCP\"\nChange the color table to better see the differences.\n0 red\n35 orange\n36 yellow\n37 cyan\n38 aqua\n42 blue\nCheck the bowl effect for the Pix4D generated products:\nr.mapcalc expression=\"p4d_GCP_p4d_noGCP = 2015_06_20_pix4d_11GCP_dsm - 2015_06_20_DSM_pix4d_NoGCP\"\nChange the color map to better see the differences. You can experiment with your own color table or enter following values directly in the dialog r.colors dialog second tab Define\n50 red\n69 orange\n70 yellow\n71 cyan\n72 aqua\n75 blue\n\n\n\n\n\n\nImportant\n\n\n\nWhich software generated a larger bowl effect?\n\n\nr.mapcalc expression=\"agi_trimble = 2015_06_20_DSM_agi_11GCP - 2015_06_20_DSM_Trimble_11GCP\"\nr.mapcalc expression=\"p4d_agi = 2015_06_20_pix4d_11GCP_dsm - 2015_06_20_DSM_agi_11GCP\"\nr.mapcalc expression=\"p4d_trimble = 2015_06_20_pix4d_11GCP_dsm - 2015_06_20_DSM_Trimble_11GCP\"\nApply the color table that varies from -1m to 1m to visualize results of the comparison using the r.colors\n-40 red\n-1 orange\n-0.5 yellow\n-0.1 grey\n0 white\n0.1 grey\n0.5 cyan\n1 aqua\n35 blue\n\n\n\n\n\n\nImportant\n\n\n\nCompare the maps. How do they relate to each other? What patterns (artifacts) can you recognize?\n\n\n\n\nDetect terrain change using two UAS surveys\nIt is also useful to a perform terrain analysis using GIS tools to see the differences even more clearly.\nRun the r.slope.aspect and r.relief command for chosen DSMs, for example:\ng.region rast=sample_DSM\nr.slope.aspect elevation=sample_DSM slope=sample_DSM_slope aspect=sample_DSM_aspect pcurvature=sample_DSM_curv\nr.relief input=sample_DSM output=sample_DSM_relief\nr.slope.aspect elevation=sample_DSM_noGCPs slope=sample_DSM_noGCP_slope aspect=sample_DSM_noGCP_aspect pcurvature=sample_DSM_noGCP_curv\nr.relief input=sample_DSM output=sample_DSM_noGCP_relief\nYou can also install GRASS GIS Addons (via g.extension):\n\nr.local.relief\nr.shaded.pca\nr.skyview\n\nand apply this modules to sample data or your data.\nSet your computational region\ng.region rast=sample_DSM\nInstall and run the r.local.relief addon.\ng.extension extension=r.local.relief\nr.local.relief input=sample_DSM output=local_relief_sample_DSM\nr.local.relief input=sample_DSM_noGCPs output=local_relief_sample_DSM_noGCPs\nInstall and run r.shaded.pca\ng.extension extension=r.shaded.pca operation=add \nr.shaded.pca input=sample_DSM output=shaded_pca_sample_DSM\nr.shaded.pca input=sample_DSM_noGCPs output=shaded_pca_sample_DSM_noGCPs\nInstall and run r.skyview\ng.extension extension=r.skyview operation=add\nr.skyview input=sample_DSM output=skyview_sample_DSM\nr.skyview input=sample_DSM_noGCPs output=skyview_sample_DSM_noGCPs\n\n\nDetect terrain change using two UAS surveys\ng.region rast=2015_06_20_DSM_agi_11GCP\nr.mapcalc \"diff_jun_oct_agis = 2015_06_20_DSM_agi_11GCP - 2015_10_06_DSM_agi_8GCPs\"\nApply the color table that varies from -1m to 1m to visualize results of the comparison using r.colors module.\nIn the Define tab, type the following rules in the enter values directly option.\n -40 red\n -1 orange\n -0.5 yellow\n -0.1 grey\n 0 white\n 0.1 grey\n 0.5 cyan\n 1 aqua\n 35 blue\nNote the accuracy of the DSM along the roads, interpret the observed gain and loss in elevation.\nUse orthophotos from:\n\nNov 2015\nMarch 2017\nJune 2017"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/assignments/assignment_4b.html#task",
    "href": "course/topics/topic_4_GIS_analytics/assignments/assignment_4b.html#task",
    "title": "Assignment 4B",
    "section": "Task",
    "text": "Task\n\nProcessing lidar and UAS SfM point clouds"
  },
  {
    "objectID": "course/topics/topic_4_GIS_analytics/assignments/assignment_4b.html#homework",
    "href": "course/topics/topic_4_GIS_analytics/assignments/assignment_4b.html#homework",
    "title": "Assignment 4B",
    "section": "Homework",
    "text": "Homework\nPrepare report on analyzing point clouds, computing and visualizing bare earth and canopy surfaces."
  },
  {
    "objectID": "course/topics.html#topic-1-uas-basics",
    "href": "course/topics.html#topic-1-uas-basics",
    "title": "Topics",
    "section": "Topic 1: UAS Basics",
    "text": "Topic 1: UAS Basics\nA. Introduction to Unmanned Aerial Systems\nB. Rules and regulations for UAS operations"
  },
  {
    "objectID": "course/topics.html#topic-2-structure-from-motion",
    "href": "course/topics.html#topic-2-structure-from-motion",
    "title": "Topics",
    "section": "Topic 2: Structure from Motion",
    "text": "Topic 2: Structure from Motion\nA.From images to 3D models: Photogrammetry and Structure from Motion concepts\nB. UAS Imagery processing"
  },
  {
    "objectID": "course/topics.html#topic-3-uas-flight-planning",
    "href": "course/topics.html#topic-3-uas-flight-planning",
    "title": "Topics",
    "section": "Topic 3: UAS Flight Planning",
    "text": "Topic 3: UAS Flight Planning\nA. UAS Flight Planning"
  },
  {
    "objectID": "course/topics.html#topic-4-gis-analysis",
    "href": "course/topics.html#topic-4-gis-analysis",
    "title": "Topics",
    "section": "Topic 4: GIS Analysis",
    "text": "Topic 4: GIS Analysis\nA. Analysis of UAS data processing results\nB. Point cloud data analysis: point clouds, surfaces, and voxel models"
  },
  {
    "objectID": "course/topics.html#topic-5-advanced-analytics",
    "href": "course/topics.html#topic-5-advanced-analytics",
    "title": "Topics",
    "section": "Topic 5: Advanced Analytics",
    "text": "Topic 5: Advanced Analytics\nA. UAS and lidar data: comparison, fusion and analysis\nB. Analysis of multitemporal UAS data and its applications"
  },
  {
    "objectID": "course/topics.html#topic-6-imagery-processing-and-sfm",
    "href": "course/topics.html#topic-6-imagery-processing-and-sfm",
    "title": "Topics",
    "section": "Topic 6: Imagery Processing and SfM",
    "text": "Topic 6: Imagery Processing and SfM"
  },
  {
    "objectID": "course/topics.html#topic-7-3d-modeling-and-geovisualization",
    "href": "course/topics.html#topic-7-3d-modeling-and-geovisualization",
    "title": "Topics",
    "section": "Topic 7: 3D Modeling and Geovisualization",
    "text": "Topic 7: 3D Modeling and Geovisualization"
  },
  {
    "objectID": "course/projects.html",
    "href": "course/projects.html",
    "title": "Projects",
    "section": "",
    "text": "The focus of this project is on the analysis of UAS data rather than the data collection process itself. You are not required to collect your own UAS data; however, the dataset you use must be sourced from UAS data. Alternatively, your project could involve enhancing some aspect of UAS data collection, such as flight planning or Structure from Motion (SfM)."
  },
  {
    "objectID": "course/projects.html#proposal-presentation-and-paper-requirements",
    "href": "course/projects.html#proposal-presentation-and-paper-requirements",
    "title": "Projects",
    "section": "Proposal, presentation and paper requirements",
    "text": "Proposal, presentation and paper requirements\n\nPre-Proposal\n\nSubmit a project pre-proposal to the project discussion forum on Moodle.\n\nIn the pre-proposal please define the scope of the project, the project location, data, and analysis you would like to perform.\n\n\nProposal\n\nDefine research question, select location, list available data, methods (theory), software tools, describe expected results.\n\nSubmit to Moodle\n\n\nPresentation\n\nLength: DE students 10 minutes, on-campus students 15 min + 5 min discussion\nIntroduction/background: problem, motivation for the research, research question / objective\nStudy site: where, why this site, geographic characteristics\nData acquisition: platform and sensor, flight plan (if applicable), flight conditions\nData properties and processing: resolution, spatial extent, accuracy\nAnalysis and/or modeling methods: describe methodology and workflows\nResults: present and explain the results using qualitative and quantitative description, tables, graphs, maps/images\nDiscussion: discuss impact of flight conditions, data and methods on the results, uncertainty issues, compare with results from other studies, which questions remain unresolved, what still needs to be done\nConclusion: summary of the most important findings including advances in methodology, future work\n\n\n\nPaper\n\nStructure and formatting should follow scientific journal standards.\nSame sections as the presentation\nText and Figures: min. 4 pages, single spaced including tables and references images, maps, graphs, presented in readable size with scale and legends where needed\nReferences: at least 5 papers from scientific journals, rest can be reports, web documents\nAppendix (optional): workflows, scripts, metadata. Software commands, issues go here"
  },
  {
    "objectID": "course/projects.html#past-projects",
    "href": "course/projects.html#past-projects",
    "title": "Projects",
    "section": "Past Projects",
    "text": "Past Projects\n\nProjects 2021\n\nBartenfelder, Amy: Areal Changes at LAke Wheeler Field After the 2017 Hurricane Season\nWebb, Andrew: Cabon Mapping of Agricultural Fields\nWilliams, Chantel: Northern Umstead State Park - Vegetation\nGoodnight, Dallas: Can NC One Map Orthoimagery be used to accurately measure Cape Lookout Park shoreline elevation changes over time?\nWingler, Dylan: Using LiDAR to Compare Impact between Hurricane Matthew and Sandy on Cape Hatteras, NC\nPelfrey, Hank: Estimating Individual Tree Location and Height with UAS Photogrammetry\nDorner, Judy: Object detection in UAS Imagery Using Machine Learning Applications\nMoy, Matthew: Creating and Interpolated DSM of Downtown Cary, NC\nPerry, Marcus: Mars Science Helicopter: Exploring Photogrammetry with Ingenuity’s NavCam Data\nSawtelle, Macy: Testing the Capability of Unmanned Aircraft vehicles to Monitor Forest Regeneration Activities\nWilliams, Patrick: Using Unmanned Aerial System Imagery to Perform Solar Potential Analysis\nErlenbach, Peter: Image Classification Using Drone Data\nJohnson, Randy: Urban Landcover Classification Utilizing UAS\nHill, Russell: Canyonlands National Park - 3D mash and a Scene Layer Package\nFarrell, Sean: Multiple Return Lidar Analysis of Hydrologic Changes Due to Highway Flooding from Hurricane Florence\nTan, Samantha: The Impact of hurricane Events on Coastlines: Measuring Coastline Loss with Pre and Post Michael Orthophotogrammetry\nHerzig, Bill: Drone vs Satellite - Multispectral NDVI Image Pixel Analysis\n\n\n\nProjects 2020\n\nAdams, Eric: Environmental Study Using UAS Data in the Edenton Bay, NC Region\nBert, Steve: Optimizing Parking Management for the City of Raleigh. An Unmanned Aircraft Systems Inventory Approach\nDavis, Britt: Structure from Motion as a Participatory Design Tool: The Landscape Architect’s Guide to Better Models and Deeper Community Engagement\nFranklin, Samuel: A Comparative Analysis of UAS in Archeology\nMcDonald, Christopher: UAS Technology for Mapping Urban Development – SouthEnd District of Charlotte, NC\nParish, William: UAS Implementation for Civil Engineering Design and Construction: NC 12 Bridge near Rodanthe Study\nPhillips, Ryan: Evaluating the Effect of Image Resolution on Digital Surface Model Construction and Usefulness\nPierson, Gardner: The Use of High Resolution UAS Data vs Traditional Ariel Imaging to Predict Inundation and Potential Urban Impact on the Southern Slope of Mt. Etna, Italy, Using a Lava Flow Emplacement Model.\nSignor, Kari: Exploring Lidar Classification Methods: Archaeological Applications in the Maya Lowlands\nSpear, Michael: Using UAS Umagery for Land Cover Classification\nWatts, Matthew: Exploring LiDAR in Area of Wildfire and Assessing the Benefits of Adding UAS Data\n\n\n\nProjects 2019\n\nBaker, Kurt: Effective ground classification of non-uniform laser data\nBrown, Tamika: The Implications of Using Unmanned Aerial Systems to Monitor Hazardous Waste Facility Sites and Better Understand Community Endangerment within the City of Chicago\nCharping, Charlie: UAS Technology for Open Pit Mining\nConrad, Matt: 3D Modelling from Video: Technology Application\nCummings, John: Automated Detection of Roadway Features Via UAS\nDavis, Jeremy: Creating a Basis for the Influence of Elevation on Wheat Varieties in North Carolina\nGroh, Erica: Proposed UAS Survey of Alluvial Fans in California\nHoffman, Dallas: Analysis of Multiple Return Lidar in ArcGIS\nJones, Alli: LIDAR and UAV to Monitor Beach Nourishment - Emerald Isle, NC\nMa, Xingli: Use of UAS to Detect Disease in Soybeans\nNicholas, D. Chase: Tracking Crop Development with UAVs: Using SfM to Estimate Plant Height and Volume\nOberrender, Daniel: Cave Detection using Local Relief Model derived from UAS SfM\nPotter, Andrew: Coastal Change Analysis of the Cape Lookout National Seashore\nRuiz, Rachel: Implementation of Unmanned Aerial System to Conduct Bridge Inspection\nScheip, Corey: UAS Imagery to Supplement Lidar-Based Landslide Programs\nWheaton, James: Utilizing UAS to Generate Land Cover Data\nWilliams, Caleb: Unmanned Aerial Systems for Waterfowl Population Studies at the Tom Yawkey Wildlife Center\n\n\n\nProjects 2018\n\nAlbert, James: A New Approach to Landfill Management in the Solid Waste Industry\nAnderson, Alexander: Using UAS Structure from Motion and and LiDAR DSMs to Identify, Monitor and Mitigate Coastal Erosion In Okaloosa County\nBastias, Sabina and Montgomery, Kellyn: Combining Nadir and Oblique Imagery to Address Distortion in UAS Data\nCatlow, Maureen, Hahn, Becca, and Voigt Erin: “RescUAV data after Hurricane Irma: Natural Disaster Analysis”\nDawson, Victor: ???\nEdenhart-Pepe, Skyler and Pierce, Austin: Evaluating the use of time series UAS and Lidar data to monitor rate of change of hydrologic flow patterns on land development projects.\nFelipe, Lauren: ???\nForte, Michael: Object Detection Using Structure From Motion Techniques\nHowell, Andrew: Estimating Area and Standing Biomass of Zizania latifolia Using sUAS\nKesselring, Todd: Using LiDAR and NDVI for Vegetation Management in Utility Right of Ways\nLamb, Kelsey: Mapping surface water and impervious surface\nLiesch, Mandy: Changes in Water Balance Associated with Farm Development\nMeyers, William: Measuring and Modeling Biomass of Pines at Tatum Farm\nSchrum, Paul: 3D UAV track in Blender: A Blender Addon to import, edit, then export UAV trajectories\nSuffern, Carrie: Monitoring Drainage Issues at a Small Farm: Use of UAS DSMs and LiDAR DEMs to Forecast Storm Runoff and Monitor Sinkhole Formation\nVincent, Sarah: UAV versus Statewide Contours, is it worth it to move dirt?\n\n\n\nProjects 2016\n\nTravis Howell: Mapping volume of wood chip pile\nCorbin Kling: Jockey’s Ridge State Park: a potential Mars dune analog\nNicholas Kruskamp: Forest structure\nWilliam Ross: Water Surface Elevation Generation & Storm Debris Volume Estimation using UAS\nJoshua Rudd: Crop growth monitoring using sUAS\n\n\n\nProjects 2015\n\nDyer Tristan: Barrier island monitoring of volume change\nFoley Molly: Mapping forest fragmentation in urbanizing landscape using sUAS\nReckling William: Utilities\nBayasgalan Gantulga: Beaver dam impact assessment\nBelica Laura: Monitoring grass conditions under different levels of management (change to solar irradiation from UAS DSM?)\nHarmon Brendan: Gully monitoring and volume change assesment\nPetras Vaclav: Optimizing point cloud density for modeling microtopography controls over surface flow.\nPetrasova Anna: Extracting bare earth from sUAS using lower resolution bare earth lidar\nSmart Lindsey Suzanne: Mapping coastal plain microtopography and its impact on surface water distribution\nVelasquez Montoya Liliana: Mapping soil erosion using sUAS DEMs"
  }
]